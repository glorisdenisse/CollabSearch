{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI9rRfYae-ir",
        "outputId": "122c4fea-0eec-448b-a361-2294371718a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: langchain in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.99.9)\n",
            "Requirement already satisfied: google-api-python-client in /opt/miniconda3/lib/python3.10/site-packages (2.178.0)\n",
            "Requirement already satisfied: langchain_community in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: tools in /opt/miniconda3/lib/python3.10/site-packages (1.0.2)\n",
            "Requirement already satisfied: langchain_google_community in /opt/miniconda3/lib/python3.10/site-packages (2.0.7)\n",
            "Requirement already satisfied: langchain_openai in /opt/miniconda3/lib/python3.10/site-packages (0.3.30)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.40.3)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (3.11.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (2.4.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (1.71.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.7)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: greenlet>=1 in /opt/miniconda3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.99.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gradio in /opt/miniconda3/lib/python3.10/site-packages (5.42.0)\n",
            "Requirement already satisfied: openpyxl in /opt/miniconda3/lib/python3.10/site-packages (3.1.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /opt/miniconda3/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (1.26.18)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai>=1.0.0 geotext transformers GeoText\n",
        "!pip install -q -U google-genai\n",
        "!pip install langchain openai google-api-python-client langchain_community tools langchain_google_community langchain_openai\n",
        "!pip install --upgrade openai\n",
        "!pip install gradio openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DibFVFaDfIq8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from langchain import GoogleSearchAPIWrapper\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sui66xUhfUlC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"IDA_LLM_API_KEY\"]=\"your_key_here\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_key_here\"\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"your_key_here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHCvF0cl9hDI"
      },
      "outputs": [],
      "source": [
        "# Initialize your private LLM client (llama-3-8b-instruct-instruct via university server)\n",
        "client = OpenAI(\n",
        "    base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\",\n",
        "    api_key=os.environ['IDA_LLM_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lso35gZXOO1H"
      },
      "outputs": [],
      "source": [
        "def initialize_database_with_sources():\n",
        "    \"\"\"Enhanced database initialization with comprehensive tracking fields\"\"\"\n",
        "    if not os.path.exists('cola_database.csv'):\n",
        "        columns = [\n",
        "            'ID', 'Original_Query', 'Rewritten_Query', 'Selected_Topic_Intent', 'Selected_Answer_Type',\n",
        "            'Routing_Type', 'Routing_Method', 'Dictionary_Match', 'Identified_Topic', 'Assigned_Roles',\n",
        "            'Linguist_Analysis', 'Expert_Analysis', 'User_Analysis', 'In_Favor', 'Against',\n",
        "            'Final_Judgement', 'Synthesis_Method', 'After_RAG_Agent', 'RAG_Source_URLs',\n",
        "            'Processing_Time_Seconds', 'Processing_Time_Seconds_RAG', 'Timestamp', 'Status'\n",
        "        ]\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "        # Comprehensive dtype mapping\n",
        "        dtype_dict = {\n",
        "            'ID': 'int64',\n",
        "            'Original_Query': 'string',\n",
        "            'Rewritten_Query': 'string',\n",
        "            'Selected_Topic_Intent': 'string',\n",
        "            'Selected_Answer_Type': 'string',\n",
        "            'Routing_Type': 'string',  # 'LLM' or 'DICTIONARY'\n",
        "            'Routing_Method': 'string',  # 'exact_match', 'partial_match', 'LLM_fallback', etc.\n",
        "            'Dictionary_Match': 'string',  # The actual dictionary key that matched\n",
        "            'Identified_Topic': 'string',  # The final topic from get_roles()\n",
        "            'Assigned_Roles': 'string',  # JSON string of the three expert roles\n",
        "            'Linguist_Analysis': 'string',\n",
        "            'Expert_Analysis': 'string',\n",
        "            'User_Analysis': 'string',\n",
        "            'In_Favor': 'string',\n",
        "            'Against': 'string',\n",
        "            'Final_Judgement': 'string',  # The final response regardless of synthesis method\n",
        "            'Synthesis_Method': 'string',  # 'decision_making', 'solution_focused', 'informational'\n",
        "            'After_RAG_Agent': 'string',\n",
        "            'RAG_Source_URLs': 'string',\n",
        "            'Processing_Time_Seconds': 'float64',\n",
        "            'Processing_Time_Seconds_RAG': 'float64',\n",
        "            'Timestamp': 'string',\n",
        "            'Status': 'string'\n",
        "        }\n",
        "\n",
        "        df = df.astype(dtype_dict)\n",
        "        df.to_csv('cola_database.csv', index=False)\n",
        "        print(\"‚úÖ Enhanced COLA database with comprehensive tracking initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY6-f5uBOO1I"
      },
      "outputs": [],
      "source": [
        "def add_new_query(query_id, query_text):\n",
        "    \"\"\"FIXED: Add a new query without pandas FutureWarning\"\"\"\n",
        "    try:\n",
        "        if os.path.exists('cola_database.csv'):\n",
        "            df = pd.read_csv('cola_database.csv')\n",
        "        else:\n",
        "            initialize_database_with_sources()\n",
        "            df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        # FIXED: Create new row data that matches DataFrame structure exactly\n",
        "        new_row_data = {\n",
        "            'ID': int(query_id),\n",
        "            'Original_Query': str(query_text),\n",
        "            'Rewritten_Query': '',\n",
        "            'Selected_Topic_Intent': '',\n",
        "            'Selected_Answer_Type': '',\n",
        "            'Routing_Type': '',\n",
        "            'Routing_Method': '',\n",
        "            'Dictionary_Match': '',\n",
        "            'Identified_Topic': '',\n",
        "            'Assigned_Roles': '',\n",
        "            'Linguist_Analysis': '',\n",
        "            'Expert_Analysis': '',\n",
        "            'User_Analysis': '',\n",
        "            'In_Favor': '',\n",
        "            'Against': '',\n",
        "            'Final_Judgement': '',\n",
        "            'Synthesis_Method': '',\n",
        "            'After_RAG_Agent': '',\n",
        "            'RAG_Source_URLs': '',\n",
        "            'Processing_Time_Seconds': None,\n",
        "            'Processing_Time_Seconds_RAG': None,\n",
        "            'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Status': 'pending'\n",
        "        }\n",
        "\n",
        "        # FIXED: Ensure all columns are present and in correct order\n",
        "        if not df.empty:\n",
        "            # Make sure new row has exact same columns as existing DataFrame\n",
        "            for col in df.columns:\n",
        "                if col not in new_row_data:\n",
        "                    new_row_data[col] = None if df[col].dtype in ['float64', 'int64'] else ''\n",
        "\n",
        "            # Create new row with matching column order\n",
        "            new_row = pd.DataFrame([new_row_data], columns=df.columns)\n",
        "        else:\n",
        "            new_row = pd.DataFrame([new_row_data])\n",
        "\n",
        "        # FIXED: Proper concatenation without FutureWarning\n",
        "        df = pd.concat([df, new_row], ignore_index=True, sort=False)\n",
        "        df.to_csv('cola_database.csv', index=False)\n",
        "        print(f\"‚úÖ Added query {query_id} to database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error adding query to database: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nTwkTBDOO1I"
      },
      "outputs": [],
      "source": [
        "def update_comprehensive_results(query_id, results_dict):\n",
        "    \"\"\"Update comprehensive tracking results for a query\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        mask = df['ID'] == query_id\n",
        "\n",
        "        if mask.any():\n",
        "            for column, value in results_dict.items():\n",
        "                if column in df.columns:\n",
        "                    if column in ['Processing_Time_Seconds', 'Processing_Time_Seconds_RAG']:\n",
        "                        try:\n",
        "                            df.loc[mask, column] = float(value) if value is not None else None\n",
        "                        except (ValueError, TypeError):\n",
        "                            df.loc[mask, column] = None\n",
        "                    else:\n",
        "                        df.loc[mask, column] = str(value) if value is not None else ''\n",
        "\n",
        "            df.loc[mask, 'Status'] = 'completed'\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"‚úÖ Updated comprehensive results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Query ID {query_id} not found in database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error updating results: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaLQ3scxOO1I"
      },
      "outputs": [],
      "source": [
        "def view_comprehensive_database_stats():\n",
        "    \"\"\"FIXED: Enhanced database statistics with complete, properly formatted output\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        total_queries = len(df)\n",
        "        completed = len(df[df['Status'] == 'completed'])\n",
        "        pending = len(df[df['Status'] == 'pending'])\n",
        "        errors = len(df[df['Status'] == 'error'])\n",
        "\n",
        "        # Enhanced routing statistics\n",
        "        routing_stats = \"\"\n",
        "        synthesis_stats = \"\"\n",
        "        dictionary_stats = \"\"\n",
        "\n",
        "        if 'Routing_Type' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "\n",
        "            # Routing type distribution\n",
        "            if 'Routing_Type' in completed_df.columns:\n",
        "                routing_dist = completed_df['Routing_Type'].value_counts()\n",
        "                routing_stats = f\"\\nüìä Routing Distribution:\"\n",
        "                for route_type, count in routing_dist.items():\n",
        "                    if pd.notna(route_type):  # Skip NaN values\n",
        "                        routing_stats += f\"\\n   - {route_type}: {count}\"\n",
        "\n",
        "            # Routing method breakdown\n",
        "            if 'Routing_Method' in completed_df.columns:\n",
        "                method_dist = completed_df['Routing_Method'].value_counts()\n",
        "                routing_stats += f\"\\nüîç Method Breakdown:\"\n",
        "                for method, count in method_dist.items():\n",
        "                    if pd.notna(method):  # Skip NaN values\n",
        "                        routing_stats += f\"\\n   - {method}: {count}\"\n",
        "\n",
        "            # Dictionary match analysis\n",
        "            if 'Dictionary_Match' in completed_df.columns:\n",
        "                dict_matches = completed_df['Dictionary_Match'].value_counts()\n",
        "                dictionary_stats = f\"\\nüîë Dictionary Matches:\"\n",
        "                for match, count in dict_matches.items():\n",
        "                    if pd.notna(match) and match.strip():  # Skip NaN and empty values\n",
        "                        dictionary_stats += f\"\\n   - '{match}': {count}\"\n",
        "\n",
        "            # Synthesis method distribution\n",
        "            if 'Synthesis_Method' in completed_df.columns:\n",
        "                synthesis_dist = completed_df['Synthesis_Method'].value_counts()\n",
        "                synthesis_stats = f\"\\n‚öôÔ∏è Synthesis Methods:\"\n",
        "                for method, count in synthesis_dist.items():\n",
        "                    if pd.notna(method):  # Skip NaN values\n",
        "                        synthesis_stats += f\"\\n   - {method}: {count}\"\n",
        "\n",
        "        # Processing time analysis\n",
        "        time_stats = \"\"\n",
        "        if 'Processing_Time_Seconds' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "            processing_times = completed_df['Processing_Time_Seconds'].dropna()\n",
        "            if not processing_times.empty:\n",
        "                avg_time = processing_times.mean()\n",
        "                min_time = processing_times.min()\n",
        "                max_time = processing_times.max()\n",
        "                time_stats = f\"\\n‚è±Ô∏è Processing Times:\\n   - Average: {avg_time:.2f}s\\n   - Minimum: {min_time:.2f}s\\n   - Maximum: {max_time:.2f}s\"\n",
        "\n",
        "        # Topic analysis\n",
        "        topic_stats = \"\"\n",
        "        if 'Identified_Topic' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "            topic_dist = completed_df['Identified_Topic'].value_counts()\n",
        "            topic_stats = f\"\\nüè∑Ô∏è Topics Identified:\"\n",
        "            for topic, count in topic_dist.head(5).items():  # Top 5 topics\n",
        "                if pd.notna(topic):\n",
        "                    topic_stats += f\"\\n   - {topic}: {count}\"\n",
        "\n",
        "        # FIXED: Complete stats output with proper formatting\n",
        "        stats = f\"\"\"\n",
        "üìà COMPREHENSIVE DATABASE STATISTICS\n",
        "=====================================\n",
        "üìä Query Status:\n",
        "  - Total Queries: {total_queries}\n",
        "  - Completed: {completed}\n",
        "  - Pending: {pending}\n",
        "  - Errors: {errors}\n",
        "{time_stats}{routing_stats}{dictionary_stats}{synthesis_stats}{topic_stats}\n",
        "\n",
        "üóÇÔ∏è Database Schema: {len(df.columns)} columns\n",
        "üìã Available Columns: {', '.join(df.columns)}\n",
        "\n",
        "üìã Recent Queries (Last 5):\n",
        "\"\"\"\n",
        "\n",
        "        if not df.empty:\n",
        "            # FIXED: Better column selection and handling of NaN values\n",
        "            display_columns = ['ID', 'Original_Query', 'Routing_Type', 'Synthesis_Method', 'Identified_Topic', 'Status', 'Timestamp']\n",
        "            existing_columns = [col for col in display_columns if col in df.columns]\n",
        "\n",
        "            recent = df.tail(5)[existing_columns].copy()\n",
        "\n",
        "            # Clean up NaN values for display\n",
        "            for col in recent.columns:\n",
        "                if col not in ['ID']:  # Don't modify ID column\n",
        "                    recent[col] = recent[col].fillna('Not Set')\n",
        "\n",
        "            # Truncate long text for better display\n",
        "            if 'Original_Query' in recent.columns:\n",
        "                recent['Original_Query'] = recent['Original_Query'].apply(\n",
        "                    lambda x: str(x)[:50] + '...' if len(str(x)) > 50 else str(x)\n",
        "                )\n",
        "\n",
        "            stats += recent.to_string(index=False, max_colwidth=50)\n",
        "\n",
        "        # FIXED: Add summary at the end\n",
        "        stats += f\"\"\"\n",
        "\n",
        "üìä SUMMARY:\n",
        "- Database is functioning properly with {total_queries} total queries\n",
        "- {completed} queries have been successfully processed\n",
        "- Enhanced tracking is capturing routing decisions and topic identification\n",
        "- Latest query processing time: {processing_times.iloc[-1]:.2f}s (if available)\n",
        "\"\"\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"\"\"\n",
        "‚ùå ERROR READING DATABASE: {str(e)}\n",
        "\n",
        "üîß TROUBLESHOOTING:\n",
        "- Check if 'cola_database.csv' exists in the current directory\n",
        "- Verify database schema is correct\n",
        "- Ensure no file permissions issues\n",
        "\n",
        "üìã Available files: {', '.join([f for f in os.listdir('.') if f.endswith('.csv')])}\n",
        "\"\"\"\n",
        "        return error_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcPsHj_Yfhvt"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt):\n",
        "    max_retries = 100\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "          messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "          response = client.chat.completions.create(\n",
        "              model=\"llama-3-8b-instruct\",\n",
        "              messages=messages,\n",
        "              temperature=0\n",
        "            )\n",
        "          return response.choices[0].message.content\n",
        "        except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzsEcMKwffxy"
      },
      "outputs": [],
      "source": [
        "def get_completion_with_role(role, instruction, content):\n",
        "    max_retries = 100\n",
        "    for i in range(max_retries):\n",
        "      try:\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{instruction}\\n{content}\"}\n",
        "        ]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "          )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "      except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG3Ue-oGOO1J"
      },
      "outputs": [],
      "source": [
        "def generate_intent_options(original_query):\n",
        "    \"\"\"\n",
        "    Generate 3 most likely DOMAIN/TOPIC interpretations for the original query.\n",
        "    This disambiguates between completely different subjects, not just different angles.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Query: \"{original_query}\"\n",
        "\n",
        "        CRITICAL: I need you to identify if this query has AMBIGUOUS WORDS that could mean completely different things.\n",
        "\n",
        "        Look for words that could be:\n",
        "        - Programming language vs. place vs. other meanings (Java, Python, Ruby, etc.)\n",
        "        - Company vs. fruit vs. other (Apple, Orange, etc.)\n",
        "        - Person vs. place vs. concept (Tesla, Darwin, etc.)\n",
        "        - Multiple different meanings entirely\n",
        "\n",
        "        If you find ambiguous words, give me 3 DIFFERENT DOMAINS/SUBJECTS.\n",
        "        If no ambiguous words, give me 3 different CONTEXTS for the same topic.\n",
        "\n",
        "        WRONG (all same domain):\n",
        "        - Java web development features\n",
        "        - Java mobile app development\n",
        "        - Java enterprise applications\n",
        "\n",
        "        RIGHT (different domains):\n",
        "        - Java (programming language)\n",
        "        - Java (Indonesian island)\n",
        "        - Java (coffee culture)\n",
        "\n",
        "        Format: Topic (context)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Clean and parse the response - be more aggressive about filtering\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        # More comprehensive filtering for instruction text\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"interpretations\",\n",
        "            \"different meanings\", \"ambiguous\", \"critical\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines that contain instruction-like phrases\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets more aggressively\n",
        "            cleaned_line = line\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\‚Ä¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            # Only keep lines that look like actual topic options (should contain parentheses ideally)\n",
        "            if cleaned_line and len(cleaned_line) > 3 and not any(phrase in cleaned_line.lower() for phrase in skip_phrases):\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # If we don't get good results, create manual disambiguation for common terms\n",
        "        query_lower = original_query.lower()\n",
        "\n",
        "        # Check for common ambiguous terms\n",
        "        if 'java' in query_lower:\n",
        "            return [\n",
        "                \"Java (programming language)\",\n",
        "                \"Java (Indonesian island)\",\n",
        "                \"Java (coffee culture)\"\n",
        "            ]\n",
        "        elif 'python' in query_lower:\n",
        "            return [\n",
        "                \"Python (programming language)\",\n",
        "                \"Python (snake species)\",\n",
        "                \"Python (Monty Python comedy)\"\n",
        "            ]\n",
        "        elif 'tesla' in query_lower:\n",
        "            return [\n",
        "                \"Tesla (car company)\",\n",
        "                \"Tesla (Nikola Tesla scientist)\",\n",
        "                \"Tesla (band/music)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic contextual fallback\n",
        "            return [\n",
        "                f\"Technical/professional information about {original_query}\",\n",
        "                f\"General educational information about {original_query}\",\n",
        "                f\"Practical applications of {original_query}\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating intent options: {e}\")\n",
        "        return [\n",
        "            f\"Technical information about {original_query}\",\n",
        "            f\"General information about {original_query}\",\n",
        "            f\"Practical guide for {original_query}\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yF23ehfOO1J"
      },
      "outputs": [],
      "source": [
        "def show_intent_options_clean(chatbot_history, state, options):\n",
        "    \"\"\"Enhanced to show 'Other' option without duplicate messages\"\"\"\n",
        "    options_text = \"üéØ **Please clarify your intent:**\\n\\n\"\n",
        "    options_text += \"Select the option that best matches what you're looking for:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** üîß **Other** - Specify your own topic/domain\\n\\n\"\n",
        "    options_text += \"üëá **Click the corresponding Option button below to proceed.**\"\n",
        "\n",
        "    intent_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    updated_history = chatbot_history + [intent_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap0NHqhHOO1J"
      },
      "outputs": [],
      "source": [
        "def generate_answer_type_options(query, selected_topic_intent):\n",
        "    \"\"\"\n",
        "    Generate answer type options based on the query and selected topic intent.\n",
        "    This helps clarify what kind of response the user is looking for.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Query: \"{query}\"\n",
        "        Selected Topic: \"{selected_topic_intent}\"\n",
        "\n",
        "        The user wants to know about this topic. What type of answer would be most helpful?\n",
        "\n",
        "        Generate 3 different ANSWER TYPE options that would be appropriate for this query:\n",
        "\n",
        "        Consider these categories:\n",
        "        - Informative (detailed explanation, facts, background information)\n",
        "        - Practical Tips (actionable advice, how-to guidance, steps to follow)\n",
        "        - Basic Overview (simple introduction, key points, beginner-friendly)\n",
        "        - Expert Analysis (in-depth professional perspective, technical details)\n",
        "        - Comparison/Evaluation (pros/cons, alternatives, recommendations)\n",
        "        - Problem-Solving (solutions, troubleshooting, addressing specific issues)\n",
        "\n",
        "        Format each option as: \"Answer Type (brief description)\"\n",
        "\n",
        "        Examples:\n",
        "        - Informative (comprehensive background and facts)\n",
        "        - Practical Tips (step-by-step actionable guidance)\n",
        "        - Basic Overview (simple introduction for beginners)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Parse response similar to intent options\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"options\",\n",
        "            \"different types\", \"answer type\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\‚Ä¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            if cleaned_line and len(cleaned_line) > 3:\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # Fallback options based on query analysis\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(word in query_lower for word in ['how to', 'steps', 'guide', 'tutorial']):\n",
        "            return [\n",
        "                \"Practical Tips (step-by-step actionable guidance)\",\n",
        "                \"Informative (detailed explanation and background)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\"\n",
        "            ]\n",
        "        elif any(word in query_lower for word in ['what is', 'explain', 'about']):\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\",\n",
        "                \"Expert Analysis (in-depth professional perspective)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic fallback\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Practical Tips (actionable advice and guidance)\",\n",
        "                \"Basic Overview (simple introduction and key points)\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer type options: {e}\")\n",
        "        return [\n",
        "            \"Informative (comprehensive background and facts)\",\n",
        "            \"Practical Tips (actionable advice and guidance)\",\n",
        "            \"Basic Overview (simple introduction and key points)\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMUV0542OO1K"
      },
      "outputs": [],
      "source": [
        "def show_answer_type_options_clean(chatbot_history, state, options):\n",
        "    \"\"\"CORRECT: Show answer type options - adds as new bubble to existing history\"\"\"\n",
        "    options_text = \"üìù **What type of answer do you need?**\\n\\n\"\n",
        "    options_text += \"Choose the response format that best fits your needs:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** üîß **Other** - Specify your preferred answer format\\n\\n\"\n",
        "    options_text += \"üëá **Click the corresponding Option button below.**\"\n",
        "\n",
        "    # SIMPLE: Just add answer type options as NEW bubble to existing history\n",
        "    # This preserves all previous conversation including user queries\n",
        "    answer_type_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    updated_history = chatbot_history + [answer_type_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO0R8PWJOO1K"
      },
      "outputs": [],
      "source": [
        "def intelligent_routing_agent(original_query, selected_answer_type, rewritten_query=None):\n",
        "    \"\"\"Intelligent agent that analyzes the query when dictionary routing fails\"\"\"\n",
        "\n",
        "    if selected_answer_type is None:\n",
        "        print(\"WARNING: selected_answer_type is None, using LLM to analyze query intent\")\n",
        "        analysis_target = original_query\n",
        "        answer_type_description = \"Unknown - need to analyze query intent\"\n",
        "    else:\n",
        "        analysis_target = f\"Query: {original_query}\\\\nAnswer Type Requested: {selected_answer_type}\"\n",
        "        answer_type_description = selected_answer_type\n",
        "\n",
        "    routing_prompt = f\"\"\"You are an intelligent routing agent for a collaborative analysis system. Your job is to analyze user queries and determine the best synthesis approach.\n",
        "\n",
        "            AVAILABLE SYNTHESIS METHODS:\n",
        "            1. **DECISION_MAKING**: Use when the user wants evaluation, comparison, recommendations, advice, or needs to make a choice between options.\n",
        "\n",
        "            2. **SOLUTION_FOCUSED**: Use when the user wants practical tips, step-by-step solutions, implementation guidance, troubleshooting, or actionable advice.\n",
        "\n",
        "            3. **INFORMATIONAL**: Use when the user wants facts, explanations, overviews, background information, or educational content.\n",
        "\n",
        "            ANALYSIS TARGET:\n",
        "            {analysis_target}\n",
        "\n",
        "            RESPOND WITH ONLY ONE WORD: \"DECISION_MAKING\", \"SOLUTION_FOCUSED\", or \"INFORMATIONAL\"\n",
        "\n",
        "            Think about what the user really wants as an outcome from their query.\"\"\"\n",
        "\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": routing_prompt}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        routing_decision = response.choices[0].message.content.strip().upper()\n",
        "\n",
        "        valid_methods = [\"DECISION_MAKING\", \"SOLUTION_FOCUSED\", \"INFORMATIONAL\"]\n",
        "        if routing_decision in valid_methods:\n",
        "            method = routing_decision.lower()\n",
        "            print(f\"ü§ñ INTELLIGENT ROUTING: LLM determined '{method}' based on query analysis\")\n",
        "            return method\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è LLM returned invalid method '{routing_decision}', using fallback analysis\")\n",
        "            return fallback_intelligent_analysis(original_query, selected_answer_type)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in intelligent routing agent: {e}\")\n",
        "        return fallback_intelligent_analysis(original_query, selected_answer_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgx1DEP5OO1K"
      },
      "outputs": [],
      "source": [
        "def fallback_intelligent_analysis(original_query, selected_answer_type):\n",
        "    \"\"\"Fallback intelligent analysis using keyword detection when LLM fails\"\"\"\n",
        "    query_lower = original_query.lower()\n",
        "    answer_type_lower = (selected_answer_type or \"\").lower()\n",
        "    combined_text = f\"{query_lower} {answer_type_lower}\"\n",
        "\n",
        "    # Decision-making indicators\n",
        "    decision_keywords = [\n",
        "        \"should i\", \"which\", \"better\", \"best\", \"recommend\", \"choose\", \"decide\",\n",
        "        \"compare\", \"versus\", \"vs\", \"pros and cons\", \"advantages\", \"disadvantages\",\n",
        "        \"worth it\", \"advice\", \"suggest\", \"opinion\", \"prefer\", \"evaluation\"\n",
        "    ]\n",
        "\n",
        "    # Solution-focused indicators\n",
        "    solution_keywords = [\n",
        "        \"how to\", \"steps\", \"guide\", \"tutorial\", \"solve\", \"fix\", \"implement\",\n",
        "        \"create\", \"build\", \"make\", \"do\", \"achieve\", \"accomplish\", \"tips\",\n",
        "        \"practical\", \"actionable\", \"process\", \"method\", \"technique\"\n",
        "    ]\n",
        "\n",
        "    decision_score = sum(1 for keyword in decision_keywords if keyword in combined_text)\n",
        "    solution_score = sum(1 for keyword in solution_keywords if keyword in combined_text)\n",
        "\n",
        "    if decision_score > solution_score and decision_score > 0:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Decision-making intent detected (score: {decision_score})\")\n",
        "        return \"DECISION_MAKING\"\n",
        "    elif solution_score > 0:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Solution-focused intent detected (score: {solution_score})\")\n",
        "        return \"SOLUTION_FOCUSED\"\n",
        "    else:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Informational intent (default)\")\n",
        "        return \"INFORMATIONAL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtoWzLC7OO1K"
      },
      "outputs": [],
      "source": [
        "def process_selected_answer_type_intelligent_dynamic(selected_answer_type, selected_topic_intent, original_query, query_id, chatbot_history, state):\n",
        "    \"\"\"Enhanced COLA processing with dynamic UI control - clean version\"\"\"\n",
        "    try:\n",
        "        # DON'T add another confirmation message - use the existing one\n",
        "        # The confirmation is already in chatbot_history from the previous step\n",
        "\n",
        "        # Update state with processing info\n",
        "        if isinstance(state, dict):\n",
        "            state[\"processing_query_id\"] = query_id\n",
        "            state[\"selected_topic_intent\"] = selected_topic_intent\n",
        "            state[\"processing\"] = True\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "        # Process with intelligent routing\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            original_query,\n",
        "            selected_topic_intent,\n",
        "            selected_answer_type,\n",
        "            query_id,\n",
        "            state\n",
        "        )\n",
        "\n",
        "        # FIXED: Replace the confirmation messages with just the final result\n",
        "        # Remove the last two messages (custom selection + processing messages)\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else []\n",
        "\n",
        "        # Add only the final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        # Reset processing state\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield final_history, state, \"\", gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå **Error processing selected answer type:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "\n",
        "        # Clean up confirmation messages and show error\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else chatbot_history\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        # Reset state on error\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield error_history, state, \"\", gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMZfyVBGOO1K"
      },
      "outputs": [],
      "source": [
        "def rewrite_query_with_dual_intent(original_query, selected_topic, selected_answer_type, state):\n",
        "    \"\"\"Rewrite query and store in state\"\"\"\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "    answer_type_description = selected_answer_type.split('(')[1].strip(')') if '(' in selected_answer_type else \"\"\n",
        "\n",
        "    instruction = f\"\"\"You have an original user query, their selected topic/domain, and their desired answer type.\n",
        "\n",
        "    Your task: Rewrite the query to be more specific and actionable based on these clarifications.\n",
        "\n",
        "    Original query: \"{original_query}\"\n",
        "    Selected topic/domain: \"{selected_topic}\"\n",
        "    Desired answer type: \"{answer_type_main}\"\n",
        "    Answer type context: \"{answer_type_description}\"\n",
        "\n",
        "    Guidelines:\n",
        "    1. Keep the core intent of the original query\n",
        "    2. Make it more specific to the selected topic, as the user has already specified to which term they refer to. DO NOT CHANGE TOPICS, selected topic: {selected_topic}.\n",
        "    3. Frame it to expect the desired answer type\n",
        "    4. Make it actionable and clear\n",
        "    5. Don't change the fundamental question, remain the query in the specified selected topic ({selected_topic})\n",
        "\n",
        "    Return only the rewritten query, nothing else.\"\"\"\n",
        "\n",
        "    working_query = get_completion(instruction)\n",
        "\n",
        "    # Clean up the response\n",
        "    working_query = working_query.strip()\n",
        "    if working_query.startswith('\"') and working_query.endswith('\"'):\n",
        "        working_query = working_query[1:-1]\n",
        "\n",
        "    # Store in state for later use\n",
        "    state[\"working_query\"] = working_query\n",
        "\n",
        "    return working_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stOoLlhzOO1K"
      },
      "outputs": [],
      "source": [
        "#GET ROLES\n",
        "def get_roles(query, topic):\n",
        "    max_retries = 100\n",
        "    prompt_roles = f\"\"\"You are a Recruiting Manager. The user has clarified they want to know about: \"{topic}\"\n",
        "\n",
        "                        IMPORTANT CONSTRAINTS:\n",
        "                        - Focus ONLY on the clarified topic: \"{topic}\"\n",
        "                        - IGNORE any ambiguous terms from the original query\n",
        "                        - All 3 expert roles must be specialists within the scope of: \"{topic}\"\n",
        "                        - DO NOT go outside this specific domain\n",
        "\n",
        "                        Provide 3 different expert roles who specialize in \"{topic}\" and can analyze the question from different perspectives within this domain.\n",
        "\n",
        "                        The 3 roles should have complementary expertise areas within \"{topic}\".\n",
        "\n",
        "                        Return ONLY a Python list in this exact format:\n",
        "                        [\"{topic}\", \"expert role 1\", \"expert role 2\", \"expert role 3\"]\n",
        "\n",
        "                        No explanations, no other text, just the list.\"\"\"\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt_roles}]\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"llama-3-8b-instruct\",\n",
        "                messages=messages,\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # CRITICAL FIX: Parse string into actual list\n",
        "            if response_text.startswith('[') and response_text.endswith(']'):\n",
        "                definition_list = eval(response_text)  # Convert string to list\n",
        "                if isinstance(definition_list, list) and len(definition_list) >= 4:\n",
        "                    return definition_list\n",
        "\n",
        "            raise ValueError(\"Invalid response format\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                # Fallback\n",
        "                return [\n",
        "                    \"General Knowledge\",\n",
        "                    \"Subject Matter Expert\",\n",
        "                    \"Technical Specialist\",\n",
        "                    \"End User Analyst\"\n",
        "                ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHx3lN9TOO1K"
      },
      "outputs": [],
      "source": [
        "def route_synthesis_by_answer_type_intelligent(selected_answer_type, original_query=None, rewritten_query=None):\n",
        "    \"\"\"ENHANCED: Now returns both routing decision AND tracking info\"\"\"\n",
        "\n",
        "    # Call the new comprehensive tracking function\n",
        "    synthesis_method, routing_info = route_synthesis_with_comprehensive_tracking(\n",
        "        selected_answer_type, original_query, rewritten_query\n",
        "    )\n",
        "\n",
        "    # For backward compatibility, return just the synthesis_method\n",
        "    # (Your existing code will still work)\n",
        "    return synthesis_method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtT_EETPOO1K"
      },
      "outputs": [],
      "source": [
        "def route_synthesis_with_comprehensive_tracking(selected_answer_type, original_query=None, rewritten_query=None, query_id=None):\n",
        "    \"\"\"NEW: Enhanced routing with detailed tracking of decision process\"\"\"\n",
        "\n",
        "    # Initialize tracking info\n",
        "    routing_info = {\n",
        "        'routing_type': 'UNKNOWN',\n",
        "        'routing_method': 'fallback',\n",
        "        'dictionary_match': '',\n",
        "        'match_found': False,\n",
        "        'answer_type_processed': selected_answer_type or 'None'\n",
        "    }\n",
        "\n",
        "    if not selected_answer_type or len(selected_answer_type.strip()) == 0:\n",
        "        print(\"‚ö†Ô∏è No answer type provided - routing to LLM analysis\")\n",
        "        routing_info.update({\n",
        "            'routing_type': 'LLM',\n",
        "            'routing_method': 'no_answer_type_provided',\n",
        "            'match_found': False\n",
        "        })\n",
        "        result = intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "        return result, routing_info\n",
        "\n",
        "    # Extract main answer type\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip().lower()\n",
        "    print(f\"üîç Processing answer type: '{answer_type_main}'\")\n",
        "\n",
        "    try:\n",
        "        # Comprehensive routing dictionary (same as before but with tracking)\n",
        "        synthesis_routing = {\n",
        "            # Decision-making patterns\n",
        "            'comparison': 'DECISION_MAKING',\n",
        "            'recommendation': 'DECISION_MAKING',\n",
        "            'advice': 'DECISION_MAKING',\n",
        "            'evaluation': 'DECISION_MAKING',\n",
        "            'assessment': 'DECISION_MAKING',\n",
        "            'pros and cons': 'DECISION_MAKING',\n",
        "            'which is better': 'DECISION_MAKING',\n",
        "            'should i': 'DECISION_MAKING',\n",
        "            'best option': 'DECISION_MAKING',\n",
        "            'choose': 'DECISION_MAKING',\n",
        "            'decision': 'DECISION_MAKING',\n",
        "            'versus': 'DECISION_MAKING',\n",
        "            'vs': 'DECISION_MAKING',\n",
        "\n",
        "            # Solution-focused patterns\n",
        "            'practical tips': 'SOLUTION_FOCUSED',\n",
        "            'problem-solving': 'SOLUTION_FOCUSED',\n",
        "            'how to': 'SOLUTION_FOCUSED',\n",
        "            'step by step': 'SOLUTION_FOCUSED',\n",
        "            'step-by-step tutorial': 'SOLUTION_FOCUSED',\n",
        "            'guide': 'SOLUTION_FOCUSED',\n",
        "            'tutorial': 'SOLUTION_FOCUSED',\n",
        "            'implementation': 'SOLUTION_FOCUSED',\n",
        "            'troubleshooting': 'SOLUTION_FOCUSED',\n",
        "            'fix': 'SOLUTION_FOCUSED',\n",
        "            'solve': 'SOLUTION_FOCUSED',\n",
        "            'create': 'SOLUTION_FOCUSED',\n",
        "            'build': 'SOLUTION_FOCUSED',\n",
        "            'actionable': 'SOLUTION_FOCUSED',\n",
        "\n",
        "            # Informational patterns\n",
        "            'expert analysis': 'INFORMATIONAL',\n",
        "            'informative': 'INFORMATIONAL',\n",
        "            'basic overview': 'INFORMATIONAL',\n",
        "            'explanation': 'INFORMATIONAL',\n",
        "            'background': 'INFORMATIONAL',\n",
        "            'what is': 'INFORMATIONAL',\n",
        "            'overview': 'INFORMATIONAL',\n",
        "            'summary': 'INFORMATIONAL',\n",
        "            'details': 'INFORMATIONAL',\n",
        "            'information': 'INFORMATIONAL',\n",
        "            'facts': 'INFORMATIONAL',\n",
        "            'learn': 'INFORMATIONAL',\n",
        "            'understand': 'INFORMATIONAL'\n",
        "        }\n",
        "\n",
        "        # Exact match check\n",
        "        if answer_type_main in synthesis_routing:\n",
        "            method = synthesis_routing[answer_type_main]\n",
        "            print(f\"‚úÖ EXACT DICTIONARY MATCH: '{answer_type_main}' ‚Üí '{method}'\")\n",
        "            routing_info.update({\n",
        "                'routing_type': 'DICTIONARY',\n",
        "                'routing_method': 'exact_match',\n",
        "                'dictionary_match': answer_type_main,\n",
        "                'match_found': True\n",
        "            })\n",
        "            return method.lower(), routing_info\n",
        "\n",
        "        # Partial match check\n",
        "        for key, method in synthesis_routing.items():\n",
        "            if key in answer_type_main or answer_type_main in key:\n",
        "                print(f\"‚úÖ PARTIAL DICTIONARY MATCH: '{answer_type_main}' ‚Üî '{key}' ‚Üí '{method}'\")\n",
        "                routing_info.update({\n",
        "                    'routing_type': 'DICTIONARY',\n",
        "                    'routing_method': 'partial_match',\n",
        "                    'dictionary_match': key,\n",
        "                    'match_found': True\n",
        "                })\n",
        "                return method.lower(), routing_info\n",
        "\n",
        "        # No dictionary match found\n",
        "        print(f\"‚ùå NO DICTIONARY MATCH for '{answer_type_main}'\")\n",
        "        print(\"ü§ñ Routing to LLM for intelligent analysis...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dictionary lookup error: {e}\")\n",
        "\n",
        "    # LLM routing fallback\n",
        "    routing_info.update({\n",
        "        'routing_type': 'LLM',\n",
        "        'routing_method': 'dictionary_fallback',\n",
        "        'match_found': False\n",
        "    })\n",
        "\n",
        "    result = intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "    return result, routing_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu5xvck9fjdz"
      },
      "outputs": [],
      "source": [
        "def local_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced local analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "\n",
        "    # Extract the main answer type from the selection\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with deep knowledge about {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    As a {role}, provide your professional analysis addressing this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your response should reflect the expertise and viewpoint that defines your role as a {role} while following the requested answer format.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def expert_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced expert analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} specializing in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Provide your professional expert analysis of this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should reflect the authority and comprehensive understanding that comes from being a recognized {role} in this domain.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def user_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced user analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with expertise in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Analyze this query from your specialized perspective as a {role}.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should complement other expert perspectives while offering the distinct value that only a {role} can provide.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_YnhbkFOO1K"
      },
      "outputs": [],
      "source": [
        "def get_answer_type_instructions(answer_type):\n",
        "    \"\"\"Get specific instructions based on answer type\"\"\"\n",
        "    instructions = {\n",
        "        \"Informative\": \"\"\"\n",
        "        - Provide comprehensive background information and facts\n",
        "        - Include detailed explanations and context\n",
        "        - Cover multiple aspects of the topic\n",
        "        - Use evidence and examples to support points\n",
        "        - Structure information clearly and logically\"\"\",\n",
        "\n",
        "        \"Practical Tips\": \"\"\"\n",
        "        - Focus on actionable advice and guidance\n",
        "        - Provide step-by-step instructions where applicable\n",
        "        - Include specific recommendations and best practices\n",
        "        - Emphasize what the user can actually do\n",
        "        - Make suggestions concrete and implementable\"\"\",\n",
        "\n",
        "        \"Basic Overview\": \"\"\"\n",
        "        - Keep explanations simple and accessible\n",
        "        - Focus on key points and essential information\n",
        "        - Avoid technical jargon or complex details\n",
        "        - Provide a clear, easy-to-understand introduction\n",
        "        - Structure information in a beginner-friendly way\"\"\",\n",
        "\n",
        "        \"Expert Analysis\": \"\"\"\n",
        "        - Provide in-depth professional perspective\n",
        "        - Include technical details and advanced insights\n",
        "        - Reference industry standards and best practices\n",
        "        - Demonstrate specialized knowledge and expertise\n",
        "        - Address complex aspects and nuances\"\"\",\n",
        "\n",
        "        \"Comparison\": \"\"\"\n",
        "        - Present pros and cons clearly\n",
        "        - Compare different options or approaches\n",
        "        - Provide balanced evaluation of alternatives\n",
        "        - Include recommendations based on comparison\n",
        "        - Help user understand trade-offs\"\"\",\n",
        "\n",
        "        \"Problem-Solving\": \"\"\"\n",
        "        - Focus on solutions and troubleshooting\n",
        "        - Address specific issues and challenges\n",
        "        - Provide practical problem-solving approaches\n",
        "        - Include preventive measures where applicable\n",
        "        - Emphasize resolution strategies\"\"\"\n",
        "    }\n",
        "\n",
        "    return instructions.get(answer_type, instructions[\"Informative\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUhCQGhaOO1K"
      },
      "outputs": [],
      "source": [
        "def stance_analysis_enhanced(query, ling_response, expert_response, user_response, topic, stance, answer_type, state=None):\n",
        "    \"\"\"Enhanced stance analysis that considers answer type\"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "\n",
        "    stance_context = {\n",
        "        \"positive\": \"highly beneficial, well-founded, and strongly recommended\",\n",
        "        \"negative\": \"problematic, risky, or not advisable\"\n",
        "    }\n",
        "\n",
        "    stance_description = stance_context.get(stance, stance)\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are conducting stance detection analysis for collaborative decision-making.\n",
        "\n",
        "    Original Query: '''{query}'''\n",
        "    Topic: {topic}\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR STANCE: You believe the approaches, recommendations, or solutions presented in response to this query are {stance_description} for the user's situation regarding {topic}.\n",
        "\n",
        "    IMPORTANT: Your argument should be formatted as {answer_type_main.upper()} since that's what the user requested.\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    TASK:\n",
        "    1. **Analyze all three expert perspectives** through your {stance} lens\n",
        "    2. **Extract supporting evidence** that supports your {stance} position\n",
        "    3. **Build your argument** using evidence while following the {answer_type_main} format\n",
        "\n",
        "    Present your {stance} argument with specific evidence from the expert analyses, formatted according to the user's requested answer type.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXBkE_I0OO1K"
      },
      "outputs": [],
      "source": [
        "def final_judgement(query, favor_response, against_response, topic):\n",
        "    \"\"\"\n",
        "    Enhanced final judgement that synthesizes collaborative analysis\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query. This means:\n",
        "\n",
        "    1. **Evaluate evidence quality**: Assess the strength and credibility of arguments from both sides\n",
        "    2. **Consider user context**: Focus on what would be most beneficial for someone asking this specific query\n",
        "    3. **Balance perspectives**: Integrate the strongest insights from both positive and negative analyses\n",
        "    4. **Provide actionable guidance**: Give the user clear, practical direction\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Deliver a comprehensive yet concise response\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Focus on practical value for the user\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Present as the authoritative answer to their query\n",
        "    - Give a brief, practical response (1-2 paragraphs).\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7alYPKN7fnYQ"
      },
      "outputs": [],
      "source": [
        "def final_judgement_enhanced(query, favor_response, against_response, topic, answer_type):\n",
        "    \"\"\"Enhanced final judgement that considers answer type\"\"\"\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "    REQUESTED ANSWER TYPE: {answer_type}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query.\n",
        "\n",
        "    CRITICAL: Format your response as {answer_type_main.upper()} as specifically requested by the user:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Follow the {answer_type_main} format strictly\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Focus on practical value for the user\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Present as the authoritative answer to their query\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process, delivered in exactly the format the user requested.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InbfNm_XOO1L"
      },
      "outputs": [],
      "source": [
        "def summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    \"\"\"\n",
        "    Summary synthesis agent - extracts and synthesizes key facts without forcing recommendations.\n",
        "    Used for informational answer types: Informative, Basic Overview, Expert Analysis\n",
        "    \"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Summary Synthesis Agent. Your role is to extract and synthesize the most relevant and important information from multiple expert analyses.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a comprehensive, factual response that directly answers the user's query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SYNTHESIS GUIDELINES:\n",
        "    1. **Extract key facts**: Pull the most important information from all three experts\n",
        "    2. **Identify consensus**: Where experts agree, present this as reliable information\n",
        "    3. **Note different perspectives**: Where experts offer different viewpoints, present both\n",
        "    4. **Stay factual**: Focus on information, not recommendations\n",
        "    5. **Be comprehensive**: Cover all important aspects mentioned by the experts\n",
        "    6. **Maintain objectivity**: Present information neutrally without bias\n",
        "    7. **Structure clearly**: Organize information logically for easy understanding\n",
        "\n",
        "    DO NOT:\n",
        "    - Force recommendations when the user wants information\n",
        "    - Create artificial pros/cons lists for factual queries\n",
        "    - Add opinions where experts provided facts\n",
        "    - Turn factual content into advice\n",
        "\n",
        "    Provide a well-structured synthesis that gives the user exactly the type of information they requested.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDZgeZk_OO1L"
      },
      "outputs": [],
      "source": [
        "def solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    \"\"\"\n",
        "    Solution synthesis agent - focuses on practical solutions and implementation.\n",
        "    Used for solution-focused answer types: Practical Tips, Problem-Solving\n",
        "    \"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Solution Synthesis Agent. Your role is to synthesize expert analyses into practical, actionable solutions.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a practical, solution-focused response.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SOLUTION GUIDELINES:\n",
        "    1. **Identify the core need**: What is the user trying to accomplish?\n",
        "    2. **Extract actionable advice**: Pull practical steps and recommendations from experts\n",
        "    3. **Prioritize solutions**: Present the most effective approaches first\n",
        "    4. **Consider implementation**: Include practical considerations for execution\n",
        "    5. **Address potential challenges**: Note important limitations or considerations\n",
        "    6. **Provide clear guidance**: Make recommendations specific and actionable\n",
        "    7. **Focus on outcomes**: Help user understand what success looks like\n",
        "\n",
        "    Focus on giving the user a clear path forward based on the expert analyses.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SIbeQC9hDI",
        "outputId": "b39b748d-ef4c-4c9f-e94e-6c772993f3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private LLM Response: AI \"learns\" by:\n",
            "\n",
            "1. Gathering Data\n",
            "2. Identifying Patterns\n",
            "3. Making Predictions\n",
            "4. Adjusting and Improving\n"
          ]
        }
      ],
      "source": [
        "# Test your private LLM\n",
        "def test_private_llm():\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain how AI works in a few words\"}\n",
        "        ]\n",
        "    )\n",
        "    print(\"Private LLM Response:\", response.choices[0].message.content)\n",
        "\n",
        "test_private_llm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHVfzy8fsM3"
      },
      "outputs": [],
      "source": [
        "search_tool = GoogleSearchAPIWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2NL4wjBOO1L"
      },
      "outputs": [],
      "source": [
        "def format_response_with_sources(enhanced_response, source_urls):\n",
        "    \"\"\"\n",
        "    IMPROVED: Format the response with clear, visible source links\n",
        "    \"\"\"\n",
        "    if not source_urls:\n",
        "        sources_section = \"\\n\\n---\\n\\nüìö **Note:** Search was performed but specific source URLs could not be extracted. Information has been validated against current web content.\"\n",
        "    else:\n",
        "        sources_section = \"\\n\\n---\\n\\nüìö **Sources Used for This Update:**\\n\\n\"\n",
        "\n",
        "        for i, url in enumerate(source_urls, 1):\n",
        "            try:\n",
        "                from urllib.parse import urlparse\n",
        "                parsed = urlparse(url)\n",
        "                domain = parsed.netloc if parsed.netloc else parsed.path\n",
        "                if domain.startswith('www.'):\n",
        "                    domain = domain[4:]\n",
        "\n",
        "                # Format as clickable link\n",
        "                sources_section += f\"{i}. [{domain}]({url})\\n\"\n",
        "            except:\n",
        "                sources_section += f\"{i}. {url}\\n\"\n",
        "\n",
        "        sources_section += \"\\n*Click on the links above to visit the sources.*\"\n",
        "\n",
        "    return enhanced_response + sources_section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9p9sXiW9hDK"
      },
      "outputs": [],
      "source": [
        "def simple_rag_with_private_llm(query):\n",
        "    \"\"\"\n",
        "    FIXED: RAG with improved source extraction and display\n",
        "    \"\"\"\n",
        "    print(f\"RAG processing query: {query[:100]}...\")\n",
        "\n",
        "    try:\n",
        "        # Extract simple search terms from the query\n",
        "        search_query = query #test\n",
        "        print(f\"Extracted search query: {search_query}\")\n",
        "\n",
        "        # Perform web search using structured results\n",
        "        raw_results = search_tool.results(search_query, num_results=5)\n",
        "\n",
        "        # Extract URLs from structured search results\n",
        "        source_urls = [r['link'] for r in raw_results if 'link' in r]\n",
        "        print(f\"Extracted {len(source_urls)} source URLs:\")\n",
        "        for url in source_urls:\n",
        "            print(f\"  - {url}\")\n",
        "\n",
        "        # Generate a readable text summary for the LLM\n",
        "        search_results = \"\\n\".join(\n",
        "        [f\"{r['title']}: {r['snippet']}\" for r in raw_results if 'title' in r and 'snippet' in r]\n",
        "        )\n",
        "\n",
        "        print(f\"Search results preview:\\n{search_results[:300]}...\")\n",
        "\n",
        "        # Check if search was successful\n",
        "        if not search_results.strip():\n",
        "            print(\"No substantial search results found.\")\n",
        "            return \"Based on current information, the original expert recommendation remains valid.\\n\\n---\\n\\nüìö **Note:** Web search was performed but no relevant results were found.\"\n",
        "\n",
        "        # Enhanced RAG prompt\n",
        "        rag_prompt = f\"\"\"Based on the search results, enhance and validate the expert recommendation.\n",
        "\n",
        "        ORIGINAL EXPERT RECOMMENDATION: {query}\n",
        "\n",
        "        CURRENT SEARCH RESULTS:\n",
        "        {search_results}\n",
        "\n",
        "        TASK: Use these search results to validate, update, and enhance the expert recommendation. Focus on:\n",
        "        - Current accuracy of the information\n",
        "        - Recent developments or changes\n",
        "        - Specific details that improve the recommendation\n",
        "        - Any corrections needed based on current data\n",
        "\n",
        "        Provide a clear, enhanced recommendation that incorporates the latest information.\n",
        "\n",
        "        IMPORTANT: Provide the links you used to update the information.\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research analyst who validates expert recommendations using current search results. Do not include URLs or sources in your response.\"},\n",
        "                {\"role\": \"user\", \"content\": rag_prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        enhanced_response = response.choices[0].message.content\n",
        "\n",
        "        # Format response with improved source display\n",
        "        final_response_with_sources = format_response_with_sources(enhanced_response, source_urls)\n",
        "\n",
        "        return final_response_with_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"RAG search error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"Unable to retrieve current information for validation.\\n\\n---\\n\\n‚ö†Ô∏è **Error Details:** {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEIBQxEPOO1L"
      },
      "outputs": [],
      "source": [
        "def define_roles(definition_list, state):\n",
        "    \"\"\"\n",
        "    FIXED: Define roles using state instead of global\n",
        "    \"\"\"\n",
        "    # REMOVE: global target_role_map\n",
        "    # USE STATE INSTEAD:\n",
        "    state[\"target_role_map\"] = {\n",
        "        \"Local\": definition_list[1],\n",
        "        \"Expert\": definition_list[2],\n",
        "        \"User Analysis\": definition_list[3]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk82xbOUfxny"
      },
      "outputs": [],
      "source": [
        "def add_predictions_sequential_intelligent(original_query, selected_intent, selected_answer_type, query_id, state):\n",
        "    \"\"\"FIXED: Single isolated processing function to prevent double execution\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\nüîí SINGLE PROCESSING: Query {query_id}\")\n",
        "    print(f\"üìù Query: {original_query}\")\n",
        "    print(f\"üéØ Intent: {selected_intent}\")\n",
        "    print(f\"üìÑ Answer Type: {selected_answer_type}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # CRITICAL: Check if this query is already being processed\n",
        "    if state.get(\"currently_processing\") == query_id:\n",
        "        print(f\"‚ö†Ô∏è DUPLICATE PROCESSING PREVENTED for query {query_id}\")\n",
        "        return \"Processing already in progress for this query.\"\n",
        "\n",
        "    # Mark as currently processing\n",
        "    state[\"currently_processing\"] = query_id\n",
        "\n",
        "    try:\n",
        "        # STEP 1: Query rewriting\n",
        "        working_query = rewrite_query_with_dual_intent(original_query, selected_intent, selected_answer_type, state)\n",
        "        state[\"working_query\"] = working_query\n",
        "        print(f\"‚úèÔ∏è Rewritten Query: {working_query}\")\n",
        "\n",
        "        # STEP 2: Enhanced routing with tracking\n",
        "        print(f\"\\nüîÄ ROUTING ANALYSIS:\")\n",
        "        synthesis_method, routing_info = route_synthesis_with_comprehensive_tracking(\n",
        "            selected_answer_type=selected_answer_type,\n",
        "            original_query=original_query,\n",
        "            rewritten_query=working_query,\n",
        "            query_id=query_id\n",
        "        )\n",
        "        print(f\"üéØ Final Decision: {synthesis_method.upper()}\")\n",
        "        print(f\"üìä Routing Method: {routing_info['routing_type']} ({routing_info['routing_method']})\")\n",
        "        if routing_info['dictionary_match']:\n",
        "            print(f\"üîç Dictionary Key: '{routing_info['dictionary_match']}'\")\n",
        "\n",
        "        # STEP 3: Role assignment and topic identification\n",
        "        definition_list = get_roles(working_query, selected_intent)\n",
        "        topic = definition_list[0]\n",
        "        assigned_roles = definition_list[1:]\n",
        "        state[\"topic\"] = topic\n",
        "        define_roles(definition_list, state)\n",
        "\n",
        "        print(f\"\\nüè∑Ô∏è TOPIC & ROLES:\")\n",
        "        print(f\"üìç Topic: {topic}\")\n",
        "        print(f\"üë• Roles: {assigned_roles}\")\n",
        "\n",
        "        # STEP 4: Expert analysis phase\n",
        "        print(f\"\\nüß† EXPERT ANALYSIS PHASE:\")\n",
        "        ling_response = local_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "        expert_response = expert_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "        user_response = user_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "\n",
        "        # STEP 5: Synthesis based on routing decision\n",
        "        final_response = \"\"\n",
        "        favor_response = \"\"\n",
        "        against_response = \"\"\n",
        "\n",
        "        print(f\"\\n‚öôÔ∏è SYNTHESIS PHASE: {synthesis_method.upper()}\")\n",
        "\n",
        "        if synthesis_method == 'decision_making':\n",
        "            print(\"üîÑ Executing decision-making framework...\")\n",
        "            favor_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"positive\", selected_answer_type, state)\n",
        "            against_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"negative\", selected_answer_type, state)\n",
        "            final_response = final_judgement_enhanced(working_query, favor_response, against_response, topic, selected_answer_type)\n",
        "\n",
        "        elif synthesis_method == 'solution_focused':\n",
        "            print(\"üîÑ Executing solution-focused synthesis...\")\n",
        "            final_response = solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "\n",
        "        else:  # informational\n",
        "            print(\"üîÑ Executing informational synthesis...\")\n",
        "            final_response = summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "\n",
        "        # STEP 6: Calculate metrics and prepare comprehensive results\n",
        "        end_time = time.time()\n",
        "        processing_time = round(end_time - start_time, 2)\n",
        "\n",
        "        print(f\"\\nüìä PROCESSING COMPLETE:\")\n",
        "        print(f\"‚è±Ô∏è Total Time: {processing_time} seconds\")\n",
        "        print(f\"üìÑ Response Length: {len(final_response)} characters\")\n",
        "\n",
        "        # STEP 7: Comprehensive database update\n",
        "        comprehensive_results = {\n",
        "            'Original_Query': original_query,\n",
        "            'Rewritten_Query': working_query,\n",
        "            'Selected_Topic_Intent': selected_intent,\n",
        "            'Selected_Answer_Type': selected_answer_type,\n",
        "            'Routing_Type': routing_info['routing_type'],\n",
        "            'Routing_Method': routing_info['routing_method'],\n",
        "            'Dictionary_Match': routing_info.get('dictionary_match', ''),\n",
        "            'Identified_Topic': topic,\n",
        "            'Assigned_Roles': json.dumps(assigned_roles),\n",
        "            'Linguist_Analysis': ling_response,\n",
        "            'Expert_Analysis': expert_response,\n",
        "            'User_Analysis': user_response,\n",
        "            'In_Favor': favor_response,\n",
        "            'Against': against_response,\n",
        "            'Final_Judgement': final_response,  # ‚Üê This is the final answer regardless of method\n",
        "            'Synthesis_Method': synthesis_method,\n",
        "            'Processing_Time_Seconds': processing_time,\n",
        "        }\n",
        "\n",
        "        update_comprehensive_results(query_id, comprehensive_results)\n",
        "\n",
        "        print(f\"‚úÖ DATABASE UPDATED: Query {query_id}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PROCESSING ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"‚ùå **Error during analysis:** {str(e)}\"\n",
        "\n",
        "    finally:\n",
        "        # CRITICAL: Clear processing lock\n",
        "        if state.get(\"currently_processing\") == query_id:\n",
        "            state[\"currently_processing\"] = None\n",
        "            print(f\"üîì Processing lock cleared for query {query_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dQNLhq2OO1L"
      },
      "outputs": [],
      "source": [
        "def get_last_query_data():\n",
        "    \"\"\"\n",
        "    Get the last query data from database - handles both old and new schema\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        if df.empty:\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Debug: Print column names to see what we actually have\n",
        "        print(f\"DEBUG - Database columns: {list(df.columns)}\")\n",
        "\n",
        "        # Try completed queries first, fallback to any query\n",
        "        completed_queries = df[df['Status'] == 'completed']\n",
        "        if not completed_queries.empty:\n",
        "            last_query = completed_queries.iloc[-1]\n",
        "        else:\n",
        "            last_query = df.iloc[-1]\n",
        "\n",
        "        query_id = int(last_query['ID'])\n",
        "\n",
        "        # Handle both old and new database schemas\n",
        "        if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "            # New schema\n",
        "            print(\"DEBUG - Using new schema (Original_Query, Rewritten_Query)\")\n",
        "            original_query = str(last_query['Original_Query']) if pd.notna(last_query['Original_Query']) else \"Original query not available\"\n",
        "            rewritten_query = str(last_query['Rewritten_Query']) if pd.notna(last_query['Rewritten_Query']) else original_query\n",
        "        elif 'Query' in df.columns:\n",
        "            # Old schema - fallback to 'Query' column\n",
        "            print(\"DEBUG - Using old schema (Query)\")\n",
        "            query_value = str(last_query['Query']) if pd.notna(last_query['Query']) else \"Query not available\"\n",
        "            original_query = query_value\n",
        "            rewritten_query = query_value  # Use same value for both\n",
        "        else:\n",
        "            print(\"DEBUG - No recognized query columns found\")\n",
        "            print(f\"Available columns: {list(df.columns)}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Handle different possible column names for final response\n",
        "        if 'Final Judgement' in df.columns and pd.notna(last_query['Final Judgement']):\n",
        "            final_response = str(last_query['Final Judgement'])\n",
        "        elif 'Final_Judgement' in df.columns and pd.notna(last_query['Final_Judgement']):\n",
        "            final_response = str(last_query['Final_Judgement'])\n",
        "        else:\n",
        "            final_response = rewritten_query\n",
        "\n",
        "        print(f\"DEBUG - Returning: query_id={query_id}, rewritten_query='{rewritten_query[:50]}...', original_query='{original_query[:50]}...'\")\n",
        "        return query_id, rewritten_query, final_response, original_query\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving last query data: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keTYCvYuOO1L"
      },
      "outputs": [],
      "source": [
        "def execute_rag_update(chatbot_history, state):\n",
        "    \"\"\"\n",
        "    IMPROVED: State-based RAG with comprehensive error handling\n",
        "    No database dependency during execution - uses session state directly\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîÑ RAG Enhancement: Starting state-based processing...\")\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 1: Check if state exists and is properly structured\n",
        "        # ========================================\n",
        "        if not state or not isinstance(state, dict):\n",
        "            print(\"‚ùå No valid state found\")\n",
        "            error_msg = \"‚ùå **No active session found.** Please start a new query to use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state or {}, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 2: Check if user has submitted a query through COLA\n",
        "        # ========================================\n",
        "        original_query = state.get(\"original_query\", \"\")\n",
        "        working_query = state.get(\"working_query\", \"\")\n",
        "        processing_query_id = state.get(\"processing_query_id\")\n",
        "\n",
        "        # Enhanced validation for pre-query clicks\n",
        "        if not original_query and not working_query and not processing_query_id:\n",
        "            print(\"‚ùå User clicked RAG before submitting any query\")\n",
        "            error_msg = \"\"\"‚ùå **No query to enhance yet!**\n",
        "\n",
        "                        Please follow these steps:\n",
        "                        1. üìù **Submit your question** in the text box above\n",
        "                        2. üéØ **Select your preferred topic focus** (Option 1, 2, or 3)\n",
        "                        3. üìã **Choose your answer type** (Option 1, 2, or 3)\n",
        "                        4. ‚è≥ **Wait for the analysis to complete**\n",
        "                        5. üîÑ **Then click this button** to enhance with current information\n",
        "\n",
        "                        *The RAG enhancement works best after you've received an initial analysis.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 3: Check if COLA processing is complete\n",
        "        # ========================================\n",
        "        if original_query and not working_query:\n",
        "            print(\"‚ö†Ô∏è Query exists but COLA processing may be incomplete\")\n",
        "            error_msg = \"\"\"‚ö†Ô∏è **Analysis still in progress!**\n",
        "\n",
        "                        Your query is being processed through the COLA framework. Please:\n",
        "                        - üéØ **Complete the topic selection** if prompted\n",
        "                        - üìã **Complete the answer type selection** if prompted\n",
        "                        - ‚è≥ **Wait for the initial analysis to finish**\n",
        "\n",
        "                        *RAG enhancement will be available once the analysis is complete.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 4: Check if there's content to enhance\n",
        "        # ========================================\n",
        "        if not chatbot_history or len(chatbot_history) == 0:\n",
        "            print(\"‚ùå No chat history to enhance\")\n",
        "            error_msg = \"‚ùå **No conversation history found.** Please submit a query first, then use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # Get the most recent assistant response to enhance\n",
        "        last_assistant_response = \"\"\n",
        "        for msg in reversed(chatbot_history):\n",
        "            if msg.get(\"role\") == \"assistant\" and msg.get(\"content\"):\n",
        "                content = msg.get(\"content\", \"\")\n",
        "                # Skip RAG-related messages to get the actual analysis\n",
        "                if not content.startswith(\"üîÑ\") and not content.startswith(\"‚úÖ\") and not content.startswith(\"‚ùå\"):\n",
        "                    last_assistant_response = content\n",
        "                    break\n",
        "\n",
        "        if not last_assistant_response:\n",
        "            print(\"‚ùå No assistant response found to enhance\")\n",
        "            error_msg = \"\"\"‚ùå **No analysis found to enhance.**\n",
        "\n",
        "                        Please ensure you have:\n",
        "                        1. ‚úÖ **Submitted a complete query**\n",
        "                        2. ‚úÖ **Received an analysis response**\n",
        "                        3. ‚úÖ **Completed the COLA framework process**\n",
        "\n",
        "                        *Then try the RAG enhancement again.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # SUCCESSFUL VALIDATION: Proceed with RAG\n",
        "        # ========================================\n",
        "        print(f\"‚úÖ Validation passed - enhancing query: '{original_query[:50]}...'\")\n",
        "        print(f\"üìù Working query: '{working_query[:50]}...'\")\n",
        "        print(f\"üìä Last response length: {len(last_assistant_response)} characters\")\n",
        "\n",
        "        # Show processing message with helpful context\n",
        "        processing_msg = f\"\"\"üîÑ **Enhancing your analysis with current information...**\n",
        "\n",
        "                        **Your Query:** {original_query[:100]}{'...' if len(original_query) > 100 else ''}\n",
        "\n",
        "                        üîç Searching for the latest information to update and validate the analysis...\"\"\"\n",
        "\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        temp_history = chatbot_history + [processing_message]\n",
        "\n",
        "        yield temp_history, state, \"\"\n",
        "\n",
        "        # ========================================\n",
        "        # RAG ENHANCEMENT: Use state data directly\n",
        "        # ========================================\n",
        "        print(\"ü§ñ Starting RAG enhancement with state data...\")\n",
        "\n",
        "        try:\n",
        "            enhanced_response = enhanced_rag_with_session_state(\n",
        "                original_query=original_query,\n",
        "                working_query=working_query,\n",
        "                previous_analysis=last_assistant_response,\n",
        "                state=state\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ RAG enhancement completed successfully\")\n",
        "\n",
        "            # Store RAG results in state for this session\n",
        "            state[\"rag_enhanced_response\"] = enhanced_response\n",
        "            state[\"rag_timestamp\"] = time.time()\n",
        "            state[\"rag_original_query\"] = original_query\n",
        "\n",
        "        except Exception as rag_error:\n",
        "            print(f\"‚ùå RAG processing failed: {rag_error}\")\n",
        "            enhanced_response = f\"\"\"**RAG Enhancement Notice:**\n",
        "\n",
        "                            The current information lookup encountered an issue: {str(rag_error)}\n",
        "\n",
        "                            **Your original analysis remains valid and complete.** This enhancement failure doesn't affect the quality of the previous response.\n",
        "\n",
        "                            *You can try the enhancement again or continue with the existing analysis.*\"\"\"\n",
        "\n",
        "        # ========================================\n",
        "        # PRESENT ENHANCED RESULTS\n",
        "        # ========================================\n",
        "        success_msg = f\"\"\"‚úÖ **Analysis Enhanced with Current Information!**\n",
        "\n",
        "                            {enhanced_response}\n",
        "\n",
        "                            ---\n",
        "                            *üí° This response combines your original analysis with the latest available information for accuracy and relevance.*\"\"\"\n",
        "\n",
        "        final_message = {\"role\": \"assistant\", \"content\": success_msg}\n",
        "        updated_history = chatbot_history + [final_message]\n",
        "\n",
        "        # ========================================\n",
        "        # OPTIONAL: Background database logging (non-blocking)\n",
        "        # ========================================\n",
        "        try:\n",
        "            if processing_query_id:\n",
        "                # This is just for logging - doesn't affect RAG functionality\n",
        "                background_database_logging(processing_query_id, enhanced_response)\n",
        "        except Exception as db_error:\n",
        "            print(f\"‚ö†Ô∏è Database logging failed (non-critical): {db_error}\")\n",
        "            # Don't show this error to user - it's just logging\n",
        "\n",
        "        yield updated_history, state, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Critical error in execute_rag_update: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Comprehensive error message for users\n",
        "        error_msg = f\"\"\"‚ùå **Enhancement Error**\n",
        "\n",
        "                                An unexpected error occurred during RAG enhancement: `{str(e)}`\n",
        "\n",
        "                                **Your original analysis is still available** in the conversation above. You can:\n",
        "                                - üìã **Continue using the existing analysis**\n",
        "                                - üîÑ **Try enhancement again** in a few moments\n",
        "                                - üí¨ **Submit a new query** if needed\n",
        "\n",
        "                                *This error has been logged for improvement.*\"\"\"\n",
        "\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        updated_history = chatbot_history + [error_message]\n",
        "        yield updated_history, state or {}, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fz67QQqOO1L"
      },
      "outputs": [],
      "source": [
        "def enhanced_rag_with_session_state(original_query, working_query, previous_analysis, state):\n",
        "    \"\"\"\n",
        "    RAG enhancement using session state data directly\n",
        "    No database dependency - pure state-based operation\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"üéØ RAG Context:\")\n",
        "    print(f\"   üìù Original: {original_query}\")\n",
        "    print(f\"   üîÑ Working: {working_query}\")\n",
        "    print(f\"   üìä Previous analysis: {len(previous_analysis)} chars\")\n",
        "\n",
        "    try:\n",
        "        # Use the working query (rewritten/contextualized) for better RAG results\n",
        "        query_for_rag = working_query if working_query else original_query\n",
        "\n",
        "        # Create RAG prompt that leverages the previous analysis\n",
        "        rag_prompt = f\"\"\"Based on this answer:\n",
        "\n",
        "                {previous_analysis[:1000]}...\n",
        "\n",
        "                Please provide updated, current information that validates, corrects, or expands upon this answer for the query: \"{query_for_rag}\"\n",
        "\n",
        "                Focus on:\n",
        "                - Latest developments or changes\n",
        "                - Current accuracy of the information\n",
        "                - Recent data or statistics\n",
        "                - Any new perspectives or considerations\"\"\"\n",
        "\n",
        "        print(\"üîç Calling RAG system...\")\n",
        "        enhanced_response = simple_rag_with_private_llm(rag_prompt)\n",
        "\n",
        "        # Extract and log source information\n",
        "        source_urls = extract_source_urls_from_response(enhanced_response)\n",
        "        print(f\"üìö Found {len(source_urls)} sources\")\n",
        "\n",
        "        # Store sources in state\n",
        "        if source_urls:\n",
        "            state[\"rag_sources\"] = source_urls\n",
        "\n",
        "        # Processing time\n",
        "        processing_time = round(time.time() - start_time, 2)\n",
        "        print(f\"‚è±Ô∏è RAG completed in {processing_time} seconds\")\n",
        "\n",
        "        return enhanced_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå RAG processing error: {e}\")\n",
        "        raise e  # Re-raise to be handled by calling function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTfOWJlJOO1M"
      },
      "outputs": [],
      "source": [
        "def extract_source_urls_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract source URLs from RAG response for transparency\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if not response_text or not isinstance(response_text, str):\n",
        "        return []\n",
        "\n",
        "    # Common patterns for URLs in RAG responses\n",
        "    patterns = [\n",
        "        r'\\((https?://[^\\)]+)\\)',  # URLs in parentheses\n",
        "        r'Source: (https?://\\S+)',  # URLs after \"Source:\"\n",
        "        r'\\[(https?://[^\\]]+)\\]',   # URLs in brackets\n",
        "        r'https?://\\S+',            # Any standalone URLs\n",
        "    ]\n",
        "\n",
        "    urls = []\n",
        "    for pattern in patterns:\n",
        "        found_urls = re.findall(pattern, response_text)\n",
        "        urls.extend(found_urls)\n",
        "\n",
        "    # Remove duplicates and return\n",
        "    return list(set(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU_a2XhdOO1M"
      },
      "outputs": [],
      "source": [
        "def background_database_logging(query_id, enhanced_response):\n",
        "    \"\"\"\n",
        "    Optional background logging to database\n",
        "    This runs independently and doesn't affect RAG functionality\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            print(\"‚ö†Ô∏è Database file not found - skipping logging\")\n",
        "            return\n",
        "\n",
        "        # Simple database update for logging\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        mask = df['ID'] == query_id\n",
        "\n",
        "        if mask.any():\n",
        "            df.loc[mask, 'After RAG Agent'] = str(enhanced_response)[:1000]  # Truncate for storage\n",
        "            df.loc[mask, 'RAG_Timestamp'] = time.time()\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"üìä Logged RAG results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Query {query_id} not found for logging\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Database logging failed: {e}\")\n",
        "        # Don't raise - this is non-critical logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIQyIX3_9hDL"
      },
      "outputs": [],
      "source": [
        "def slow_echo_with_dual_intent_disambiguation_dynamic(message, chatbot_history, state):\n",
        "    \"\"\"FIXED: Preserve chat history while resetting only processing state\"\"\"\n",
        "\n",
        "    # PRESERVE existing chat history, reset only processing variables\n",
        "    if not state:\n",
        "        state = {}\n",
        "\n",
        "    # SELECTIVE RESET - only reset processing-related variables, keep UI state\n",
        "    processing_reset = {\n",
        "        \"query_id\": None,\n",
        "        \"original_query\": \"\",\n",
        "        \"intent_options\": [],\n",
        "        \"step\": \"topic_selection\",\n",
        "        \"processing_query_id\": None,\n",
        "        \"answer_type_options\": [],\n",
        "        \"selected_topic_intent\": \"\",\n",
        "        \"working_query\": \"\",\n",
        "        \"topic\": \"\",\n",
        "        \"target_role_map\": {},\n",
        "        \"selected_answer_type\": \"\",\n",
        "        \"processing\": False,\n",
        "        # CRITICAL: Reset any synthesis variables that might cause concatenation\n",
        "        \"rag_enhanced_response\": \"\",\n",
        "        \"synthesis_method\": \"\",\n",
        "        \"assigned_roles\": []\n",
        "    }\n",
        "\n",
        "    # UPDATE state with reset values but preserve other settings\n",
        "    state.update(processing_reset)\n",
        "\n",
        "    # UI state - ensure clean setup\n",
        "    state.update({\n",
        "        \"waiting_for_custom\": False,\n",
        "        \"custom_input_type\": \"\",\n",
        "        \"show_options\": False,\n",
        "        \"show_main_input\": True\n",
        "    })\n",
        "\n",
        "    # Generate unique query ID\n",
        "    import time\n",
        "    current_id = int(time.time() * 1000000)\n",
        "\n",
        "    # Set fresh state for this query\n",
        "    state[\"query_id\"] = current_id\n",
        "    state[\"original_query\"] = message\n",
        "    state[\"step\"] = \"topic_selection\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"processing\"] = False\n",
        "\n",
        "    # PRESERVE existing chat history + add new user message\n",
        "    if chatbot_history is None:\n",
        "        chatbot_history = []\n",
        "\n",
        "    updated_history = chatbot_history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    # Initial yield - ensure UI is clean\n",
        "    yield updated_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        print(f\"üöÄ STARTING CLEAN QUERY PROCESSING\")\n",
        "        print(f\"üìã Query ID: {current_id}\")\n",
        "        print(f\"üìù Query: {message}\")\n",
        "        print(f\"üìö Chat history preserved: {len(chatbot_history)} previous messages\")\n",
        "\n",
        "        # Add query to database\n",
        "        add_new_query(current_id, message)\n",
        "\n",
        "        # Generate topic intent options\n",
        "        topic_intent_options = generate_intent_options(message)\n",
        "        print(f\"üéØ Generated options: {topic_intent_options}\")\n",
        "\n",
        "        # Store in state\n",
        "        state[\"intent_options\"] = topic_intent_options\n",
        "\n",
        "        # Show intent options\n",
        "        final_history, _ = show_intent_options_clean(updated_history, state, topic_intent_options)\n",
        "\n",
        "        yield final_history, state, \"\", gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"‚ùå **Error in intent disambiguation:** {str(e)}\"\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "        error_history = updated_history + [{\"role\": \"assistant\", \"content\": error_message}]\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = False\n",
        "\n",
        "        yield error_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLCNF6q_OO1M"
      },
      "outputs": [],
      "source": [
        "def view_database_stats():\n",
        "    \"\"\"View database statistics - handles both old and new schema\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        total_queries = len(df)\n",
        "        completed = len(df[df['Status'] == 'completed'])\n",
        "        pending = len(df[df['Status'] == 'pending'])\n",
        "        errors = len(df[df['Status'] == 'error'])\n",
        "\n",
        "        # Calculate average processing time for completed queries\n",
        "        completed_df = df[df['Status'] == 'completed']\n",
        "        if not completed_df.empty and 'Processing_Time_Seconds' in completed_df.columns:\n",
        "            # Filter out NaN values before calculating mean\n",
        "            processing_times = completed_df['Processing_Time_Seconds'].dropna()\n",
        "            if not processing_times.empty:\n",
        "                avg_time = processing_times.mean()\n",
        "                avg_time_str = f\"- Average processing time: {avg_time:.2f} seconds\"\n",
        "            else:\n",
        "                avg_time_str = \"- Average processing time: N/A\"\n",
        "        else:\n",
        "            avg_time_str = \"- Average processing time: N/A\"\n",
        "\n",
        "        stats = f\"\"\"\n",
        "        Database Statistics:\n",
        "        - Total queries: {total_queries}\n",
        "        - Completed: {completed}\n",
        "        - Pending: {pending}\n",
        "        - Errors: {errors}\n",
        "        {avg_time_str}\n",
        "\n",
        "        Database Schema: {list(df.columns)}\n",
        "\n",
        "        Recent queries:\n",
        "        \"\"\"\n",
        "\n",
        "        if not df.empty:\n",
        "            # Handle both old and new schema for display\n",
        "            if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "                # New schema\n",
        "                display_columns = ['ID', 'Original_Query', 'Rewritten_Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "            else:\n",
        "                # Old schema\n",
        "                display_columns = ['ID', 'Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "\n",
        "            # Only show columns that actually exist\n",
        "            existing_columns = [col for col in display_columns if col in df.columns]\n",
        "            recent = df.tail(5)[existing_columns]\n",
        "            stats += recent.to_string(index=False)\n",
        "\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        return f\"Error reading database: {e}\\n\\nColumns found: {list(pd.read_csv('cola_database.csv').columns) if os.path.exists('cola_database.csv') else 'File not found'}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZfY7GMzOO1M"
      },
      "outputs": [],
      "source": [
        "def handle_option_1_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"FINAL: Clean flow - options disappear, only keep essential content\"\"\"\n",
        "    if not isinstance(state, dict):\n",
        "        state = {\"step\": \"topic_selection\", \"intent_options\": [], \"answer_type_options\": [],\n",
        "                \"selected_topic_intent\": \"\", \"query_id\": None, \"original_query\": \"\",\n",
        "                \"show_options\": False, \"processing\": False, \"show_main_input\": True}\n",
        "\n",
        "    if state.get(\"step\") == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 0:\n",
        "        # STEP 1: Topic selection - REMOVE intent options, show answer type options\n",
        "        selected_topic = state[\"intent_options\"][0]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove intent options bubble, add answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif state.get(\"step\") == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 0:\n",
        "        # STEP 2: Answer type selection - REMOVE answer type options, show final answer\n",
        "        selected_answer_type = state[\"answer_type_options\"][0]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove answer type options bubble completely\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        # Show processing message temporarily\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        # Hide buttons immediately\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process and replace with final answer\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            # REPLACE processing message with final answer\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5fqmsbEOO1M"
      },
      "outputs": [],
      "source": [
        "def handle_option_2_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"FINAL: Clean flow - options disappear, only keep essential content\"\"\"\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 1:\n",
        "        selected_topic = state[\"intent_options\"][1]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove intent options bubble, add answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 1:\n",
        "        selected_answer_type = state[\"answer_type_options\"][1]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove answer type options bubble completely\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        # Show processing message temporarily\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process and replace with final answer\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            # REPLACE processing message with final answer\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_kpYaXFOO1M"
      },
      "outputs": [],
      "source": [
        "def handle_option_3_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"FINAL: Clean flow - options disappear, only keep essential content\"\"\"\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 2:\n",
        "        selected_topic = state[\"intent_options\"][2]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove intent options bubble, add answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 2:\n",
        "        selected_answer_type = state[\"answer_type_options\"][2]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # CLEAN: Remove answer type options bubble completely\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        # Show processing message temporarily\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process and replace with final answer\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            # REPLACE processing message with final answer\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nwb-aWfOO1M"
      },
      "outputs": [],
      "source": [
        "def handle_other_option_click_dynamic(chatbot_history, state):\n",
        "    \"\"\"FIXED: Handle 'Other' option click with clean flow\"\"\"\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        # Topic selection phase - ask for custom topic\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"topic\"\n",
        "        state[\"show_options\"] = False  # Hide option buttons\n",
        "        state[\"show_main_input\"] = False  # Hide main input\n",
        "\n",
        "        # CLEAN: Remove intent options bubble, show custom input request\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        custom_msg = \"\"\"üîç **Custom Topic Selection**\n",
        "\n",
        "                The provided options don't match what you're looking for? No problem!\n",
        "\n",
        "                Please specify your preferred topic or domain in the text box below. For example:\n",
        "                - \"Python machine learning\"\n",
        "                - \"Apple company stock\"\n",
        "                - \"Java coffee brewing\"\n",
        "\n",
        "                *Tip: Be as specific as possible to get the best results.*\"\"\"\n",
        "\n",
        "        custom_message = {\"role\": \"assistant\", \"content\": custom_msg}\n",
        "        updated_history = cleaned_history + [custom_message]\n",
        "\n",
        "        # Hide main input, hide option buttons, show custom input\n",
        "        return updated_history, state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        # Answer type selection phase - ask for custom answer type\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"answer_type\"\n",
        "        state[\"show_options\"] = False  # Hide option buttons\n",
        "        state[\"show_main_input\"] = False  # Hide main input\n",
        "\n",
        "        # CLEAN: Remove answer type options bubble, show custom input request\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        custom_msg = \"\"\"üìù **Custom Answer Type**\n",
        "\n",
        "                Need a different type of response? Please specify what kind of answer you're looking for:\n",
        "\n",
        "                Examples:\n",
        "                - \"Step-by-step tutorial\"\n",
        "                - \"Pros and cons comparison\"\n",
        "                - \"Historical timeline\"\n",
        "                - \"Technical specifications\"\n",
        "\n",
        "                *Tip: Describe the format or style of response you prefer.*\"\"\"\n",
        "\n",
        "        custom_message = {\"role\": \"assistant\", \"content\": custom_msg}\n",
        "        updated_history = cleaned_history + [custom_message]\n",
        "\n",
        "        # Hide main input, hide option buttons, show custom input\n",
        "        return updated_history, state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \"\"\n",
        "\n",
        "    else:\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmiMkC1jOO1N"
      },
      "outputs": [],
      "source": [
        "def process_custom_input_with_spellcheck(user_input, input_type):\n",
        "    \"\"\"Process user's custom input with basic spell checking (UNCHANGED)\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Basic cleaning\n",
        "    cleaned_input = user_input.strip()\n",
        "\n",
        "    # Basic spell checking for common terms\n",
        "    spell_corrections = {\n",
        "        # Programming languages\n",
        "        \"phyton\": \"Python\", \"pyhton\": \"Python\", \"pythn\": \"Python\",\n",
        "        \"javas\": \"Java\", \"jave\": \"Java\",\n",
        "        \"javascript\": \"JavaScript\", \"js\": \"JavaScript\",\n",
        "\n",
        "        # Common topics\n",
        "        \"machien learning\": \"machine learning\",\n",
        "        \"artifical intelligence\": \"artificial intelligence\",\n",
        "        \"blockchian\": \"blockchain\",\n",
        "        \"cyrptocurrency\": \"cryptocurrency\",\n",
        "\n",
        "        # Answer types\n",
        "        \"tutorail\": \"tutorial\", \"tutoral\": \"tutorial\",\n",
        "        \"comparision\": \"comparison\", \"comparsion\": \"comparison\",\n",
        "        \"recomendation\": \"recommendation\", \"recomendations\": \"recommendations\"\n",
        "    }\n",
        "\n",
        "    # Apply corrections\n",
        "    for wrong, correct in spell_corrections.items():\n",
        "        if wrong.lower() in cleaned_input.lower():\n",
        "            cleaned_input = re.sub(re.escape(wrong), correct, cleaned_input, flags=re.IGNORECASE)\n",
        "\n",
        "    return cleaned_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvkZqhqeOO1N"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_submit_dynamic(custom_input, chatbot_history, state):\n",
        "    \"\"\"FINAL FIX: Use yield for proper UI state management during custom processing\"\"\"\n",
        "    if not state or not state.get(\"waiting_for_custom\"):\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    if not custom_input or not custom_input.strip():\n",
        "        error_msg = {\"role\": \"assistant\", \"content\": \"‚ùå Please enter your custom option before submitting.\"}\n",
        "        return chatbot_history + [error_msg], state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), custom_input\n",
        "\n",
        "    # Process the custom input\n",
        "    input_type = state.get(\"custom_input_type\", \"topic\")\n",
        "    processed_input = process_custom_input_with_spellcheck(custom_input, input_type)\n",
        "\n",
        "    # PROPER state cleanup\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_main_input\"] = True\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        # Handle custom topic selection - SAME AS BEFORE\n",
        "        state[\"selected_topic_intent\"] = processed_input\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), processed_input)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # CLEAN: Remove custom input instruction, show answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        return final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        # CRITICAL FIX: This needs to be a GENERATOR FUNCTION to match other handlers\n",
        "        # Convert this to a generator that yields properly\n",
        "\n",
        "        yield from handle_custom_answer_type_processing(processed_input, chatbot_history, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hSVp7zSOO1N"
      },
      "outputs": [],
      "source": [
        "def handle_custom_answer_type_processing(processed_input, chatbot_history, state):\n",
        "    \"\"\"GENERATOR: Handle custom answer type with proper yield-based UI management\"\"\"\n",
        "    # Handle custom answer type selection\n",
        "    state[\"selected_answer_type\"] = processed_input\n",
        "    state[\"show_options\"] = False\n",
        "    state[\"processing\"] = True\n",
        "\n",
        "    # CLEAN: Remove custom input instruction completely\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "    # Show processing message\n",
        "    processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' with custom answer type: {processed_input}\"\n",
        "    processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "    processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "    # CRITICAL: Ensure UI is completely clean during processing\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_options\"] = False\n",
        "    state[\"show_main_input\"] = True\n",
        "    state[\"step\"] = \"topic_selection\"  # Reset for next query\n",
        "\n",
        "    # FIRST YIELD: Hide custom input UI immediately, show processing\n",
        "    yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # THEN PROCESS: Now do the actual processing\n",
        "    try:\n",
        "        print(f\"üîÑ STARTING CUSTOM PROCESSING for query: {state.get('query_id')}\")\n",
        "\n",
        "        # Use the same function as regular options\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            state.get(\"original_query\", \"\"),\n",
        "            state.get(\"selected_topic_intent\", \"\"),\n",
        "            processed_input,\n",
        "            state.get(\"query_id\"),\n",
        "            state\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ CUSTOM PROCESSING COMPLETE: {len(answer)} characters\")\n",
        "\n",
        "        # REPLACE processing message with ONLY the final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        # Complete state cleanup\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"waiting_for_custom\"] = False\n",
        "\n",
        "        # FINAL YIELD: Show final answer, ensure everything is hidden\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CUSTOM PROCESSING ERROR: {e}\")\n",
        "        error_msg = f\"‚ùå **Processing Error:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        # Complete state cleanup on error\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"waiting_for_custom\"] = False\n",
        "\n",
        "        # ERROR YIELD: Show error, ensure everything is hidden\n",
        "        yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frC1474aOO1N"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_cancel_dynamic(chatbot_history, state):\n",
        "    \"\"\"Handle cancellation of custom input with dynamic UI - SHOW main input again\"\"\"\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # Clear custom input state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"show_main_input\"] = True  # SHOW main input again\n",
        "\n",
        "    # Remove the custom input request message\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "    # Add cancellation message\n",
        "    cancel_msg = {\"role\": \"assistant\", \"content\": \"‚ùå **Custom input cancelled.** Please select one of the provided options above.\"}\n",
        "    updated_history = cleaned_history + [cancel_msg]\n",
        "\n",
        "    # Show main input, show option buttons, hide custom input\n",
        "    return updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiTaw9ntOO1N"
      },
      "outputs": [],
      "source": [
        "def safe_close():\n",
        "    \"\"\"Clean shutdown function\"\"\"\n",
        "    print(\"üîÑ Closing application...\")\n",
        "    # Add any cleanup here\n",
        "    return \"‚úÖ Application closed safely. You can close the browser tab now.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQLp5qylOO1N"
      },
      "outputs": [],
      "source": [
        "def create_enhanced_gradio_interface():\n",
        "    \"\"\"UPDATED: Handle custom input generator properly\"\"\"\n",
        "    with gr.Blocks() as demo:\n",
        "        # Initialize database on startup\n",
        "        initialize_database_with_sources()\n",
        "\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Enhanced Collaborative Search with Dual Intent Clarification\",\n",
        "            type=\"messages\",\n",
        "            height=400\n",
        "        )\n",
        "\n",
        "        # Main input components\n",
        "        with gr.Row(visible=True) as main_input_row:\n",
        "            with gr.Column():\n",
        "                msg = gr.Textbox(label=\"Your query\", placeholder=\"How can I help you today?\")\n",
        "                send_btn = gr.Button(\"Send\")\n",
        "\n",
        "        # Intent selection buttons\n",
        "        with gr.Row(visible=False) as option_buttons_row:\n",
        "            option1_btn = gr.Button(\"Option 1\", size=\"lg\", variant=\"secondary\")\n",
        "            option2_btn = gr.Button(\"Option 2\", size=\"lg\", variant=\"secondary\")\n",
        "            option3_btn = gr.Button(\"Option 3\", size=\"lg\", variant=\"secondary\")\n",
        "            option4_btn = gr.Button(\"Other\", size=\"lg\", variant=\"secondary\")\n",
        "\n",
        "        # Custom input modal components\n",
        "        with gr.Row(visible=False) as custom_input_row:\n",
        "            with gr.Column():\n",
        "                custom_input_label = gr.Markdown(\"**Please specify your preferred topic/answer type:**\")\n",
        "                custom_input = gr.Textbox(\n",
        "                    label=\"Your custom option\",\n",
        "                    placeholder=\"Type your preferred topic or answer type here...\",\n",
        "                    lines=2\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    submit_custom_btn = gr.Button(\"Submit Custom Option\", variant=\"primary\")\n",
        "                    cancel_custom_btn = gr.Button(\"Cancel\", variant=\"secondary\")\n",
        "\n",
        "        extra_btn = gr.Button(\"Update information using RAG\")\n",
        "\n",
        "        # Database management buttons\n",
        "        with gr.Row():\n",
        "            stats_btn = gr.Button(\"View Database Stats\")\n",
        "            export_btn = gr.Button(\"Export Database\")\n",
        "            close_btn = gr.Button(\"üî¥ Close App\", variant=\"stop\")\n",
        "\n",
        "        stats_output = gr.Textbox(label=\"Database Information\", lines=10)\n",
        "\n",
        "        # Enhanced state\n",
        "        state = gr.State({\n",
        "            \"query_id\": None,\n",
        "            \"original_query\": \"\",\n",
        "            \"processing_query_id\": None,\n",
        "            \"intent_options\": [],\n",
        "            \"answer_type_options\": [],\n",
        "            \"selected_topic_intent\": \"\",\n",
        "            \"selected_answer_type\": \"\",\n",
        "            \"step\": \"topic_selection\",\n",
        "            \"working_query\": \"\",\n",
        "            \"topic\": \"\",\n",
        "            \"target_role_map\": {},\n",
        "            \"waiting_for_custom\": False,\n",
        "            \"custom_input_type\": \"\",\n",
        "            \"show_options\": False,\n",
        "            \"processing\": False,\n",
        "            \"show_main_input\": True,\n",
        "            \"currently_processing\": None  # NEW: Prevent double processing\n",
        "        })\n",
        "\n",
        "        # Event handlers\n",
        "        send_btn.click(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        msg.submit(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        # Regular option handlers\n",
        "        option1_btn.click(\n",
        "            handle_option_1_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "        option2_btn.click(\n",
        "            handle_option_2_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "        option3_btn.click(\n",
        "            handle_option_3_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        # \"Other\" option handler\n",
        "        option4_btn.click(\n",
        "            handle_other_option_click_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # FIXED: Custom input handlers with proper generator support\n",
        "        submit_custom_btn.click(\n",
        "            handle_custom_input_submit_dynamic,\n",
        "            inputs=[custom_input, chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        cancel_custom_btn.click(\n",
        "            handle_custom_input_cancel_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # Other handlers\n",
        "        extra_btn.click(execute_rag_update, inputs=[chatbot, state], outputs=[chatbot, state, msg])\n",
        "        stats_btn.click(view_comprehensive_database_stats, outputs=[stats_output])\n",
        "        close_btn.click(safe_close, outputs=[stats_output])\n",
        "\n",
        "    demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4DUftknOO1N",
        "outputId": "cbe7dc77-67df-478d-9263-bd2c9979dfc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7871\n",
            "* Running on public URL: https://82aefc826c89b2831e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://82aefc826c89b2831e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ STARTING CLEAN QUERY PROCESSING\n",
            "üìã Query ID: 1755045318372963\n",
            "üìù Query: where is java\n",
            "üìö Chat history preserved: 0 previous messages\n",
            "‚úÖ Added query 1755045318372963 to database\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_159/2014738032.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, new_row], ignore_index=True, sort=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Generated options: ['Java (geography)', 'Java (programming language)', 'Java (coffee culture)']\n",
            "üîÑ STARTING SINGLE PROCESSING for query: 1755045318372963\n",
            "\n",
            "üîí SINGLE PROCESSING: Query 1755045318372963\n",
            "üìù Query: where is java\n",
            "üéØ Intent: Java Indonesia\n",
            "üìÑ Answer Type: Historical timeline\"\n",
            "======================================================================\n",
            "‚úèÔ∏è Rewritten Query: Provide a historical timeline of the development and spread of Java in Indonesia\n",
            "\n",
            "üîÄ ROUTING ANALYSIS:\n",
            "üîç Processing answer type: 'historical timeline\"'\n",
            "‚ùå NO DICTIONARY MATCH for 'historical timeline\"'\n",
            "ü§ñ Routing to LLM for intelligent analysis...\n",
            "ü§ñ INTELLIGENT ROUTING: LLM determined 'informational' based on query analysis\n",
            "üéØ Final Decision: INFORMATIONAL\n",
            "üìä Routing Method: LLM (dictionary_fallback)\n",
            "\n",
            "üè∑Ô∏è TOPIC & ROLES:\n",
            "üìç Topic: Java Indonesia\n",
            "üë• Roles: ['Java Developer', 'Indonesian Culture Specialist', 'Coffee Connoisseur']\n",
            "\n",
            "üß† EXPERT ANALYSIS PHASE:\n",
            "\n",
            "‚öôÔ∏è SYNTHESIS PHASE: INFORMATIONAL\n",
            "üîÑ Executing informational synthesis...\n",
            "\n",
            "üìä PROCESSING COMPLETE:\n",
            "‚è±Ô∏è Total Time: 24.85 seconds\n",
            "üìÑ Response Length: 4058 characters\n",
            "‚úÖ Updated comprehensive results for query 1755045318372963\n",
            "‚úÖ DATABASE UPDATED: Query 1755045318372963\n",
            "======================================================================\n",
            "üîì Processing lock cleared for query 1755045318372963\n",
            "‚úÖ SINGLE RESPONSE GENERATED: 4058 characters\n",
            "\n",
            "üîí SINGLE PROCESSING: Query 1755045318372963\n",
            "üìù Query: where is java\n",
            "üéØ Intent: Java Indonesia\n",
            "üìÑ Answer Type: Historical timeline\"\n",
            "======================================================================\n",
            "‚úèÔ∏è Rewritten Query: Provide a historical timeline of the development and spread of Java in Indonesia\n",
            "\n",
            "üîÄ ROUTING ANALYSIS:\n",
            "üîç Processing answer type: 'historical timeline\"'\n",
            "‚ùå NO DICTIONARY MATCH for 'historical timeline\"'\n",
            "ü§ñ Routing to LLM for intelligent analysis...\n",
            "ü§ñ INTELLIGENT ROUTING: LLM determined 'informational' based on query analysis\n",
            "üéØ Final Decision: INFORMATIONAL\n",
            "üìä Routing Method: LLM (dictionary_fallback)\n",
            "\n",
            "üè∑Ô∏è TOPIC & ROLES:\n",
            "üìç Topic: Java Indonesia\n",
            "üë• Roles: ['Java Developer', 'Indonesian Culture Specialist', 'Coffee Connoisseur']\n",
            "\n",
            "üß† EXPERT ANALYSIS PHASE:\n",
            "üîÑ Closing application...\n",
            "\n",
            "‚öôÔ∏è SYNTHESIS PHASE: INFORMATIONAL\n",
            "üîÑ Executing informational synthesis...\n",
            "\n",
            "üìä PROCESSING COMPLETE:\n",
            "‚è±Ô∏è Total Time: 24.06 seconds\n",
            "üìÑ Response Length: 4058 characters\n",
            "‚úÖ Updated comprehensive results for query 1755045318372963\n",
            "‚úÖ DATABASE UPDATED: Query 1755045318372963\n",
            "======================================================================\n",
            "üîì Processing lock cleared for query 1755045318372963\n"
          ]
        }
      ],
      "source": [
        "create_enhanced_gradio_interface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9da8N-DXOO1N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}