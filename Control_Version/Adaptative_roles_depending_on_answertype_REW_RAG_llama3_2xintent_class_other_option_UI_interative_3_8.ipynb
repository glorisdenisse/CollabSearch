{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI9rRfYae-ir",
        "outputId": "122c4fea-0eec-448b-a361-2294371718a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: langchain in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.99.5)\n",
            "Requirement already satisfied: google-api-python-client in /opt/miniconda3/lib/python3.10/site-packages (2.178.0)\n",
            "Requirement already satisfied: langchain_community in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: tools in /opt/miniconda3/lib/python3.10/site-packages (1.0.2)\n",
            "Requirement already satisfied: langchain_google_community in /opt/miniconda3/lib/python3.10/site-packages (2.0.7)\n",
            "Requirement already satisfied: langchain_openai in /opt/miniconda3/lib/python3.10/site-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.40.3)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (3.11.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (2.4.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (1.71.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.7)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: greenlet>=1 in /opt/miniconda3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.99.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gradio in /opt/miniconda3/lib/python3.10/site-packages (5.41.1)\n",
            "Requirement already satisfied: openpyxl in /opt/miniconda3/lib/python3.10/site-packages (3.1.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /opt/miniconda3/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (1.26.18)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai>=1.0.0 geotext transformers GeoText\n",
        "!pip install -q -U google-genai\n",
        "!pip install langchain openai google-api-python-client langchain_community tools langchain_google_community langchain_openai\n",
        "!pip install --upgrade openai\n",
        "!pip install gradio openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DibFVFaDfIq8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "import csv\n",
        "import time\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from langchain import GoogleSearchAPIWrapper\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sui66xUhfUlC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"IDA_LLM_API_KEY\"]=\"your_key_here\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_key_here\"\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"your_key_here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHCvF0cl9hDI"
      },
      "outputs": [],
      "source": [
        "# Initialize your private LLM client (llama-3-8b-instruct-instruct via university server)\n",
        "client = OpenAI(\n",
        "    base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\",\n",
        "    api_key=os.environ['IDA_LLM_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE2KJmtyODfS"
      },
      "outputs": [],
      "source": [
        "def initialize_database_with_sources():\n",
        "    \"\"\"\n",
        "    OPTIONAL: Enhanced database initialization that includes RAG source storage.\n",
        "    Only use this if you want to store source URLs in the database.\n",
        "    \"\"\"\n",
        "    if not os.path.exists('cola_database.csv'):\n",
        "        columns = [\n",
        "            'ID', 'Original_Query', 'Rewritten_Query', 'Selected_Topic_Intent', 'Selected_Answer_Type',\n",
        "            'Linguist Analysis', 'Expert Analysis', 'User Analysis', 'In Favor', 'Against',\n",
        "            'Final Judgement', 'After RAG Agent', 'Final Plan',\n",
        "            'RAG_Source_URLs',  # NEW COLUMN for storing source URLs\n",
        "            'Processing_Time_Seconds', 'Processing_Time_Seconds_RAG','Timestamp', 'Status'\n",
        "        ]\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "        # Set proper data types to avoid warnings\n",
        "        df = df.astype({\n",
        "            'ID': 'int64',\n",
        "            'Original_Query': 'string',\n",
        "            'Rewritten_Query': 'string',\n",
        "            'Selected_Topic_Intent': 'string',\n",
        "            'Selected_Answer_Type': 'string',\n",
        "            'Linguist Analysis': 'string',\n",
        "            'Expert Analysis': 'string',\n",
        "            'User Analysis': 'string',\n",
        "            'In Favor': 'string',\n",
        "            'Against': 'string',\n",
        "            'Final Judgement': 'string',\n",
        "            'After RAG Agent': 'string',\n",
        "            'Final Plan': 'string',\n",
        "            'RAG_Source_URLs': 'string',  # NEW COLUMN\n",
        "            'Processing_Time_Seconds': 'float64',\n",
        "            'Processing_Time_Seconds_RAG': 'float64',\n",
        "            'Timestamp': 'string',\n",
        "            'Status': 'string'\n",
        "        })\n",
        "        df.to_csv('cola_database.csv', index=False)\n",
        "        print(\"Enhanced COLA database with source tracking initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edw3JlKHODfS"
      },
      "outputs": [],
      "source": [
        "def add_new_query(query_id, query_text):\n",
        "    \"\"\"Add a new query to the database with pending status\"\"\"\n",
        "    try:\n",
        "        if os.path.exists('cola_database.csv'):\n",
        "            df = pd.read_csv('cola_database.csv', dtype={\n",
        "                'ID': 'int64',\n",
        "                'Original_Query': 'string',\n",
        "                'Rewritten_Query': 'string',\n",
        "                'Selected_Topic_Intent': 'string',\n",
        "                'Selected_Answer_Type': 'string',\n",
        "                'Linguist Analysis': 'string',\n",
        "                'Expert Analysis': 'string',\n",
        "                'User Analysis': 'string',\n",
        "                'In Favor': 'string',\n",
        "                'Against': 'string',\n",
        "                'Final Judgement': 'string',\n",
        "                'After RAG Agent': 'string',\n",
        "                'Final Plan': 'string',\n",
        "                'Processing_Time_Seconds': 'float64',\n",
        "                'Processing_Time_Seconds_RAG': 'float64',\n",
        "                'Timestamp': 'string',\n",
        "                'Status': 'string'\n",
        "            })\n",
        "        else:\n",
        "            initialize_database()\n",
        "            df = pd.read_csv('cola_database.csv', dtype={\n",
        "                'ID': 'int64',\n",
        "                'Original_Query': 'string',\n",
        "                'Rewritten_Query': 'string',\n",
        "                'Selected_Topic_Intent': 'string',\n",
        "                'Selected_Answer_Type': 'string',\n",
        "                'Linguist Analysis': 'string',\n",
        "                'Expert Analysis': 'string',\n",
        "                'User Analysis': 'string',\n",
        "                'In Favor': 'string',\n",
        "                'Against': 'string',\n",
        "                'Final Judgement': 'string',\n",
        "                'After RAG Agent': 'string',\n",
        "                'Final Plan': 'string',\n",
        "                'Processing_Time_Seconds': 'float64',\n",
        "                'Processing_Time_Seconds_RAG': 'float64',\n",
        "                'Timestamp': 'string',\n",
        "                'Status': 'string'\n",
        "            })\n",
        "\n",
        "        # Create new row with proper data types\n",
        "        new_row = pd.DataFrame([{\n",
        "            'ID': int(query_id),\n",
        "            'Original_Query': str(query_text),\n",
        "            'Rewritten_Query': '',\n",
        "            'Selected_Topic_Intent': '',\n",
        "            'Selected_Answer_Type': '',\n",
        "            'Linguist Analysis': '',\n",
        "            'Expert Analysis': '',\n",
        "            'User Analysis': '',\n",
        "            'In Favor': '',\n",
        "            'Against': '',\n",
        "            'Final Judgement': '',\n",
        "            'After RAG Agent': '',\n",
        "            'Final Plan': '',\n",
        "            'Processing_Time_Seconds': pd.NA,  # Use pd.NA instead of None for float columns\n",
        "            'Processing_Time_Seconds_RAG': pd.NA,\n",
        "            'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Status': 'pending'\n",
        "        }])\n",
        "\n",
        "        # Ensure the new row has the same dtypes as the main dataframe\n",
        "        new_row = new_row.astype(df.dtypes.to_dict())\n",
        "\n",
        "        # Concatenate with proper handling\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        df.to_csv('cola_database.csv', index=False)\n",
        "        print(f\"Added query {query_id} to enhanced COLA database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding query to database: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZxLXJgjODfS"
      },
      "outputs": [],
      "source": [
        "def update_query_results(query_id, results_dict):\n",
        "    \"\"\"Update all COLA results for a specific query ID\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv', dtype={\n",
        "            'ID': 'int64',\n",
        "            'Original_Query': 'string',\n",
        "            'Rewritten_Query': 'string',\n",
        "            'Selected_Topic_Intent': 'string',\n",
        "            'Selected_Answer_Type': 'string',\n",
        "            'Linguist Analysis': 'string',\n",
        "            'Expert Analysis': 'string',\n",
        "            'User Analysis': 'string',\n",
        "            'In Favor': 'string',\n",
        "            'Against': 'string',\n",
        "            'Final Judgement': 'string',\n",
        "            'After RAG Agent': 'string',\n",
        "            'Final Plan': 'string',\n",
        "            'Processing_Time_Seconds': 'float64',\n",
        "            'Processing_Time_Seconds_RAG': 'float64',\n",
        "            'Timestamp': 'string',\n",
        "            'Status': 'string'\n",
        "        })\n",
        "\n",
        "        # Find the row with the specific ID\n",
        "        mask = df['ID'] == query_id\n",
        "        if mask.any():\n",
        "            # Update all columns with results - handle data types properly\n",
        "            for column, value in results_dict.items():\n",
        "                if column in df.columns and value is not None:\n",
        "                    # Handle float columns specifically\n",
        "                    if column in ['Processing_Time_Seconds', 'Processing_Time_Seconds_RAG']:\n",
        "                        df.loc[mask, column] = float(value)\n",
        "                    else:\n",
        "                        df.loc[mask, column] = str(value)\n",
        "                elif column in df.columns:\n",
        "                    # Handle None values\n",
        "                    if column in ['Processing_Time_Seconds', 'Processing_Time_Seconds_RAG']:\n",
        "                        df.loc[mask, column] = None\n",
        "                    else:\n",
        "                        df.loc[mask, column] = ''\n",
        "\n",
        "            df.loc[mask, 'Status'] = 'completed'\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"Updated all results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"Query ID {query_id} not found in database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating results in database: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcPsHj_Yfhvt"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt):\n",
        "    max_retries = 100\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "          messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "          response = client.chat.completions.create(\n",
        "              model=\"llama-3-8b-instruct\",\n",
        "              messages=messages,\n",
        "              temperature=0\n",
        "            )\n",
        "          return response.choices[0].message.content\n",
        "        except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzsEcMKwffxy"
      },
      "outputs": [],
      "source": [
        "def get_completion_with_role(role, instruction, content):\n",
        "    max_retries = 100\n",
        "    for i in range(max_retries):\n",
        "      try:\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{instruction}\\n{content}\"}\n",
        "        ]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "          )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "      except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQv7rO44ODfU"
      },
      "outputs": [],
      "source": [
        "def generate_intent_options(original_query):\n",
        "    \"\"\"\n",
        "    Generate 3 most likely DOMAIN/TOPIC interpretations for the original query.\n",
        "    This disambiguates between completely different subjects, not just different angles.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Query: \"{original_query}\"\n",
        "\n",
        "        CRITICAL: I need you to identify if this query has AMBIGUOUS WORDS that could mean completely different things.\n",
        "\n",
        "        Look for words that could be:\n",
        "        - Programming language vs. place vs. other meanings (Java, Python, Ruby, etc.)\n",
        "        - Company vs. fruit vs. other (Apple, Orange, etc.)\n",
        "        - Person vs. place vs. concept (Tesla, Darwin, etc.)\n",
        "        - Multiple different meanings entirely\n",
        "\n",
        "        If you find ambiguous words, give me 3 DIFFERENT DOMAINS/SUBJECTS.\n",
        "        If no ambiguous words, give me 3 different CONTEXTS for the same topic.\n",
        "\n",
        "        WRONG (all same domain):\n",
        "        - Java web development features\n",
        "        - Java mobile app development\n",
        "        - Java enterprise applications\n",
        "\n",
        "        RIGHT (different domains):\n",
        "        - Java (programming language)\n",
        "        - Java (Indonesian island)\n",
        "        - Java (coffee culture)\n",
        "\n",
        "        Format: Topic (context)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Clean and parse the response - be more aggressive about filtering\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        # More comprehensive filtering for instruction text\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"interpretations\",\n",
        "            \"different meanings\", \"ambiguous\", \"critical\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines that contain instruction-like phrases\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets more aggressively\n",
        "            cleaned_line = line\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\â€¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            # Only keep lines that look like actual topic options (should contain parentheses ideally)\n",
        "            if cleaned_line and len(cleaned_line) > 3 and not any(phrase in cleaned_line.lower() for phrase in skip_phrases):\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # If we don't get good results, create manual disambiguation for common terms\n",
        "        query_lower = original_query.lower()\n",
        "\n",
        "        # Check for common ambiguous terms\n",
        "        if 'java' in query_lower:\n",
        "            return [\n",
        "                \"Java (programming language)\",\n",
        "                \"Java (Indonesian island)\",\n",
        "                \"Java (coffee culture)\"\n",
        "            ]\n",
        "        elif 'python' in query_lower:\n",
        "            return [\n",
        "                \"Python (programming language)\",\n",
        "                \"Python (snake species)\",\n",
        "                \"Python (Monty Python comedy)\"\n",
        "            ]\n",
        "        elif 'tesla' in query_lower:\n",
        "            return [\n",
        "                \"Tesla (car company)\",\n",
        "                \"Tesla (Nikola Tesla scientist)\",\n",
        "                \"Tesla (band/music)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic contextual fallback\n",
        "            return [\n",
        "                f\"Technical/professional information about {original_query}\",\n",
        "                f\"General educational information about {original_query}\",\n",
        "                f\"Practical applications of {original_query}\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating intent options: {e}\")\n",
        "        return [\n",
        "            f\"Technical information about {original_query}\",\n",
        "            f\"General information about {original_query}\",\n",
        "            f\"Practical guide for {original_query}\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db2xB_OgODfU"
      },
      "outputs": [],
      "source": [
        "def show_intent_options_clean(chatbot_history, state, options):\n",
        "    \"\"\"Enhanced to show 'Other' option without duplicate messages\"\"\"\n",
        "    options_text = \"ðŸŽ¯ **Please clarify your intent:**\\n\\n\"\n",
        "    options_text += \"Select the option that best matches what you're looking for:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** ðŸ”§ **Other** - Specify your own topic/domain\\n\\n\"\n",
        "    options_text += \"ðŸ‘‡ **Click the corresponding Option button below to proceed.**\"\n",
        "\n",
        "    intent_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    updated_history = chatbot_history + [intent_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03StwcBgODfU"
      },
      "outputs": [],
      "source": [
        "def generate_answer_type_options(query, selected_topic_intent):\n",
        "    \"\"\"\n",
        "    Generate answer type options based on the query and selected topic intent.\n",
        "    This helps clarify what kind of response the user is looking for.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Query: \"{query}\"\n",
        "        Selected Topic: \"{selected_topic_intent}\"\n",
        "\n",
        "        The user wants to know about this topic. What type of answer would be most helpful?\n",
        "\n",
        "        Generate 3 different ANSWER TYPE options that would be appropriate for this query:\n",
        "\n",
        "        Consider these categories:\n",
        "        - Informative (detailed explanation, facts, background information)\n",
        "        - Practical Tips (actionable advice, how-to guidance, steps to follow)\n",
        "        - Basic Overview (simple introduction, key points, beginner-friendly)\n",
        "        - Expert Analysis (in-depth professional perspective, technical details)\n",
        "        - Comparison/Evaluation (pros/cons, alternatives, recommendations)\n",
        "        - Problem-Solving (solutions, troubleshooting, addressing specific issues)\n",
        "\n",
        "        Format each option as: \"Answer Type (brief description)\"\n",
        "\n",
        "        Examples:\n",
        "        - Informative (comprehensive background and facts)\n",
        "        - Practical Tips (step-by-step actionable guidance)\n",
        "        - Basic Overview (simple introduction for beginners)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Parse response similar to intent options\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"options\",\n",
        "            \"different types\", \"answer type\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\â€¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            if cleaned_line and len(cleaned_line) > 3:\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # Fallback options based on query analysis\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(word in query_lower for word in ['how to', 'steps', 'guide', 'tutorial']):\n",
        "            return [\n",
        "                \"Practical Tips (step-by-step actionable guidance)\",\n",
        "                \"Informative (detailed explanation and background)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\"\n",
        "            ]\n",
        "        elif any(word in query_lower for word in ['what is', 'explain', 'about']):\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\",\n",
        "                \"Expert Analysis (in-depth professional perspective)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic fallback\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Practical Tips (actionable advice and guidance)\",\n",
        "                \"Basic Overview (simple introduction and key points)\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer type options: {e}\")\n",
        "        return [\n",
        "            \"Informative (comprehensive background and facts)\",\n",
        "            \"Practical Tips (actionable advice and guidance)\",\n",
        "            \"Basic Overview (simple introduction and key points)\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYPL48mWODfV",
        "outputId": "f649a7d4-f081-4554-dc02-dd3541ed9ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error adding query to database: float() argument must be a string or a real number, not 'NAType'\n",
            "Original: what is a point \n",
            "Topic intent options: ['Point (geography)', 'Point (mathematics)', 'Point (debate)']\n",
            "Processing query 1754688993933\n",
            "Original query: what is a point \n",
            "Selected intent: Point (geography)\n",
            "Selected answer type: Step-by-step tutorial\n",
            "---------DUAL INTENT CLARIFICATION COMPLETE--------\n",
            "\n",
            "Rewritten query: Can you provide a step-by-step tutorial on how to define and identify a point in geography, including its characteristics and applications?\n",
            "---------QUERY REWRITING COMPLETE--------\n",
            "\n",
            "ðŸš¦ ROUTING ANALYSIS STARTING...\n",
            "ðŸ” Checking dictionary for: 'step-by-step tutorial'\n",
            "âœ… DICTIONARY PARTIAL MATCH: 'step-by-step tutorial' matched 'tutorial' â†’ 'SOLUTION_FOCUSED'\n",
            "ðŸŽ¯ FINAL ROUTING DECISION: 'SOLUTION_FOCUSED'\n",
            "---------INTELLIGENT ROUTING COMPLETE--------\n",
            "\n",
            "Identified topic: Point (geography)\n",
            "Assigned roles: ['Cartographer', 'Geographic Information Systems (GIS) Specialist', 'Spatial Analyst']\n",
            "---------ROLE ASSIGNMENT COMPLETE--------\n",
            "\n",
            "---------LOCAL RESPONSE--------\n",
            "\n",
            "**Step-by-Step Tutorial: Defining and Identifying a Point in Geography**\n",
            "\n",
            "**Step 1: Understanding the Concept of a Point**\n",
            "\n",
            "A point in geography is a fundamental geographic feature that represents a specific location on the Earth's surface. It is a fundamental unit of measurement in cartography, used to identify and locate features on a map. A point can be a physical location, such as a city, a mountain, or a river, or a conceptual location, such as a boundary or a landmark.\n",
            "\n",
            "**Step 2: Characteristics of a Point**\n",
            "\n",
            "A point has several key characteristics that distinguish it from other geographic features:\n",
            "\n",
            "1. **Location**: A point has a specific location on the Earth's surface, defined by its latitude and longitude coordinates.\n",
            "2. **Size**: A point is a single, discrete location, with no size or extent.\n",
            "3. **Shape**: A point is a single, unchanging shape, with no complexity or variation.\n",
            "4. **Boundary**: A point does not have a boundary, as it is a single location.\n",
            "\n",
            "**Step 3: Identifying a Point**\n",
            "\n",
            "To identify a point, you need to determine its location on the Earth's surface. This can be done using various methods, including:\n",
            "\n",
            "1. **Latitude and Longitude**: Use a GPS device or online mapping tools to determine the latitude and longitude coordinates of the point.\n",
            "2. **Map Reading**: Use a map to identify the point, paying attention to the scale, projection, and symbols used.\n",
            "3. **Field Observation**: Conduct a field observation to identify the point, taking note of its physical characteristics and surroundings.\n",
            "\n",
            "**Step 4: Applications of Points**\n",
            "\n",
            "Points have numerous applications in geography and cartography, including:\n",
            "\n",
            "1. **Boundary Definition**: Points are used to define boundaries between countries, states, or territories.\n",
            "2. **Landmark Identification**: Points are used to identify landmarks, such as monuments, statues, or buildings.\n",
            "3. **Route Planning**: Points are used to plan routes, such as navigation routes or travel itineraries.\n",
            "4. **Data Analysis**: Points are used to analyze geographic data, such as population density or economic activity.\n",
            "\n",
            "**Step 5: Conclusion**\n",
            "\n",
            "In conclusion, a point in geography is a fundamental feature that represents a specific location on the Earth's surface. By understanding the characteristics and applications of points, you can better identify and analyze geographic features, and make informed decisions in a variety of fields.\n",
            "---------EXPERT RESPONSE--------\n",
            "\n",
            "**Step-by-Step Tutorial: Defining and Identifying a Point in Geography**\n",
            "\n",
            "**Step 1: Understanding the Concept of a Point**\n",
            "\n",
            "In geography, a point is a location on the Earth's surface that has a specific set of coordinates, typically represented by latitude and longitude. A point can be thought of as a single location, such as a city, a building, or a specific spot on a map.\n",
            "\n",
            "**Step 2: Characteristics of a Point**\n",
            "\n",
            "A point has several key characteristics:\n",
            "\n",
            "1. **Location**: A point has a specific location on the Earth's surface, defined by its latitude and longitude coordinates.\n",
            "2. **Size**: A point is a single location, with no size or dimension.\n",
            "3. **Shape**: A point is a single point, with no shape or boundary.\n",
            "4. **Topology**: A point is a topological point, meaning it has no neighbors or adjacent points.\n",
            "\n",
            "**Step 3: Types of Points**\n",
            "\n",
            "There are several types of points in geography:\n",
            "\n",
            "1. **Geographic Points**: These are points that represent specific locations on the Earth's surface, such as cities, landmarks, or monuments.\n",
            "2. **Cartographic Points**: These are points that represent specific locations on a map, such as map pins or markers.\n",
            "3. **Spatial Points**: These are points that represent specific locations in space, such as GPS coordinates or satellite imagery.\n",
            "\n",
            "**Step 4: Identifying a Point**\n",
            "\n",
            "To identify a point, you can use various methods:\n",
            "\n",
            "1. **Latitude and Longitude**: Use a GPS device or online mapping tools to determine the latitude and longitude coordinates of a point.\n",
            "2. **Map Pin**: Use a map to identify a point by placing a pin or marker on the location.\n",
            "3. **Satellite Imagery**: Use satellite imagery to identify a point by zooming in on a specific location.\n",
            "\n",
            "**Step 5: Applications of Points**\n",
            "\n",
            "Points have numerous applications in geography and GIS:\n",
            "\n",
            "1. **Navigation**: Points are used in navigation systems, such as GPS devices, to provide location information.\n",
            "2. **Mapping**: Points are used in mapping to represent specific locations on a map.\n",
            "3. **Spatial Analysis**: Points are used in spatial analysis to study patterns and relationships between locations.\n",
            "4. **Emergency Services**: Points are used in emergency services, such as 911 systems, to provide location information.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "In this step-by-step tutorial, we have defined and identified a point in geography, covering its characteristics, types, and applications. Points are a fundamental concept in geography and GIS, and understanding their properties and uses is essential for working with spatial data.\n",
            "---------USER ANALYSIS RESPONSE--------\n",
            "\n",
            "**Step-by-Step Tutorial: Defining and Identifying a Point in Geography**\n",
            "\n",
            "**Step 1: Understanding the Concept of a Point**\n",
            "\n",
            "In geography, a point is a location on the Earth's surface that has a specific set of coordinates, typically represented by latitude and longitude. A point can be thought of as a single location, rather than an area or a boundary. Points are often used to represent specific features, such as buildings, monuments, or landmarks.\n",
            "\n",
            "**Step 2: Characteristics of a Point**\n",
            "\n",
            "A point in geography has several key characteristics:\n",
            "\n",
            "1. **Location**: A point has a specific location on the Earth's surface, defined by its latitude and longitude coordinates.\n",
            "2. **Size**: A point is a single location, with no size or dimension.\n",
            "3. **Shape**: A point is a single point, with no shape or boundary.\n",
            "4. **Type**: Points can be classified into different types, such as:\n",
            "\t* Geographic points (e.g., a mountain peak)\n",
            "\t* Man-made points (e.g., a building or monument)\n",
            "\t* Abstract points (e.g., a mathematical concept or a theoretical location)\n",
            "\n",
            "**Step 3: Identifying a Point**\n",
            "\n",
            "To identify a point in geography, you can use various methods:\n",
            "\n",
            "1. **Coordinate System**: Use a coordinate system, such as the Universal Transverse Mercator (UTM) or the World Geodetic System (WGS), to specify the location of a point.\n",
            "2. **Map**: Use a map to identify a point, either by tracing the location or by using a map's grid system.\n",
            "3. **GPS**: Use a Global Positioning System (GPS) device to determine the location of a point.\n",
            "4. **Surveying**: Conduct a survey to measure the location of a point using techniques such as triangulation or trilateration.\n",
            "\n",
            "**Step 4: Applications of Points in Geography**\n",
            "\n",
            "Points have numerous applications in geography, including:\n",
            "\n",
            "1. **Location-Based Services**: Points are used in location-based services, such as GPS navigation, to provide users with specific locations.\n",
            "2. **Geographic Information Systems (GIS)**: Points are used in GIS to represent specific locations, such as buildings, roads, or landmarks.\n",
            "3. **Urban Planning**: Points are used in urban planning to identify specific locations, such as buildings, parks, or public transportation hubs.\n",
            "4. **Emergency Services**: Points are used in emergency services, such as search and rescue operations, to quickly locate individuals or objects.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "In this step-by-step tutorial, we have explored the concept of a point in geography, its characteristics, and its applications. Points are a fundamental component of geographic analysis, and understanding how to define and identify them is essential for a wide range of applications in fields such as urban planning, emergency services, and location-based services.\n",
            "---------ROUTING TO SOLUTION-FOCUSED SYNTHESIS--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_answer_type_options_clean(chatbot_history, state, options):\n",
        "    \"\"\"Enhanced to show 'Other' option without duplicate messages\"\"\"\n",
        "    options_text = \"ðŸ“ **What type of answer do you need?**\\n\\n\"\n",
        "    options_text += \"Choose the response format that best fits your needs:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** ðŸ”§ **Other** - Specify your preferred answer format\\n\\n\"\n",
        "    options_text += \"ðŸ‘‡ **Click the corresponding Option button below.**\"\n",
        "\n",
        "    answer_type_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    # IMPORTANT: Don't add to existing history, replace the last message\n",
        "    updated_history = chatbot_history + [answer_type_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvjosRHMODfV"
      },
      "outputs": [],
      "source": [
        "def intelligent_routing_agent(original_query, selected_answer_type, rewritten_query=None):\n",
        "    \"\"\"\n",
        "    Intelligent agent that analyzes the query and answer type to determine\n",
        "    the best synthesis method when it's not in our predefined dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    # First, handle None or invalid inputs\n",
        "    if selected_answer_type is None:\n",
        "        print(\"WARNING: selected_answer_type is None, using LLM to analyze query intent\")\n",
        "        analysis_target = original_query\n",
        "        answer_type_description = \"Unknown - need to analyze query intent\"\n",
        "    else:\n",
        "        analysis_target = f\"Query: {original_query}\\nAnswer Type Requested: {selected_answer_type}\"\n",
        "        answer_type_description = selected_answer_type\n",
        "\n",
        "    routing_prompt = f\"\"\"You are an intelligent routing agent for a collaborative analysis system. Your job is to analyze user queries and determine the best synthesis approach.\n",
        "\n",
        "            AVAILABLE SYNTHESIS METHODS:\n",
        "            1. **DECISION_MAKING**: Use when the user wants evaluation, comparison, recommendations, advice, or needs to make a choice between options. This method provides pro/con analysis and helps with decision-making.\n",
        "\n",
        "            2. **SOLUTION_FOCUSED**: Use when the user wants practical tips, step-by-step solutions, implementation guidance, troubleshooting, or actionable advice. This method focuses on solving problems and providing practical steps.\n",
        "\n",
        "            3. **INFORMATIONAL**: Use when the user wants facts, explanations, overviews, background information, or educational content. This method synthesizes information without forcing decisions or solutions.\n",
        "\n",
        "            ANALYSIS TARGET:\n",
        "            {analysis_target}\n",
        "\n",
        "            ADDITIONAL CONTEXT:\n",
        "            - Original Query: \"{original_query}\"\n",
        "            - Requested Answer Type: \"{answer_type_description}\"\n",
        "            {f'- Rewritten Query: \"{rewritten_query}\"' if rewritten_query else ''}\n",
        "\n",
        "            YOUR TASK:\n",
        "            Analyze the user's intent and determine which synthesis method would best serve their needs.\n",
        "\n",
        "            DECISION CRITERIA:\n",
        "            - Does the user need to make a decision or choose between options? â†’ DECISION_MAKING\n",
        "            - Does the user want practical steps or solutions to implement? â†’ SOLUTION_FOCUSED\n",
        "            - Does the user want to understand or learn about something? â†’ INFORMATIONAL\n",
        "\n",
        "            RESPOND WITH ONLY ONE WORD: \"DECISION_MAKING\", \"SOLUTION_FOCUSED\", or \"INFORMATIONAL\"\n",
        "\n",
        "            Think about what the user really wants as an outcome from their query.\"\"\"\n",
        "\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": routing_prompt}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        routing_decision = response.choices[0].message.content.strip().upper()\n",
        "\n",
        "        # Validate the response\n",
        "        valid_methods = [\"DECISION_MAKING\", \"SOLUTION_FOCUSED\", \"INFORMATIONAL\"]\n",
        "        if routing_decision in valid_methods:\n",
        "            method = routing_decision.lower()\n",
        "            print(f\"ðŸ¤– INTELLIGENT ROUTING: LLM determined '{method}' based on query analysis\")\n",
        "            return method\n",
        "        else:\n",
        "            print(f\"âš ï¸ LLM returned invalid method '{routing_decision}', using fallback analysis\")\n",
        "            return fallback_intelligent_analysis(original_query, selected_answer_type)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in intelligent routing agent: {e}\")\n",
        "        return fallback_intelligent_analysis(original_query, selected_answer_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvnXyVJiODfV"
      },
      "outputs": [],
      "source": [
        "def fallback_intelligent_analysis(original_query, selected_answer_type):\n",
        "    \"\"\"\n",
        "    Fallback intelligent analysis using keyword detection when LLM fails\n",
        "    \"\"\"\n",
        "    query_lower = original_query.lower()\n",
        "    answer_type_lower = (selected_answer_type or \"\").lower()\n",
        "    combined_text = f\"{query_lower} {answer_type_lower}\"\n",
        "\n",
        "    # Decision-making indicators\n",
        "    decision_keywords = [\n",
        "        \"should i\", \"which\", \"better\", \"best\", \"recommend\", \"choose\", \"decide\",\n",
        "        \"compare\", \"versus\", \"vs\", \"pros and cons\", \"advantages\", \"disadvantages\",\n",
        "        \"worth it\", \"advice\", \"suggest\", \"opinion\", \"prefer\", \"evaluation\"\n",
        "    ]\n",
        "\n",
        "    # Solution-focused indicators\n",
        "    solution_keywords = [\n",
        "        \"how to\", \"steps\", \"guide\", \"tutorial\", \"solve\", \"fix\", \"implement\",\n",
        "        \"create\", \"build\", \"make\", \"do\", \"achieve\", \"accomplish\", \"tips\",\n",
        "        \"practical\", \"actionable\", \"process\", \"method\", \"technique\"\n",
        "    ]\n",
        "\n",
        "    # Check for decision-making intent\n",
        "    decision_score = sum(1 for keyword in decision_keywords if keyword in combined_text)\n",
        "    solution_score = sum(1 for keyword in solution_keywords if keyword in combined_text)\n",
        "\n",
        "    if decision_score > solution_score and decision_score > 0:\n",
        "        print(f\"ðŸ” FALLBACK ANALYSIS: Decision-making intent detected (score: {decision_score})\")\n",
        "        return \"decision_making\"\n",
        "    elif solution_score > 0:\n",
        "        print(f\"ðŸ” FALLBACK ANALYSIS: Solution-focused intent detected (score: {solution_score})\")\n",
        "        return \"solution_focused\"\n",
        "    else:\n",
        "        print(f\"ðŸ” FALLBACK ANALYSIS: Informational intent (default)\")\n",
        "        return \"informational\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S71Xg8iODfW"
      },
      "outputs": [],
      "source": [
        "def process_selected_answer_type_intelligent_dynamic(selected_answer_type, selected_topic_intent, original_query, query_id, chatbot_history, state):\n",
        "    \"\"\"Enhanced COLA processing with dynamic UI control - clean version\"\"\"\n",
        "    try:\n",
        "        # DON'T add another confirmation message - use the existing one\n",
        "        # The confirmation is already in chatbot_history from the previous step\n",
        "\n",
        "        # Update state with processing info\n",
        "        if isinstance(state, dict):\n",
        "            state[\"processing_query_id\"] = query_id\n",
        "            state[\"selected_topic_intent\"] = selected_topic_intent\n",
        "            state[\"processing\"] = True\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "        # Process with intelligent routing\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            original_query,\n",
        "            selected_topic_intent,\n",
        "            selected_answer_type,\n",
        "            query_id,\n",
        "            state\n",
        "        )\n",
        "\n",
        "        # FIXED: Replace the confirmation messages with just the final result\n",
        "        # Remove the last two messages (custom selection + processing messages)\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else []\n",
        "\n",
        "        # Add only the final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        # Reset processing state\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield final_history, state, \"\", gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ **Error processing selected answer type:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "\n",
        "        # Clean up confirmation messages and show error\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else chatbot_history\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        # Reset state on error\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield error_history, state, \"\", gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMW07UgUODfW"
      },
      "outputs": [],
      "source": [
        "def rewrite_query_with_dual_intent(original_query, selected_topic, selected_answer_type, state):\n",
        "    \"\"\"\n",
        "    FIXED: Rewrite query and store in state\n",
        "    \"\"\"\n",
        "    # Extract the main answer type from the selection\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "    answer_type_description = selected_answer_type.split('(')[1].strip(')') if '(' in selected_answer_type else \"\"\n",
        "\n",
        "    instruction = f\"\"\"You have an original user query, their selected topic/domain, and their desired answer type.\n",
        "\n",
        "    Your task: Rewrite the query to be more specific and actionable based on these clarifications.\n",
        "\n",
        "    Original query: \"{original_query}\"\n",
        "    Selected topic/domain: \"{selected_topic}\"\n",
        "    Desired answer type: \"{answer_type_main}\"\n",
        "    Answer type context: \"{answer_type_description}\"\n",
        "\n",
        "    Guidelines:\n",
        "    1. Keep the core intent of the original query\n",
        "    2. Make it more specific to the selected topic, as the user has already specified to which term they refer to. DO NOT CHANGE TOPICS, selected topic: {selected_topic}.\n",
        "    3. Frame it to expect the desired answer type\n",
        "    4. Make it actionable and clear\n",
        "    5. Don't change the fundamental question, remain the query in the specified selected topic ({selected_topic})\n",
        "\n",
        "    Return only the rewritten query, nothing else.\"\"\"\n",
        "\n",
        "    working_query = get_completion(instruction)\n",
        "\n",
        "    # Clean up the response\n",
        "    working_query = working_query.strip()\n",
        "    if working_query.startswith('\"') and working_query.endswith('\"'):\n",
        "        working_query = working_query[1:-1]\n",
        "\n",
        "    # Store in state for later use\n",
        "    state[\"working_query\"] = working_query\n",
        "\n",
        "    return working_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWDLq-SBODfW"
      },
      "outputs": [],
      "source": [
        "#GET ROLES\n",
        "def get_roles(query, topic):\n",
        "    max_retries = 100\n",
        "    prompt_roles = f\"\"\"You are a Recruiting Manager. The user has clarified they want to know about: \"{topic}\"\n",
        "\n",
        "                        IMPORTANT CONSTRAINTS:\n",
        "                        - Focus ONLY on the clarified topic: \"{topic}\"\n",
        "                        - IGNORE any ambiguous terms from the original query\n",
        "                        - All 3 expert roles must be specialists within the scope of: \"{topic}\"\n",
        "                        - DO NOT go outside this specific domain\n",
        "\n",
        "                        Provide 3 different expert roles who specialize in \"{topic}\" and can analyze the question from different perspectives within this domain.\n",
        "\n",
        "                        The 3 roles should have complementary expertise areas within \"{topic}\".\n",
        "\n",
        "                        Return ONLY a Python list in this exact format:\n",
        "                        [\"{topic}\", \"expert role 1\", \"expert role 2\", \"expert role 3\"]\n",
        "\n",
        "                        No explanations, no other text, just the list.\"\"\"\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt_roles}]\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"llama-3-8b-instruct\",\n",
        "                messages=messages,\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # CRITICAL FIX: Parse string into actual list\n",
        "            if response_text.startswith('[') and response_text.endswith(']'):\n",
        "                definition_list = eval(response_text)  # Convert string to list\n",
        "                if isinstance(definition_list, list) and len(definition_list) >= 4:\n",
        "                    return definition_list\n",
        "\n",
        "            raise ValueError(\"Invalid response format\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                # Fallback\n",
        "                return [\n",
        "                    \"General Knowledge\",\n",
        "                    \"Subject Matter Expert\",\n",
        "                    \"Technical Specialist\",\n",
        "                    \"End User Analyst\"\n",
        "                ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp2ueNz_ODfW"
      },
      "outputs": [],
      "source": [
        "def route_synthesis_by_answer_type_intelligent(selected_answer_type, original_query = None, rewritten_query=None):\n",
        "    \"\"\"\n",
        "    CLEAN FLOW: Try dictionary first, then LLM if no match found\n",
        "\n",
        "    Args:\n",
        "        selected_answer_type: The user's selected answer type (can be None)\n",
        "        original_query: Original user query (for LLM context)\n",
        "        rewritten_query: Rewritten query (for additional LLM context)\n",
        "\n",
        "    Returns:\n",
        "        str: One of 'decision_making', 'solution_focused', or 'informational'\n",
        "    \"\"\"\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip().lower()\n",
        "\n",
        "    if answer_type_main is None:\n",
        "        print(\"âš ï¸  No answer type provided - using LLM to analyze query intent\")\n",
        "        return intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "\n",
        "    if not isinstance(selected_answer_type, str) or len(selected_answer_type.strip()) == 0:\n",
        "        print(f\"âš ï¸  Invalid answer type: {selected_answer_type} - using LLM analysis\")\n",
        "        return intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "    try:\n",
        "        # Extract main answer type (remove parenthetical descriptions)\n",
        "        answer_type_main = selected_answer_type.split('(')[0].strip().lower()\n",
        "        print(f\"ðŸ” Checking dictionary for: '{answer_type_main}'\")\n",
        "\n",
        "        # Comprehensive predefined routing dictionary\n",
        "        synthesis_routing = {\n",
        "            # === DECISION-MAKING PATTERNS ===\n",
        "            'comparison': 'DECISION_MAKING',\n",
        "            'recommendation': 'DECISION_MAKING',\n",
        "            'advice': 'DECISION_MAKING',\n",
        "            'evaluation': 'DECISION_MAKING',\n",
        "            'assessment': 'DECISION_MAKING',\n",
        "            'pros and cons': 'DECISION_MAKING',\n",
        "            'which is better': 'DECISION_MAKING',\n",
        "            'should i': 'DECISION_MAKING',\n",
        "            'best option': 'DECISION_MAKING',\n",
        "            'choose': 'DECISION_MAKING',\n",
        "            'decision': 'DECISION_MAKING',\n",
        "            'versus': 'DECISION_MAKING',\n",
        "            'vs': 'DECISION_MAKING',\n",
        "\n",
        "            # === SOLUTION-FOCUSED PATTERNS ===\n",
        "            'practical tips': 'SOLUTION_FOCUSED',\n",
        "            'problem-solving': 'SOLUTION_FOCUSED',\n",
        "            'how to': 'SOLUTION_FOCUSED',\n",
        "            'step by step': 'SOLUTION_FOCUSED',\n",
        "            'guide': 'SOLUTION_FOCUSED',\n",
        "            'tutorial': 'SOLUTION_FOCUSED',\n",
        "            'implementation': 'SOLUTION_FOCUSED',\n",
        "            'troubleshooting': 'SOLUTION_FOCUSED',\n",
        "            'fix': 'SOLUTION_FOCUSED',\n",
        "            'solve': 'SOLUTION_FOCUSED',\n",
        "            'create': 'SOLUTION_FOCUSED',\n",
        "            'build': 'SOLUTION_FOCUSED',\n",
        "            'actionable': 'SOLUTION_FOCUSED',\n",
        "\n",
        "            # === INFORMATIONAL PATTERNS ===\n",
        "            'expert analysis': 'INFORMATIONAL',\n",
        "            'informative': 'INFORMATIONAL',\n",
        "            'basic overview': 'INFORMATIONAL',\n",
        "            'explanation': 'INFORMATIONAL',\n",
        "            'background': 'INFORMATIONAL',\n",
        "            'what is': 'INFORMATIONAL',\n",
        "            'overview': 'INFORMATIONAL',\n",
        "            'summary': 'INFORMATIONAL',\n",
        "            'details': 'INFORMATIONAL',\n",
        "            'information': 'INFORMATIONAL',\n",
        "            'facts': 'INFORMATIONAL',\n",
        "            'learn': 'INFORMATIONAL',\n",
        "            'understand': 'INFORMATIONAL'\n",
        "        }\n",
        "\n",
        "        # Check exact match first\n",
        "        if answer_type_main in synthesis_routing:\n",
        "            synthesis_method = synthesis_routing[answer_type_main]\n",
        "            print(f\"âœ… DICTIONARY MATCH: '{answer_type_main}' â†’ '{synthesis_method}'\")\n",
        "            return synthesis_method\n",
        "\n",
        "        # Check partial matches (more flexible matching)\n",
        "        for key, method in synthesis_routing.items():\n",
        "            if key in answer_type_main or answer_type_main in key:\n",
        "                print(f\"âœ… DICTIONARY PARTIAL MATCH: '{answer_type_main}' matched '{key}' â†’ '{method}'\")\n",
        "                return method\n",
        "\n",
        "        # No dictionary match found\n",
        "        print(f\"âŒ NO DICTIONARY MATCH for '{answer_type_main}'\")\n",
        "        print(\"ðŸ¤– Calling LLM for intelligent routing...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR in dictionary lookup: {e}\")\n",
        "        print(\"ðŸ¤– Falling back to LLM routing...\")\n",
        "\n",
        "    # ========================================\n",
        "    # STEP 3: LLM ROUTING (when dictionary fails)\n",
        "    # ========================================\n",
        "    return intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu5xvck9fjdz"
      },
      "outputs": [],
      "source": [
        "def local_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced local analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "\n",
        "    # Extract the main answer type from the selection\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with deep knowledge about {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    As a {role}, provide your professional analysis addressing this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your response should reflect the expertise and viewpoint that defines your role as a {role} while following the requested answer format.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def expert_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced expert analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} specializing in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Provide your professional expert analysis of this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should reflect the authority and comprehensive understanding that comes from being a recognized {role} in this domain.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def user_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    \"\"\"Enhanced user analysis with answer type consideration\"\"\"\n",
        "    role = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with expertise in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Analyze this query from your specialized perspective as a {role}.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should complement other expert perspectives while offering the distinct value that only a {role} can provide.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZyaO3puODfX"
      },
      "outputs": [],
      "source": [
        "def get_answer_type_instructions(answer_type):\n",
        "    \"\"\"Get specific instructions based on answer type\"\"\"\n",
        "    instructions = {\n",
        "        \"Informative\": \"\"\"\n",
        "        - Provide comprehensive background information and facts\n",
        "        - Include detailed explanations and context\n",
        "        - Cover multiple aspects of the topic\n",
        "        - Use evidence and examples to support points\n",
        "        - Structure information clearly and logically\"\"\",\n",
        "\n",
        "        \"Practical Tips\": \"\"\"\n",
        "        - Focus on actionable advice and guidance\n",
        "        - Provide step-by-step instructions where applicable\n",
        "        - Include specific recommendations and best practices\n",
        "        - Emphasize what the user can actually do\n",
        "        - Make suggestions concrete and implementable\"\"\",\n",
        "\n",
        "        \"Basic Overview\": \"\"\"\n",
        "        - Keep explanations simple and accessible\n",
        "        - Focus on key points and essential information\n",
        "        - Avoid technical jargon or complex details\n",
        "        - Provide a clear, easy-to-understand introduction\n",
        "        - Structure information in a beginner-friendly way\"\"\",\n",
        "\n",
        "        \"Expert Analysis\": \"\"\"\n",
        "        - Provide in-depth professional perspective\n",
        "        - Include technical details and advanced insights\n",
        "        - Reference industry standards and best practices\n",
        "        - Demonstrate specialized knowledge and expertise\n",
        "        - Address complex aspects and nuances\"\"\",\n",
        "\n",
        "        \"Comparison\": \"\"\"\n",
        "        - Present pros and cons clearly\n",
        "        - Compare different options or approaches\n",
        "        - Provide balanced evaluation of alternatives\n",
        "        - Include recommendations based on comparison\n",
        "        - Help user understand trade-offs\"\"\",\n",
        "\n",
        "        \"Problem-Solving\": \"\"\"\n",
        "        - Focus on solutions and troubleshooting\n",
        "        - Address specific issues and challenges\n",
        "        - Provide practical problem-solving approaches\n",
        "        - Include preventive measures where applicable\n",
        "        - Emphasize resolution strategies\"\"\"\n",
        "    }\n",
        "\n",
        "    return instructions.get(answer_type, instructions[\"Informative\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wosOiJRJODfX"
      },
      "outputs": [],
      "source": [
        "def stance_analysis_enhanced(query, ling_response, expert_response, user_response, topic, stance, answer_type, state=None):\n",
        "    \"\"\"Enhanced stance analysis that considers answer type\"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "\n",
        "    stance_context = {\n",
        "        \"positive\": \"highly beneficial, well-founded, and strongly recommended\",\n",
        "        \"negative\": \"problematic, risky, or not advisable\"\n",
        "    }\n",
        "\n",
        "    stance_description = stance_context.get(stance, stance)\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are conducting stance detection analysis for collaborative decision-making.\n",
        "\n",
        "    Original Query: '''{query}'''\n",
        "    Topic: {topic}\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR STANCE: You believe the approaches, recommendations, or solutions presented in response to this query are {stance_description} for the user's situation regarding {topic}.\n",
        "\n",
        "    IMPORTANT: Your argument should be formatted as {answer_type_main.upper()} since that's what the user requested.\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    TASK:\n",
        "    1. **Analyze all three expert perspectives** through your {stance} lens\n",
        "    2. **Extract supporting evidence** that supports your {stance} position\n",
        "    3. **Build your argument** using evidence while following the {answer_type_main} format\n",
        "\n",
        "    Present your {stance} argument with specific evidence from the expert analyses, formatted according to the user's requested answer type.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC0B2_ITODfX"
      },
      "outputs": [],
      "source": [
        "def final_judgement(query, favor_response, against_response, topic):\n",
        "    \"\"\"\n",
        "    Enhanced final judgement that synthesizes collaborative analysis\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query. This means:\n",
        "\n",
        "    1. **Evaluate evidence quality**: Assess the strength and credibility of arguments from both sides\n",
        "    2. **Consider user context**: Focus on what would be most beneficial for someone asking this specific query\n",
        "    3. **Balance perspectives**: Integrate the strongest insights from both positive and negative analyses\n",
        "    4. **Provide actionable guidance**: Give the user clear, practical direction\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Deliver a comprehensive yet concise response\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Focus on practical value for the user\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Present as the authoritative answer to their query\n",
        "    - Give a brief, practical response (1-2 paragraphs).\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7alYPKN7fnYQ"
      },
      "outputs": [],
      "source": [
        "def final_judgement_enhanced(query, favor_response, against_response, topic, answer_type):\n",
        "    \"\"\"Enhanced final judgement that considers answer type\"\"\"\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "    REQUESTED ANSWER TYPE: {answer_type}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query.\n",
        "\n",
        "    CRITICAL: Format your response as {answer_type_main.upper()} as specifically requested by the user:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Follow the {answer_type_main} format strictly\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Focus on practical value for the user\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Present as the authoritative answer to their query\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process, delivered in exactly the format the user requested.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6FBTa1uODfX"
      },
      "outputs": [],
      "source": [
        "def summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    \"\"\"\n",
        "    Summary synthesis agent - extracts and synthesizes key facts without forcing recommendations.\n",
        "    Used for informational answer types: Informative, Basic Overview, Expert Analysis\n",
        "    \"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Summary Synthesis Agent. Your role is to extract and synthesize the most relevant and important information from multiple expert analyses.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a comprehensive, factual response that directly answers the user's query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SYNTHESIS GUIDELINES:\n",
        "    1. **Extract key facts**: Pull the most important information from all three experts\n",
        "    2. **Identify consensus**: Where experts agree, present this as reliable information\n",
        "    3. **Note different perspectives**: Where experts offer different viewpoints, present both\n",
        "    4. **Stay factual**: Focus on information, not recommendations\n",
        "    5. **Be comprehensive**: Cover all important aspects mentioned by the experts\n",
        "    6. **Maintain objectivity**: Present information neutrally without bias\n",
        "    7. **Structure clearly**: Organize information logically for easy understanding\n",
        "\n",
        "    DO NOT:\n",
        "    - Force recommendations when the user wants information\n",
        "    - Create artificial pros/cons lists for factual queries\n",
        "    - Add opinions where experts provided facts\n",
        "    - Turn factual content into advice\n",
        "\n",
        "    Provide a well-structured synthesis that gives the user exactly the type of information they requested.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RUiqdD_ODfX"
      },
      "outputs": [],
      "source": [
        "def solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    \"\"\"\n",
        "    Solution synthesis agent - focuses on practical solutions and implementation.\n",
        "    Used for solution-focused answer types: Practical Tips, Problem-Solving\n",
        "    \"\"\"\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Solution Synthesis Agent. Your role is to synthesize expert analyses into practical, actionable solutions.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a practical, solution-focused response.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SOLUTION GUIDELINES:\n",
        "    1. **Identify the core need**: What is the user trying to accomplish?\n",
        "    2. **Extract actionable advice**: Pull practical steps and recommendations from experts\n",
        "    3. **Prioritize solutions**: Present the most effective approaches first\n",
        "    4. **Consider implementation**: Include practical considerations for execution\n",
        "    5. **Address potential challenges**: Note important limitations or considerations\n",
        "    6. **Provide clear guidance**: Make recommendations specific and actionable\n",
        "    7. **Focus on outcomes**: Help user understand what success looks like\n",
        "\n",
        "    Focus on giving the user a clear path forward based on the expert analyses.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SIbeQC9hDI",
        "outputId": "b39b748d-ef4c-4c9f-e94e-6c772993f3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private LLM Response: Artificial Intelligence (AI) processes data through algorithms that:\n",
            "\n",
            "1. **Learn** from patterns in data\n",
            "2. **Recognize** relationships and make predictions\n",
            "3. **Improve** accuracy through iterative refining\n",
            "\n",
            "Think of AI like a super-smart, self-improving calculator that gets better over time!\n"
          ]
        }
      ],
      "source": [
        "# Test your private LLM\n",
        "def test_private_llm():\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain how AI works in a few words\"}\n",
        "        ]\n",
        "    )\n",
        "    print(\"Private LLM Response:\", response.choices[0].message.content)\n",
        "\n",
        "test_private_llm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHVfzy8fsM3"
      },
      "outputs": [],
      "source": [
        "search_tool = GoogleSearchAPIWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdXT_rhNODfY"
      },
      "outputs": [],
      "source": [
        "def format_response_with_sources(enhanced_response, source_urls):\n",
        "    \"\"\"\n",
        "    IMPROVED: Format the response with clear, visible source links\n",
        "    \"\"\"\n",
        "    if not source_urls:\n",
        "        sources_section = \"\\n\\n---\\n\\nðŸ“š **Note:** Search was performed but specific source URLs could not be extracted. Information has been validated against current web content.\"\n",
        "    else:\n",
        "        sources_section = \"\\n\\n---\\n\\nðŸ“š **Sources Used for This Update:**\\n\\n\"\n",
        "\n",
        "        for i, url in enumerate(source_urls, 1):\n",
        "            try:\n",
        "                from urllib.parse import urlparse\n",
        "                parsed = urlparse(url)\n",
        "                domain = parsed.netloc if parsed.netloc else parsed.path\n",
        "                if domain.startswith('www.'):\n",
        "                    domain = domain[4:]\n",
        "\n",
        "                # Format as clickable link\n",
        "                sources_section += f\"{i}. [{domain}]({url})\\n\"\n",
        "            except:\n",
        "                sources_section += f\"{i}. {url}\\n\"\n",
        "\n",
        "        sources_section += \"\\n*Click on the links above to visit the sources.*\"\n",
        "\n",
        "    return enhanced_response + sources_section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9p9sXiW9hDK"
      },
      "outputs": [],
      "source": [
        "def simple_rag_with_private_llm(query):\n",
        "    \"\"\"\n",
        "    FIXED: RAG with improved source extraction and display\n",
        "    \"\"\"\n",
        "    print(f\"RAG processing query: {query[:100]}...\")\n",
        "\n",
        "    try:\n",
        "        # Extract simple search terms from the query\n",
        "        search_query = query #test\n",
        "        print(f\"Extracted search query: {search_query}\")\n",
        "\n",
        "        # Perform web search using structured results\n",
        "        raw_results = search_tool.results(search_query, num_results=5)\n",
        "\n",
        "        # Extract URLs from structured search results\n",
        "        source_urls = [r['link'] for r in raw_results if 'link' in r]\n",
        "        print(f\"Extracted {len(source_urls)} source URLs:\")\n",
        "        for url in source_urls:\n",
        "            print(f\"  - {url}\")\n",
        "\n",
        "        # Generate a readable text summary for the LLM\n",
        "        search_results = \"\\n\".join(\n",
        "        [f\"{r['title']}: {r['snippet']}\" for r in raw_results if 'title' in r and 'snippet' in r]\n",
        "        )\n",
        "\n",
        "        print(f\"Search results preview:\\n{search_results[:300]}...\")\n",
        "\n",
        "        # Check if search was successful\n",
        "        if not search_results.strip():\n",
        "            print(\"No substantial search results found.\")\n",
        "            return \"Based on current information, the original expert recommendation remains valid.\\n\\n---\\n\\nðŸ“š **Note:** Web search was performed but no relevant results were found.\"\n",
        "\n",
        "        # Enhanced RAG prompt\n",
        "        rag_prompt = f\"\"\"Based on the search results, enhance and validate the expert recommendation.\n",
        "\n",
        "        ORIGINAL EXPERT RECOMMENDATION: {query}\n",
        "\n",
        "        CURRENT SEARCH RESULTS:\n",
        "        {search_results}\n",
        "\n",
        "        TASK: Use these search results to validate, update, and enhance the expert recommendation. Focus on:\n",
        "        - Current accuracy of the information\n",
        "        - Recent developments or changes\n",
        "        - Specific details that improve the recommendation\n",
        "        - Any corrections needed based on current data\n",
        "\n",
        "        Provide a clear, enhanced recommendation that incorporates the latest information.\n",
        "\n",
        "        IMPORTANT: Provide the links you used to update the information.\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research analyst who validates expert recommendations using current search results. Do not include URLs or sources in your response.\"},\n",
        "                {\"role\": \"user\", \"content\": rag_prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        enhanced_response = response.choices[0].message.content\n",
        "\n",
        "        # Format response with improved source display\n",
        "        final_response_with_sources = format_response_with_sources(enhanced_response, source_urls)\n",
        "\n",
        "        return final_response_with_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"RAG search error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"Unable to retrieve current information for validation.\\n\\n---\\n\\nâš ï¸ **Error Details:** {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqYMaHJBODfY"
      },
      "outputs": [],
      "source": [
        "def define_roles(definition_list, state):\n",
        "    \"\"\"\n",
        "    FIXED: Define roles using state instead of global\n",
        "    \"\"\"\n",
        "    # REMOVE: global target_role_map\n",
        "    # USE STATE INSTEAD:\n",
        "    state[\"target_role_map\"] = {\n",
        "        \"Local\": definition_list[1],\n",
        "        \"Expert\": definition_list[2],\n",
        "        \"User Analysis\": definition_list[3]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk82xbOUfxny"
      },
      "outputs": [],
      "source": [
        "def add_predictions_sequential_intelligent(original_query, selected_intent, selected_answer_type, query_id, state):\n",
        "    \"\"\"\n",
        "    FIXED: Enhanced COLA framework using state instead of global variables\n",
        "    \"\"\"\n",
        "    # REMOVE: global current_processing_query_id, topic\n",
        "    # USE STATE INSTEAD:\n",
        "    state[\"processing_query_id\"] = query_id  # STORE the current query ID for RAG\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Processing query {query_id}\")\n",
        "    print(f\"Original query: {original_query}\")\n",
        "    print(f\"Selected intent: {selected_intent}\")\n",
        "    print(f\"Selected answer type: {selected_answer_type}\")\n",
        "    print(\"---------DUAL INTENT CLARIFICATION COMPLETE--------\\n\")\n",
        "\n",
        "    # STEP 1: Rewrite query using both intents\n",
        "    working_query = rewrite_query_with_dual_intent(original_query, selected_intent, selected_answer_type, state)\n",
        "    state[\"working_query\"] = working_query  # STORE in state\n",
        "    print(f\"Rewritten query: {working_query}\")\n",
        "    print(\"---------QUERY REWRITING COMPLETE--------\\n\")\n",
        "\n",
        "    # STEP 2: CLEAN ROUTING FLOW\n",
        "    print(\"ðŸš¦ ROUTING ANALYSIS STARTING...\")\n",
        "    synthesis_method = route_synthesis_by_answer_type_intelligent(\n",
        "    selected_answer_type=selected_answer_type,\n",
        "    original_query=original_query,\n",
        "    rewritten_query=working_query  # Pass rewritten query for context\n",
        "    )\n",
        "    print(f\"ðŸŽ¯ FINAL ROUTING DECISION: '{synthesis_method}'\")\n",
        "    print(\"---------INTELLIGENT ROUTING COMPLETE--------\\n\")\n",
        "\n",
        "    # STEP 3: Get roles and topic based on the rewritten query\n",
        "    definition_list = get_roles(working_query, selected_intent)  # Pass state\n",
        "    topic = definition_list[0]\n",
        "    state[\"topic\"] = topic  # STORE in state\n",
        "    define_roles(definition_list, state)  # Pass state\n",
        "\n",
        "    print(f\"Identified topic: {topic}\")\n",
        "    print(f\"Assigned roles: {definition_list[1:]}\")\n",
        "    print(\"---------ROLE ASSIGNMENT COMPLETE--------\\n\")\n",
        "\n",
        "    # STEP 4: Expert analysis - pass state to all functions\n",
        "    ling_response = local_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "    print(\"---------LOCAL RESPONSE--------\\n\")\n",
        "    print(ling_response)\n",
        "\n",
        "    expert_response = expert_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "    print(\"---------EXPERT RESPONSE--------\\n\")\n",
        "    print(expert_response)\n",
        "\n",
        "    user_response = user_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "    print(\"---------USER ANALYSIS RESPONSE--------\\n\")\n",
        "    print(user_response)\n",
        "\n",
        "    # STEP 5: INTELLIGENT ROUTING - Choose synthesis method based on answer type\n",
        "    if synthesis_method == 'DECISION_MAKING':          # â† HERE IS THE CONDITIONAL CHECK\n",
        "        print(\"---------ROUTING TO DECISION-MAKING FRAMEWORK--------\\n\")\n",
        "        # Original COLA flow for decision-making (Comparison answer type)\n",
        "        favor_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"positive\", selected_answer_type,state)\n",
        "        print(\"---------IN FAVOR RESPONSE--------\\n\")\n",
        "        print(favor_response)\n",
        "\n",
        "        against_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"negative\", selected_answer_type,state)\n",
        "        print(\"---------AGAINST RESPONSE--------\\n\")\n",
        "        print(against_response)\n",
        "\n",
        "        final_response = final_judgement_enhanced(working_query, favor_response, against_response, topic, selected_answer_type)\n",
        "        print(\"---------DECISION-MAKING FINAL JUDGEMENT--------\\n\")\n",
        "\n",
        "    elif synthesis_method == 'SOLUTION_FOCUSED':       # â† Alternative path\n",
        "        print(\"---------ROUTING TO SOLUTION-FOCUSED SYNTHESIS--------\\n\")\n",
        "        final_response = solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "        print(\"---------SOLUTION-FOCUSED SYNTHESIS--------\\n\")\n",
        "\n",
        "    else:   #synthesis_method == 'INFORMATIONAL'       # â† Default path (most common)\n",
        "        print(\"---------ROUTING TO INFORMATIONAL SYNTHESIS--------\\n\")\n",
        "        final_response = summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "        print(\"---------INFORMATIONAL SYNTHESIS--------\\n\")\n",
        "\n",
        "\n",
        "    print(\"---------FINAL RESPONSE--------\\n\")\n",
        "    print(final_response)\n",
        "\n",
        "    # Calculate processing time\n",
        "    end_time = time.time()\n",
        "    processing_time = round(end_time - start_time, 2)\n",
        "    print(f\"Total processing time: {processing_time} seconds\")\n",
        "\n",
        "    # Prepare results for database update\n",
        "    results = {\n",
        "        'Original_Query': str(original_query),\n",
        "        'Rewritten_Query': str(working_query),\n",
        "        'Selected_Topic_Intent': str(selected_intent),\n",
        "        'Selected_Answer_Type': str(selected_answer_type),\n",
        "        'Synthesis_Method': str(synthesis_method),\n",
        "        'Identified_Topic': str(topic),\n",
        "        'Local Analysis': str(ling_response),\n",
        "        'Expert Analysis': str(expert_response),\n",
        "        'User Analysis': str(user_response),\n",
        "        'Final Judgement': str(final_judgement),\n",
        "        'Processing_Time_Seconds': processing_time,\n",
        "    }\n",
        "\n",
        "    update_query_results(query_id, results)\n",
        "    return final_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neM42IDlODfZ"
      },
      "outputs": [],
      "source": [
        "def get_last_query_data():\n",
        "    \"\"\"\n",
        "    Get the last query data from database - handles both old and new schema\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        if df.empty:\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Debug: Print column names to see what we actually have\n",
        "        print(f\"DEBUG - Database columns: {list(df.columns)}\")\n",
        "\n",
        "        # Try completed queries first, fallback to any query\n",
        "        completed_queries = df[df['Status'] == 'completed']\n",
        "        if not completed_queries.empty:\n",
        "            last_query = completed_queries.iloc[-1]\n",
        "        else:\n",
        "            last_query = df.iloc[-1]\n",
        "\n",
        "        query_id = int(last_query['ID'])\n",
        "\n",
        "        # Handle both old and new database schemas\n",
        "        if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "            # New schema\n",
        "            print(\"DEBUG - Using new schema (Original_Query, Rewritten_Query)\")\n",
        "            original_query = str(last_query['Original_Query']) if pd.notna(last_query['Original_Query']) else \"Original query not available\"\n",
        "            rewritten_query = str(last_query['Rewritten_Query']) if pd.notna(last_query['Rewritten_Query']) else original_query\n",
        "        elif 'Query' in df.columns:\n",
        "            # Old schema - fallback to 'Query' column\n",
        "            print(\"DEBUG - Using old schema (Query)\")\n",
        "            query_value = str(last_query['Query']) if pd.notna(last_query['Query']) else \"Query not available\"\n",
        "            original_query = query_value\n",
        "            rewritten_query = query_value  # Use same value for both\n",
        "        else:\n",
        "            print(\"DEBUG - No recognized query columns found\")\n",
        "            print(f\"Available columns: {list(df.columns)}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Handle different possible column names for final response\n",
        "        if 'Final Judgement' in df.columns and pd.notna(last_query['Final Judgement']):\n",
        "            final_response = str(last_query['Final Judgement'])\n",
        "        elif 'Final_Judgement' in df.columns and pd.notna(last_query['Final_Judgement']):\n",
        "            final_response = str(last_query['Final_Judgement'])\n",
        "        else:\n",
        "            final_response = rewritten_query\n",
        "\n",
        "        print(f\"DEBUG - Returning: query_id={query_id}, rewritten_query='{rewritten_query[:50]}...', original_query='{original_query[:50]}...'\")\n",
        "        return query_id, rewritten_query, final_response, original_query\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving last query data: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TRNfy61ODfZ"
      },
      "outputs": [],
      "source": [
        "def execute_rag_update(chatbot_history, state):\n",
        "    \"\"\"\n",
        "    IMPROVED: State-based RAG with comprehensive error handling\n",
        "    No database dependency during execution - uses session state directly\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ðŸ”„ RAG Enhancement: Starting state-based processing...\")\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 1: Check if state exists and is properly structured\n",
        "        # ========================================\n",
        "        if not state or not isinstance(state, dict):\n",
        "            print(\"âŒ No valid state found\")\n",
        "            error_msg = \"âŒ **No active session found.** Please start a new query to use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state or {}, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 2: Check if user has submitted a query through COLA\n",
        "        # ========================================\n",
        "        original_query = state.get(\"original_query\", \"\")\n",
        "        working_query = state.get(\"working_query\", \"\")\n",
        "        processing_query_id = state.get(\"processing_query_id\")\n",
        "\n",
        "        # Enhanced validation for pre-query clicks\n",
        "        if not original_query and not working_query and not processing_query_id:\n",
        "            print(\"âŒ User clicked RAG before submitting any query\")\n",
        "            error_msg = \"\"\"âŒ **No query to enhance yet!**\n",
        "\n",
        "                        Please follow these steps:\n",
        "                        1. ðŸ“ **Submit your question** in the text box above\n",
        "                        2. ðŸŽ¯ **Select your preferred topic focus** (Option 1, 2, or 3)\n",
        "                        3. ðŸ“‹ **Choose your answer type** (Option 1, 2, or 3)\n",
        "                        4. â³ **Wait for the analysis to complete**\n",
        "                        5. ðŸ”„ **Then click this button** to enhance with current information\n",
        "\n",
        "                        *The RAG enhancement works best after you've received an initial analysis.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 3: Check if COLA processing is complete\n",
        "        # ========================================\n",
        "        if original_query and not working_query:\n",
        "            print(\"âš ï¸ Query exists but COLA processing may be incomplete\")\n",
        "            error_msg = \"\"\"âš ï¸ **Analysis still in progress!**\n",
        "\n",
        "                        Your query is being processed through the COLA framework. Please:\n",
        "                        - ðŸŽ¯ **Complete the topic selection** if prompted\n",
        "                        - ðŸ“‹ **Complete the answer type selection** if prompted\n",
        "                        - â³ **Wait for the initial analysis to finish**\n",
        "\n",
        "                        *RAG enhancement will be available once the analysis is complete.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 4: Check if there's content to enhance\n",
        "        # ========================================\n",
        "        if not chatbot_history or len(chatbot_history) == 0:\n",
        "            print(\"âŒ No chat history to enhance\")\n",
        "            error_msg = \"âŒ **No conversation history found.** Please submit a query first, then use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # Get the most recent assistant response to enhance\n",
        "        last_assistant_response = \"\"\n",
        "        for msg in reversed(chatbot_history):\n",
        "            if msg.get(\"role\") == \"assistant\" and msg.get(\"content\"):\n",
        "                content = msg.get(\"content\", \"\")\n",
        "                # Skip RAG-related messages to get the actual analysis\n",
        "                if not content.startswith(\"ðŸ”„\") and not content.startswith(\"âœ…\") and not content.startswith(\"âŒ\"):\n",
        "                    last_assistant_response = content\n",
        "                    break\n",
        "\n",
        "        if not last_assistant_response:\n",
        "            print(\"âŒ No assistant response found to enhance\")\n",
        "            error_msg = \"\"\"âŒ **No analysis found to enhance.**\n",
        "\n",
        "                        Please ensure you have:\n",
        "                        1. âœ… **Submitted a complete query**\n",
        "                        2. âœ… **Received an analysis response**\n",
        "                        3. âœ… **Completed the COLA framework process**\n",
        "\n",
        "                        *Then try the RAG enhancement again.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # SUCCESSFUL VALIDATION: Proceed with RAG\n",
        "        # ========================================\n",
        "        print(f\"âœ… Validation passed - enhancing query: '{original_query[:50]}...'\")\n",
        "        print(f\"ðŸ“ Working query: '{working_query[:50]}...'\")\n",
        "        print(f\"ðŸ“Š Last response length: {len(last_assistant_response)} characters\")\n",
        "\n",
        "        # Show processing message with helpful context\n",
        "        processing_msg = f\"\"\"ðŸ”„ **Enhancing your analysis with current information...**\n",
        "\n",
        "                        **Your Query:** {original_query[:100]}{'...' if len(original_query) > 100 else ''}\n",
        "\n",
        "                        ðŸ” Searching for the latest information to update and validate the analysis...\"\"\"\n",
        "\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        temp_history = chatbot_history + [processing_message]\n",
        "\n",
        "        yield temp_history, state, \"\"\n",
        "\n",
        "        # ========================================\n",
        "        # RAG ENHANCEMENT: Use state data directly\n",
        "        # ========================================\n",
        "        print(\"ðŸ¤– Starting RAG enhancement with state data...\")\n",
        "\n",
        "        try:\n",
        "            enhanced_response = enhanced_rag_with_session_state(\n",
        "                original_query=original_query,\n",
        "                working_query=working_query,\n",
        "                previous_analysis=last_assistant_response,\n",
        "                state=state\n",
        "            )\n",
        "\n",
        "            print(\"âœ… RAG enhancement completed successfully\")\n",
        "\n",
        "            # Store RAG results in state for this session\n",
        "            state[\"rag_enhanced_response\"] = enhanced_response\n",
        "            state[\"rag_timestamp\"] = time.time()\n",
        "            state[\"rag_original_query\"] = original_query\n",
        "\n",
        "        except Exception as rag_error:\n",
        "            print(f\"âŒ RAG processing failed: {rag_error}\")\n",
        "            enhanced_response = f\"\"\"**RAG Enhancement Notice:**\n",
        "\n",
        "                            The current information lookup encountered an issue: {str(rag_error)}\n",
        "\n",
        "                            **Your original analysis remains valid and complete.** This enhancement failure doesn't affect the quality of the previous response.\n",
        "\n",
        "                            *You can try the enhancement again or continue with the existing analysis.*\"\"\"\n",
        "\n",
        "        # ========================================\n",
        "        # PRESENT ENHANCED RESULTS\n",
        "        # ========================================\n",
        "        success_msg = f\"\"\"âœ… **Analysis Enhanced with Current Information!**\n",
        "\n",
        "                            {enhanced_response}\n",
        "\n",
        "                            ---\n",
        "                            *ðŸ’¡ This response combines your original analysis with the latest available information for accuracy and relevance.*\"\"\"\n",
        "\n",
        "        final_message = {\"role\": \"assistant\", \"content\": success_msg}\n",
        "        updated_history = chatbot_history + [final_message]\n",
        "\n",
        "        # ========================================\n",
        "        # OPTIONAL: Background database logging (non-blocking)\n",
        "        # ========================================\n",
        "        try:\n",
        "            if processing_query_id:\n",
        "                # This is just for logging - doesn't affect RAG functionality\n",
        "                background_database_logging(processing_query_id, enhanced_response)\n",
        "        except Exception as db_error:\n",
        "            print(f\"âš ï¸ Database logging failed (non-critical): {db_error}\")\n",
        "            # Don't show this error to user - it's just logging\n",
        "\n",
        "        yield updated_history, state, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Critical error in execute_rag_update: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Comprehensive error message for users\n",
        "        error_msg = f\"\"\"âŒ **Enhancement Error**\n",
        "\n",
        "                                An unexpected error occurred during RAG enhancement: `{str(e)}`\n",
        "\n",
        "                                **Your original analysis is still available** in the conversation above. You can:\n",
        "                                - ðŸ“‹ **Continue using the existing analysis**\n",
        "                                - ðŸ”„ **Try enhancement again** in a few moments\n",
        "                                - ðŸ’¬ **Submit a new query** if needed\n",
        "\n",
        "                                *This error has been logged for improvement.*\"\"\"\n",
        "\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        updated_history = chatbot_history + [error_message]\n",
        "        yield updated_history, state or {}, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00af6yXBODfZ"
      },
      "outputs": [],
      "source": [
        "def enhanced_rag_with_session_state(original_query, working_query, previous_analysis, state):\n",
        "    \"\"\"\n",
        "    RAG enhancement using session state data directly\n",
        "    No database dependency - pure state-based operation\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"ðŸŽ¯ RAG Context:\")\n",
        "    print(f\"   ðŸ“ Original: {original_query}\")\n",
        "    print(f\"   ðŸ”„ Working: {working_query}\")\n",
        "    print(f\"   ðŸ“Š Previous analysis: {len(previous_analysis)} chars\")\n",
        "\n",
        "    try:\n",
        "        # Use the working query (rewritten/contextualized) for better RAG results\n",
        "        query_for_rag = working_query if working_query else original_query\n",
        "\n",
        "        # Create RAG prompt that leverages the previous analysis\n",
        "        rag_prompt = f\"\"\"Based on this answer:\n",
        "\n",
        "                {previous_analysis[:1000]}...\n",
        "\n",
        "                Please provide updated, current information that validates, corrects, or expands upon this answer for the query: \"{query_for_rag}\"\n",
        "\n",
        "                Focus on:\n",
        "                - Latest developments or changes\n",
        "                - Current accuracy of the information\n",
        "                - Recent data or statistics\n",
        "                - Any new perspectives or considerations\"\"\"\n",
        "\n",
        "        print(\"ðŸ” Calling RAG system...\")\n",
        "        enhanced_response = simple_rag_with_private_llm(rag_prompt)\n",
        "\n",
        "        # Extract and log source information\n",
        "        source_urls = extract_source_urls_from_response(enhanced_response)\n",
        "        print(f\"ðŸ“š Found {len(source_urls)} sources\")\n",
        "\n",
        "        # Store sources in state\n",
        "        if source_urls:\n",
        "            state[\"rag_sources\"] = source_urls\n",
        "\n",
        "        # Processing time\n",
        "        processing_time = round(time.time() - start_time, 2)\n",
        "        print(f\"â±ï¸ RAG completed in {processing_time} seconds\")\n",
        "\n",
        "        return enhanced_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ RAG processing error: {e}\")\n",
        "        raise e  # Re-raise to be handled by calling function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5r1EJkCODfZ"
      },
      "outputs": [],
      "source": [
        "def extract_source_urls_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract source URLs from RAG response for transparency\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    if not response_text or not isinstance(response_text, str):\n",
        "        return []\n",
        "\n",
        "    # Common patterns for URLs in RAG responses\n",
        "    patterns = [\n",
        "        r'\\((https?://[^\\)]+)\\)',  # URLs in parentheses\n",
        "        r'Source: (https?://\\S+)',  # URLs after \"Source:\"\n",
        "        r'\\[(https?://[^\\]]+)\\]',   # URLs in brackets\n",
        "        r'https?://\\S+',            # Any standalone URLs\n",
        "    ]\n",
        "\n",
        "    urls = []\n",
        "    for pattern in patterns:\n",
        "        found_urls = re.findall(pattern, response_text)\n",
        "        urls.extend(found_urls)\n",
        "\n",
        "    # Remove duplicates and return\n",
        "    return list(set(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8RjHyxqODfZ"
      },
      "outputs": [],
      "source": [
        "def background_database_logging(query_id, enhanced_response):\n",
        "    \"\"\"\n",
        "    Optional background logging to database\n",
        "    This runs independently and doesn't affect RAG functionality\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            print(\"âš ï¸ Database file not found - skipping logging\")\n",
        "            return\n",
        "\n",
        "        # Simple database update for logging\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        mask = df['ID'] == query_id\n",
        "\n",
        "        if mask.any():\n",
        "            df.loc[mask, 'After RAG Agent'] = str(enhanced_response)[:1000]  # Truncate for storage\n",
        "            df.loc[mask, 'RAG_Timestamp'] = time.time()\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"ðŸ“Š Logged RAG results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Query {query_id} not found for logging\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Database logging failed: {e}\")\n",
        "        # Don't raise - this is non-critical logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIQyIX3_9hDL"
      },
      "outputs": [],
      "source": [
        "def slow_echo_with_dual_intent_disambiguation_dynamic(message, chatbot_history, state):\n",
        "    \"\"\"Enhanced with dynamic UI control including main input visibility\"\"\"\n",
        "    # Initialize state if needed\n",
        "    if not state:\n",
        "        state = {\n",
        "            \"query_id\": None,\n",
        "            \"original_query\": \"\",\n",
        "            \"intent_options\": [],\n",
        "            \"step\": \"topic_selection\",\n",
        "            \"processing_query_id\": None,\n",
        "            \"answer_type_options\": [],\n",
        "            \"selected_topic_intent\": \"\",\n",
        "            \"working_query\": \"\",\n",
        "            \"topic\": \"\",\n",
        "            \"target_role_map\": {},\n",
        "            \"waiting_for_custom\": False,\n",
        "            \"custom_input_type\": \"\",\n",
        "            \"selected_answer_type\": \"\",\n",
        "            \"show_options\": False,\n",
        "            \"processing\": False,\n",
        "            \"show_main_input\": True\n",
        "        }\n",
        "\n",
        "    # Generate new query ID\n",
        "    import time\n",
        "    current_id = int(time.time() * 1000)\n",
        "\n",
        "    # Update state\n",
        "    state[\"query_id\"] = current_id\n",
        "    state[\"original_query\"] = message\n",
        "    state[\"step\"] = \"topic_selection\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"processing\"] = False\n",
        "    state[\"show_main_input\"] = True  # Keep main input visible\n",
        "\n",
        "    # Add user message to history\n",
        "    if chatbot_history is None:\n",
        "        chatbot_history = []\n",
        "\n",
        "    updated_history = chatbot_history + [{\"role\": \"user\", \"content\": message}]\n",
        "    yield updated_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Add query to database\n",
        "        add_new_query(current_id, message)\n",
        "\n",
        "        # Step 2: Generate topic intent options\n",
        "        topic_intent_options = generate_intent_options(message)\n",
        "        print(f\"Original: {message}\")\n",
        "        print(f\"Topic intent options: {topic_intent_options}\")\n",
        "\n",
        "        # Store in state\n",
        "        state[\"intent_options\"] = topic_intent_options\n",
        "\n",
        "        # Step 3: Show options with buttons visible, main input still visible\n",
        "        final_history, _ = show_intent_options_clean(updated_history, state, topic_intent_options)\n",
        "        yield final_history, state, \"\", gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error in intent disambiguation: {str(e)}\"\n",
        "        print(f\"Error: {e}\")\n",
        "        error_history = updated_history + [{\"role\": \"assistant\", \"content\": error_message}]\n",
        "        state[\"show_options\"] = False\n",
        "        yield error_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc1IDK8TODfa"
      },
      "outputs": [],
      "source": [
        "def view_database_stats():\n",
        "    \"\"\"View database statistics - handles both old and new schema\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        total_queries = len(df)\n",
        "        completed = len(df[df['Status'] == 'completed'])\n",
        "        pending = len(df[df['Status'] == 'pending'])\n",
        "        errors = len(df[df['Status'] == 'error'])\n",
        "\n",
        "        # Calculate average processing time for completed queries\n",
        "        completed_df = df[df['Status'] == 'completed']\n",
        "        if not completed_df.empty and 'Processing_Time_Seconds' in completed_df.columns:\n",
        "            # Filter out NaN values before calculating mean\n",
        "            processing_times = completed_df['Processing_Time_Seconds'].dropna()\n",
        "            if not processing_times.empty:\n",
        "                avg_time = processing_times.mean()\n",
        "                avg_time_str = f\"- Average processing time: {avg_time:.2f} seconds\"\n",
        "            else:\n",
        "                avg_time_str = \"- Average processing time: N/A\"\n",
        "        else:\n",
        "            avg_time_str = \"- Average processing time: N/A\"\n",
        "\n",
        "        stats = f\"\"\"\n",
        "        Database Statistics:\n",
        "        - Total queries: {total_queries}\n",
        "        - Completed: {completed}\n",
        "        - Pending: {pending}\n",
        "        - Errors: {errors}\n",
        "        {avg_time_str}\n",
        "\n",
        "        Database Schema: {list(df.columns)}\n",
        "\n",
        "        Recent queries:\n",
        "        \"\"\"\n",
        "\n",
        "        if not df.empty:\n",
        "            # Handle both old and new schema for display\n",
        "            if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "                # New schema\n",
        "                display_columns = ['ID', 'Original_Query', 'Rewritten_Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "            else:\n",
        "                # Old schema\n",
        "                display_columns = ['ID', 'Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "\n",
        "            # Only show columns that actually exist\n",
        "            existing_columns = [col for col in display_columns if col in df.columns]\n",
        "            recent = df.tail(5)[existing_columns]\n",
        "            stats += recent.to_string(index=False)\n",
        "\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        return f\"Error reading database: {e}\\n\\nColumns found: {list(pd.read_csv('cola_database.csv').columns) if os.path.exists('cola_database.csv') else 'File not found'}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-vdkAZiODfa"
      },
      "outputs": [],
      "source": [
        "def handle_option_1_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"Enhanced option 1 handler with dynamic UI including main input\"\"\"\n",
        "    if not isinstance(state, dict):\n",
        "        state = {\"step\": \"topic_selection\", \"intent_options\": [], \"answer_type_options\": [],\n",
        "                \"selected_topic_intent\": \"\", \"query_id\": None, \"original_query\": \"\",\n",
        "                \"show_options\": False, \"processing\": False, \"show_main_input\": True}\n",
        "\n",
        "    if state.get(\"step\") == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 0:\n",
        "        # First step: topic selection\n",
        "        selected_topic = state[\"intent_options\"][0]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        # Generate answer type options\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True  # Keep main input visible\n",
        "\n",
        "        # Remove topic selection message and show answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        updated_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "        yield updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif state.get(\"step\") == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 0:\n",
        "        # Second step: answer type selection\n",
        "        selected_answer_type = state[\"answer_type_options\"][0]\n",
        "\n",
        "        # Hide option buttons during processing, keep main input visible\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # Remove answer type selection message and process\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        yield from process_selected_answer_type_intelligent_dynamic(\n",
        "            selected_answer_type,\n",
        "            state.get(\"selected_topic_intent\", \"\"),\n",
        "            state.get(\"original_query\", \"\"),\n",
        "            state.get(\"query_id\"),\n",
        "            cleaned_history,\n",
        "            state\n",
        "        )\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2KaPRUGODfa"
      },
      "outputs": [],
      "source": [
        "def handle_option_2_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"Enhanced option 2 handler with dynamic UI including main input\"\"\"\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 1:\n",
        "        selected_topic = state[\"intent_options\"][1]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        updated_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "        yield updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 1:\n",
        "        selected_answer_type = state[\"answer_type_options\"][1]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        yield from process_selected_answer_type_intelligent_dynamic(\n",
        "            selected_answer_type,\n",
        "            state.get(\"selected_topic_intent\", \"\"),\n",
        "            state.get(\"original_query\", \"\"),\n",
        "            state.get(\"query_id\"),\n",
        "            cleaned_history,\n",
        "            state\n",
        "        )\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2obIIRkODfb"
      },
      "outputs": [],
      "source": [
        "def handle_option_3_click_enhanced_dynamic(chatbot_history, state):\n",
        "    \"\"\"Enhanced option 3 handler with dynamic UI including main input\"\"\"\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 2:\n",
        "        selected_topic = state[\"intent_options\"][2]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        updated_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "        yield updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 2:\n",
        "        selected_answer_type = state[\"answer_type_options\"][2]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        yield from process_selected_answer_type_intelligent_dynamic(\n",
        "            selected_answer_type,\n",
        "            state.get(\"selected_topic_intent\", \"\"),\n",
        "            state.get(\"original_query\", \"\"),\n",
        "            state.get(\"query_id\"),\n",
        "            cleaned_history,\n",
        "            state\n",
        "        )\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GbDxmQBODfb"
      },
      "outputs": [],
      "source": [
        "def handle_other_option_click_dynamic(chatbot_history, state):\n",
        "    \"\"\"Handle 'Other' option click with dynamic UI - HIDE main input\"\"\"\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"topic\"\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = False  # HIDE main input\n",
        "\n",
        "        custom_msg = \"\"\"ðŸ” **Custom Topic Selection**\n",
        "\n",
        "The provided options don't match what you're looking for? No problem!\n",
        "\n",
        "Please specify your preferred topic or domain in the text box below. For example:\n",
        "- \"Python machine learning\"\n",
        "- \"Apple company stock\"\n",
        "- \"Java coffee brewing\"\n",
        "\n",
        "*Tip: Be as specific as possible to get the best results.*\"\"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"answer_type\"\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = False  # HIDE main input\n",
        "\n",
        "        custom_msg = \"\"\"ðŸ“ **Custom Answer Type**\n",
        "\n",
        "Need a different type of response? Please specify what kind of answer you're looking for:\n",
        "\n",
        "Examples:\n",
        "- \"Step-by-step tutorial\"\n",
        "- \"Pros and cons comparison\"\n",
        "- \"Historical timeline\"\n",
        "- \"Technical specifications\"\n",
        "\n",
        "*Tip: Describe the format or style of response you prefer.*\"\"\"\n",
        "\n",
        "    else:\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # Remove the original options message and replace with custom input message\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "    custom_message = {\"role\": \"assistant\", \"content\": custom_msg}\n",
        "    updated_history = cleaned_history + [custom_message]\n",
        "\n",
        "    # Hide main input, hide option buttons, show custom input\n",
        "    return updated_history, state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po6QSEv6ODfb"
      },
      "outputs": [],
      "source": [
        "def process_custom_input_with_spellcheck(user_input, input_type):\n",
        "    \"\"\"Process user's custom input with basic spell checking only\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Basic cleaning\n",
        "    cleaned_input = user_input.strip()\n",
        "\n",
        "    # Basic spell checking for common terms (you can expand this)\n",
        "    spell_corrections = {\n",
        "        # Programming languages\n",
        "        \"phyton\": \"Python\", \"pyhton\": \"Python\", \"pythn\": \"Python\",\n",
        "        \"javas\": \"Java\", \"jave\": \"Java\",\n",
        "        \"javascript\": \"JavaScript\", \"js\": \"JavaScript\",\n",
        "\n",
        "        # Common topics\n",
        "        \"machien learning\": \"machine learning\",\n",
        "        \"artifical intelligence\": \"artificial intelligence\",\n",
        "        \"blockchian\": \"blockchain\",\n",
        "        \"cyrptocurrency\": \"cryptocurrency\",\n",
        "\n",
        "        # Answer types\n",
        "        \"tutorail\": \"tutorial\", \"tutoral\": \"tutorial\",\n",
        "        \"comparision\": \"comparison\", \"comparsion\": \"comparison\",\n",
        "        \"recomendation\": \"recommendation\", \"recomendations\": \"recommendations\"\n",
        "    }\n",
        "\n",
        "    # Apply corrections\n",
        "    for wrong, correct in spell_corrections.items():\n",
        "        if wrong.lower() in cleaned_input.lower():\n",
        "            cleaned_input = re.sub(re.escape(wrong), correct, cleaned_input, flags=re.IGNORECASE)\n",
        "\n",
        "    # Return cleaned input without any format bias\n",
        "    return cleaned_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2OHR2b1ODfb"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_submit_dynamic(custom_input, chatbot_history, state):\n",
        "    \"\"\"Handle custom input submission with dynamic UI - SHOW main input again\"\"\"\n",
        "    if not state or not state.get(\"waiting_for_custom\"):\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    if not custom_input or not custom_input.strip():\n",
        "        error_msg = {\"role\": \"assistant\", \"content\": \"âŒ Please enter your custom option before submitting.\"}\n",
        "        return chatbot_history + [error_msg], state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), custom_input\n",
        "\n",
        "    # Process the custom input\n",
        "    input_type = state.get(\"custom_input_type\", \"topic\")\n",
        "    processed_input = process_custom_input_with_spellcheck(custom_input, input_type)\n",
        "\n",
        "    # Clear custom input state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_main_input\"] = True  # SHOW main input again\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        # Handle custom topic selection\n",
        "        state[\"selected_topic_intent\"] = processed_input\n",
        "\n",
        "        # Generate answer type options\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), processed_input)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # Remove custom input instruction message and replace with confirmation\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        confirmation_msg = f\"âœ… **Custom topic selected:** {processed_input}\\n\\nNow, what type of answer do you need?\"\n",
        "        confirmation_message = {\"role\": \"assistant\", \"content\": confirmation_msg}\n",
        "        updated_history = cleaned_history + [confirmation_message]\n",
        "\n",
        "        # Show answer type options\n",
        "        final_history, _ = show_answer_type_options_clean(updated_history, state, answer_type_options)\n",
        "\n",
        "        # Show main input, show option buttons, hide custom input\n",
        "        return final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        # Handle custom answer type selection\n",
        "        state[\"selected_answer_type\"] = processed_input\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "\n",
        "        # Remove custom input instruction message and replace with confirmation\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        confirmation_msg = f\"âœ… **Custom answer type selected:** {processed_input}\\n\\nðŸ”„ Processing your query...\"\n",
        "        confirmation_message = {\"role\": \"assistant\", \"content\": confirmation_msg}\n",
        "        final_history = cleaned_history + [confirmation_message]\n",
        "\n",
        "        # Reset step for next query\n",
        "        state[\"step\"] = \"topic_selection\"\n",
        "\n",
        "        # Show main input, hide buttons, hide custom input\n",
        "        return final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLnf-amYODfb"
      },
      "outputs": [],
      "source": [
        "def process_if_answer_type_selected_dynamic(chatbot_history, state):\n",
        "    \"\"\"Process if we have both topic and answer type selected with dynamic UI including main input\"\"\"\n",
        "    if not state:\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    # Check if we have both selections ready for processing\n",
        "    selected_topic = state.get(\"selected_topic_intent\", \"\")\n",
        "    selected_answer_type = state.get(\"selected_answer_type\", \"\")\n",
        "\n",
        "    if selected_topic and selected_answer_type:\n",
        "        # We have both - proceed with COLA processing\n",
        "        original_query = state.get(\"original_query\", \"\")\n",
        "        query_id = state.get(\"query_id\")\n",
        "\n",
        "        # Hide option buttons during processing, keep main input visible\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        # Clear the selected answer type so we don't process again\n",
        "        state[\"selected_answer_type\"] = \"\"\n",
        "\n",
        "        # Process through the intelligent framework\n",
        "        yield from process_selected_answer_type_intelligent_dynamic(\n",
        "            selected_answer_type,\n",
        "            selected_topic,\n",
        "            original_query,\n",
        "            query_id,\n",
        "            chatbot_history,\n",
        "            state\n",
        "        )\n",
        "    else:\n",
        "        # Not ready for processing yet, just return as-is\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmka1RrSODfb"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_cancel_dynamic(chatbot_history, state):\n",
        "    \"\"\"Handle cancellation of custom input with dynamic UI - SHOW main input again\"\"\"\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # Clear custom input state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"show_main_input\"] = True  # SHOW main input again\n",
        "\n",
        "    # Remove the custom input request message\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "    # Add cancellation message\n",
        "    cancel_msg = {\"role\": \"assistant\", \"content\": \"âŒ **Custom input cancelled.** Please select one of the provided options above.\"}\n",
        "    updated_history = cleaned_history + [cancel_msg]\n",
        "\n",
        "    # Show main input, show option buttons, hide custom input\n",
        "    return updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5oicXqODfc"
      },
      "outputs": [],
      "source": [
        "def process_selected_answer_type_intelligent_dynamic(selected_answer_type, selected_topic_intent, original_query, query_id, chatbot_history, state):\n",
        "    \"\"\"Enhanced COLA processing with dynamic UI control including main input - clean version\"\"\"\n",
        "    try:\n",
        "        # Update state with processing info\n",
        "        if isinstance(state, dict):\n",
        "            state[\"processing_query_id\"] = query_id\n",
        "            state[\"selected_topic_intent\"] = selected_topic_intent\n",
        "            state[\"processing\"] = True\n",
        "            state[\"show_options\"] = False\n",
        "            state[\"show_main_input\"] = True  # Keep main input visible during processing\n",
        "\n",
        "        # Process with intelligent routing\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            original_query,\n",
        "            selected_topic_intent,\n",
        "            selected_answer_type,\n",
        "            query_id,\n",
        "            state\n",
        "        )\n",
        "\n",
        "        # Remove the confirmation messages and show only the final result\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else []\n",
        "\n",
        "        # Add only the final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        # Reset processing state, keep main input visible\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ **Error processing selected answer type:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "\n",
        "        # Clean up confirmation messages and show error\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else chatbot_history\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        # Reset state on error, keep main input visible\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = True\n",
        "\n",
        "        yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)# UPDATED DISPLAY FUNCTIONS FOR CLEAN CHAT HISTORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ__vlgBODfc"
      },
      "outputs": [],
      "source": [
        "def safe_close():\n",
        "    \"\"\"Clean shutdown function\"\"\"\n",
        "    print(\"ðŸ”„ Closing application...\")\n",
        "    # Add any cleanup here\n",
        "    return \"âœ… Application closed safely. You can close the browser tab now.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPoo4f-KODfc"
      },
      "outputs": [],
      "source": [
        "def create_enhanced_gradio_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        # Initialize database on startup\n",
        "        initialize_database_with_sources()\n",
        "\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Enhanced Collaborative Search with Dual Intent Clarification\",\n",
        "            type=\"messages\",\n",
        "            height=400\n",
        "        )\n",
        "\n",
        "        # Main input components (initially visible, will hide when custom input appears)\n",
        "        with gr.Row(visible=True) as main_input_row:\n",
        "            with gr.Column():\n",
        "                msg = gr.Textbox(label=\"Your query\", placeholder=\"How can I help you today?\")\n",
        "                send_btn = gr.Button(\"Send\")\n",
        "\n",
        "        # Intent selection buttons (initially hidden, will show when needed)\n",
        "        with gr.Row(visible=False) as option_buttons_row:\n",
        "            option1_btn = gr.Button(\"Option 1\", size=\"lg\", variant=\"secondary\")\n",
        "            option2_btn = gr.Button(\"Option 2\", size=\"lg\", variant=\"secondary\")\n",
        "            option3_btn = gr.Button(\"Option 3\", size=\"lg\", variant=\"secondary\")\n",
        "            option4_btn = gr.Button(\"Other\", size=\"lg\", variant=\"secondary\")\n",
        "\n",
        "        # Custom input modal components (initially hidden)\n",
        "        with gr.Row(visible=False) as custom_input_row:\n",
        "            with gr.Column():\n",
        "                custom_input_label = gr.Markdown(\"**Please specify your preferred topic/answer type:**\")\n",
        "                custom_input = gr.Textbox(\n",
        "                    label=\"Your custom option\",\n",
        "                    placeholder=\"Type your preferred topic or answer type here...\",\n",
        "                    lines=2\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    submit_custom_btn = gr.Button(\"Submit Custom Option\", variant=\"primary\")\n",
        "                    cancel_custom_btn = gr.Button(\"Cancel\", variant=\"secondary\")\n",
        "\n",
        "        extra_btn = gr.Button(\"Update information using RAG\")\n",
        "\n",
        "        # Database management buttons\n",
        "        with gr.Row():\n",
        "            stats_btn = gr.Button(\"View Database Stats\")\n",
        "            export_btn = gr.Button(\"Export Database\")\n",
        "            close_btn = gr.Button(\"ðŸ”´ Close App\", variant=\"stop\")\n",
        "\n",
        "        stats_output = gr.Textbox(label=\"Database Information\", lines=10)\n",
        "\n",
        "        # Enhanced state with UI visibility tracking\n",
        "        state = gr.State({\n",
        "            \"query_id\": None,\n",
        "            \"original_query\": \"\",\n",
        "            \"processing_query_id\": None,\n",
        "            \"intent_options\": [],\n",
        "            \"answer_type_options\": [],\n",
        "            \"selected_topic_intent\": \"\",\n",
        "            \"selected_answer_type\": \"\",\n",
        "            \"step\": \"topic_selection\",\n",
        "            \"j\": 0,\n",
        "            \"working_query\": \"\",\n",
        "            \"topic\": \"\",\n",
        "            \"target_role_map\": {},\n",
        "            \"waiting_for_custom\": False,\n",
        "            \"custom_input_type\": \"\",\n",
        "            \"show_options\": False,\n",
        "            \"processing\": False,\n",
        "            \"show_main_input\": True       # NEW: Track main input visibility\n",
        "        })\n",
        "\n",
        "        # Event handlers with dynamic UI updates including main input visibility\n",
        "        send_btn.click(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        msg.submit(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        # Regular option handlers with UI updates\n",
        "        option1_btn.click(\n",
        "            handle_option_1_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "        option2_btn.click(\n",
        "            handle_option_2_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "        option3_btn.click(\n",
        "            handle_option_3_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        # \"Other\" option handler with UI updates including hiding main input\n",
        "        option4_btn.click(\n",
        "            handle_other_option_click_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # Custom input handlers with UI updates including showing main input again\n",
        "        submit_custom_btn.click(\n",
        "            handle_custom_input_submit_dynamic,\n",
        "            inputs=[custom_input, chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        ).then(\n",
        "            process_if_answer_type_selected_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        cancel_custom_btn.click(\n",
        "            handle_custom_input_cancel_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # Existing handlers (no UI changes needed)\n",
        "        extra_btn.click(execute_rag_update, inputs=[chatbot, state], outputs=[chatbot, state, msg])\n",
        "        stats_btn.click(view_database_stats, outputs=[stats_output])\n",
        "        close_btn.click(safe_close, outputs=[stats_output])\n",
        "\n",
        "    demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koxfooBeODfc",
        "outputId": "d94afa85-dd48-4c02-f0d7-d7131a08ac72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7883\n",
            "* Running on public URL: https://47c89bd69384eb2694.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://47c89bd69384eb2694.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error adding query to database: float() argument must be a string or a real number, not 'NAType'\n",
            "Original: what is a point \n",
            "Topic intent options: ['Point (geography)', 'Point (mathematics)', 'Point (debate)']\n",
            "Error: name 'show_intent_options_clean' is not defined\n"
          ]
        }
      ],
      "source": [
        "create_enhanced_gradio_interface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKrA9c9XODfc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}