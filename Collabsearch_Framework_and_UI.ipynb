{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CollabSearch Framework\n",
        "## Comprehensive Code Structure and Implementation Guide\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "The CollabSearch (Collaborative Analysis) Framework represents a sophisticated multi-agent system that demonstrates advanced patterns in AI orchestration, intent disambiguation, and collaborative reasoning. The system combines retrieval-augmented generation (RAG), dynamic role-based prompting, and multi-stage pipeline processing to deliver contextually appropriate responses through intelligent routing and synthesis mechanisms.\n"
      ],
      "metadata": {
        "id": "q1btqEeh8PJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install necessary libraries"
      ],
      "metadata": {
        "id": "CDZr_KAx8UKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI9rRfYae-ir",
        "outputId": "122c4fea-0eec-448b-a361-2294371718a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: langchain in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.100.2)\n",
            "Requirement already satisfied: google-api-python-client in /opt/miniconda3/lib/python3.10/site-packages (2.179.0)\n",
            "Requirement already satisfied: langchain_community in /opt/miniconda3/lib/python3.10/site-packages (0.3.27)\n",
            "Requirement already satisfied: tools in /opt/miniconda3/lib/python3.10/site-packages (1.0.4)\n",
            "Requirement already satisfied: langchain_google_community in /opt/miniconda3/lib/python3.10/site-packages (2.0.7)\n",
            "Requirement already satisfied: langchain_openai in /opt/miniconda3/lib/python3.10/site-packages (0.3.30)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.40.3)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (3.11.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (2.4.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_google_community) (1.71.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/miniconda3/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.7)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/miniconda3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /opt/miniconda3/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: greenlet>=1 in /opt/miniconda3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai in /opt/miniconda3/lib/python3.10/site-packages (1.100.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/lib/python3.10/site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gradio in /opt/miniconda3/lib/python3.10/site-packages (5.43.1)\n",
            "Requirement already satisfied: openpyxl in /opt/miniconda3/lib/python3.10/site-packages (3.1.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.12.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/miniconda3/lib/python3.10/site-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /opt/miniconda3/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (1.26.18)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai>=1.0.0 geotext transformers GeoText\n",
        "!pip install -q -U google-genai\n",
        "!pip install langchain openai google-api-python-client langchain_community tools langchain_google_community langchain_openai\n",
        "!pip install --upgrade openai\n",
        "!pip install gradio openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Libraries"
      ],
      "metadata": {
        "id": "DLBuu8N38rkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DibFVFaDfIq8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from langchain import GoogleSearchAPIWrapper\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Environmental Variables\n",
        "\n"
      ],
      "metadata": {
        "id": "qQSVFV2D887P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sui66xUhfUlC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"IDA_LLM_API_KEY\"] = \"your_api_key_here\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key_here\"\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"your_cse_id_here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHCvF0cl9hDI"
      },
      "outputs": [],
      "source": [
        "# Initialize your private LLM client (llama-3-8b-instruct-instruct via university server)\n",
        "client = OpenAI(\n",
        "    base_url=\"http://api.llm.apps.os.dcs.gla.ac.uk/v1\",\n",
        "    api_key=os.environ['IDA_LLM_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database Management and State Persistence\n",
        "\n",
        "### Comprehensive Schema Design\n",
        "\n",
        "**Database Architecture Insights:**\n",
        "1. **Comprehensive Tracking**: Every processing stage logged\n",
        "2. **Performance Monitoring**: Separate timing fields\n",
        "3. **Source Attribution**: RAG source URLs preserved\n",
        "4. **Process Lineage**: Complete transformation trail"
      ],
      "metadata": {
        "id": "0GfllV-d9h8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Di9G29z20S2"
      },
      "outputs": [],
      "source": [
        "def initialize_database_with_sources():\n",
        "    '''\n",
        "    Initialize CSV database for COLA system tracking with predefined schema.\n",
        "    Creates empty database file with columns for query processing, routing, analysis results, and performance metrics.\n",
        "    '''\n",
        "    try:\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            columns = [\n",
        "                'ID', 'Original_Query', 'Rewritten_Query', 'Selected_Topic_Intent', 'Selected_Answer_Type',\n",
        "                'Routing_Type', 'Routing_Method', 'Dictionary_Match', 'Identified_Topic', 'Assigned_Roles',\n",
        "                'Linguist_Analysis', 'Expert_Analysis', 'User_Analysis', 'In_Favor', 'Against',\n",
        "                'Final_Judgement', 'Synthesis_Method', 'After_RAG_Agent', 'RAG_Source_URLs',\n",
        "                'Processing_Time_Seconds', 'Processing_Time_Seconds_RAG', 'Timestamp', 'Status'\n",
        "            ]\n",
        "            df = pd.DataFrame(columns=columns)\n",
        "\n",
        "            # Comprehensive dtype mapping\n",
        "            dtype_dict = {\n",
        "                'ID': 'int64',\n",
        "                'Original_Query': 'string',\n",
        "                'Rewritten_Query': 'string',\n",
        "                'Selected_Topic_Intent': 'string',\n",
        "                'Selected_Answer_Type': 'string',\n",
        "                'Routing_Type': 'string',\n",
        "                'Routing_Method': 'string',\n",
        "                'Dictionary_Match': 'string',\n",
        "                'Identified_Topic': 'string',\n",
        "                'Assigned_Roles': 'string',\n",
        "                'Linguist_Analysis': 'string',\n",
        "                'Expert_Analysis': 'string',\n",
        "                'User_Analysis': 'string',\n",
        "                'In_Favor': 'string',\n",
        "                'Against': 'string',\n",
        "                'Final_Judgement': 'string',\n",
        "                'Synthesis_Method': 'string',\n",
        "                'After_RAG_Agent': 'string',\n",
        "                'RAG_Source_URLs': 'string',\n",
        "                'Processing_Time_Seconds': 'float64',\n",
        "                'Processing_Time_Seconds_RAG': 'float64',\n",
        "                'Timestamp': 'string',\n",
        "                'Status': 'string'\n",
        "            }\n",
        "\n",
        "            df = df.astype(dtype_dict)\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(\"‚úÖ Enhanced COLA database with comprehensive tracking initialized\")\n",
        "        else:\n",
        "            print(\"‚úÖ Database already exists\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error initializing database: {e}\")\n",
        "        # Create minimal fallback\n",
        "        pd.DataFrame(columns=['ID', 'Original_Query', 'Status']).to_csv('cola_database.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6FaMneO20S6"
      },
      "outputs": [],
      "source": [
        "def add_new_query(query_id, query_text):\n",
        "\n",
        "    try:\n",
        "        if os.path.exists('cola_database.csv'):\n",
        "            df = pd.read_csv('cola_database.csv')\n",
        "        else:\n",
        "            initialize_database_with_sources()\n",
        "            df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        # FIXED: Create new row data that matches DataFrame structure exactly\n",
        "        new_row_data = {\n",
        "            'ID': int(query_id),\n",
        "            'Original_Query': str(query_text),\n",
        "            'Rewritten_Query': '',\n",
        "            'Selected_Topic_Intent': '',\n",
        "            'Selected_Answer_Type': '',\n",
        "            'Routing_Type': '',\n",
        "            'Routing_Method': '',\n",
        "            'Dictionary_Match': '',\n",
        "            'Identified_Topic': '',\n",
        "            'Assigned_Roles': '',\n",
        "            'Linguist_Analysis': '',\n",
        "            'Expert_Analysis': '',\n",
        "            'User_Analysis': '',\n",
        "            'In_Favor': '',\n",
        "            'Against': '',\n",
        "            'Final_Judgement': '',\n",
        "            'Synthesis_Method': '',\n",
        "            'After_RAG_Agent': '',\n",
        "            'RAG_Source_URLs': '',\n",
        "            'Processing_Time_Seconds': None,\n",
        "            'Processing_Time_Seconds_RAG': None,\n",
        "            'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Status': 'pending'\n",
        "        }\n",
        "\n",
        "        if not df.empty:\n",
        "            # Make sure new row has exact same columns as existing DataFrame\n",
        "            for col in df.columns:\n",
        "                if col not in new_row_data:\n",
        "                    new_row_data[col] = None if df[col].dtype in ['float64', 'int64'] else ''\n",
        "\n",
        "            # Create new row with matching column order\n",
        "            new_row = pd.DataFrame([new_row_data], columns=df.columns)\n",
        "        else:\n",
        "            new_row = pd.DataFrame([new_row_data])\n",
        "\n",
        "        df = pd.concat([df, new_row], ignore_index=True, sort=False)\n",
        "        df.to_csv('cola_database.csv', index=False)\n",
        "        print(f\"‚úÖ Added query {query_id} to database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error adding query to database: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKzm4H9k20S8"
      },
      "outputs": [],
      "source": [
        "def update_comprehensive_results(query_id, results_dict):\n",
        "    '''\n",
        "    Add new query entry to CSV database with initial default values.\n",
        "    Creates new row with query ID and text, initializes empty fields for tracking processing stages.\n",
        "    '''\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        mask = df['ID'] == query_id\n",
        "\n",
        "        if mask.any():\n",
        "            for column, value in results_dict.items():\n",
        "                if column in df.columns:\n",
        "                    if column in ['Processing_Time_Seconds', 'Processing_Time_Seconds_RAG']:\n",
        "                        try:\n",
        "                            df.loc[mask, column] = float(value) if value is not None else None\n",
        "                        except (ValueError, TypeError):\n",
        "                            df.loc[mask, column] = None\n",
        "                    else:\n",
        "                        df.loc[mask, column] = str(value) if value is not None else ''\n",
        "\n",
        "            df.loc[mask, 'Status'] = 'completed'\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"‚úÖ Updated comprehensive results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Query ID {query_id} not found in database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error updating results: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwmwuhYX20S9"
      },
      "outputs": [],
      "source": [
        "def view_comprehensive_database_stats():\n",
        "    '''\n",
        "    Display comprehensive database statistics and analysis.\n",
        "    Provides detailed breakdown of query status, routing patterns, processing times, topic distribution, and recent entries.\n",
        "    '''\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        total_queries = len(df)\n",
        "        completed = len(df[df['Status'] == 'completed'])\n",
        "        pending = len(df[df['Status'] == 'pending'])\n",
        "        errors = len(df[df['Status'] == 'error'])\n",
        "\n",
        "        # Enhanced routing statistics\n",
        "        routing_stats = \"\"\n",
        "        synthesis_stats = \"\"\n",
        "        dictionary_stats = \"\"\n",
        "\n",
        "        if 'Routing_Type' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "\n",
        "            # Routing type distribution\n",
        "            if 'Routing_Type' in completed_df.columns:\n",
        "                routing_dist = completed_df['Routing_Type'].value_counts()\n",
        "                routing_stats = f\"\\nüìä Routing Distribution:\"\n",
        "                for route_type, count in routing_dist.items():\n",
        "                    if pd.notna(route_type):  # Skip NaN values\n",
        "                        routing_stats += f\"\\n   - {route_type}: {count}\"\n",
        "\n",
        "            # Routing method breakdown\n",
        "            if 'Routing_Method' in completed_df.columns:\n",
        "                method_dist = completed_df['Routing_Method'].value_counts()\n",
        "                routing_stats += f\"\\nüîç Method Breakdown:\"\n",
        "                for method, count in method_dist.items():\n",
        "                    if pd.notna(method):  # Skip NaN values\n",
        "                        routing_stats += f\"\\n   - {method}: {count}\"\n",
        "\n",
        "            # Dictionary match analysis\n",
        "            if 'Dictionary_Match' in completed_df.columns:\n",
        "                dict_matches = completed_df['Dictionary_Match'].value_counts()\n",
        "                dictionary_stats = f\"\\nüîë Dictionary Matches:\"\n",
        "                for match, count in dict_matches.items():\n",
        "                    if pd.notna(match) and match.strip():  # Skip NaN and empty values\n",
        "                        dictionary_stats += f\"\\n   - '{match}': {count}\"\n",
        "\n",
        "            # Synthesis method distribution\n",
        "            if 'Synthesis_Method' in completed_df.columns:\n",
        "                synthesis_dist = completed_df['Synthesis_Method'].value_counts()\n",
        "                synthesis_stats = f\"\\n‚öôÔ∏è Synthesis Methods:\"\n",
        "                for method, count in synthesis_dist.items():\n",
        "                    if pd.notna(method):  # Skip NaN values\n",
        "                        synthesis_stats += f\"\\n   - {method}: {count}\"\n",
        "\n",
        "        # Processing time analysis\n",
        "        time_stats = \"\"\n",
        "        if 'Processing_Time_Seconds' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "            processing_times = completed_df['Processing_Time_Seconds'].dropna()\n",
        "            if not processing_times.empty:\n",
        "                avg_time = processing_times.mean()\n",
        "                min_time = processing_times.min()\n",
        "                max_time = processing_times.max()\n",
        "                time_stats = f\"\\n‚è±Ô∏è Processing Times:\\n   - Average: {avg_time:.2f}s\\n   - Minimum: {min_time:.2f}s\\n   - Maximum: {max_time:.2f}s\"\n",
        "\n",
        "        # Topic analysis\n",
        "        topic_stats = \"\"\n",
        "        if 'Identified_Topic' in df.columns and completed > 0:\n",
        "            completed_df = df[df['Status'] == 'completed']\n",
        "            topic_dist = completed_df['Identified_Topic'].value_counts()\n",
        "            topic_stats = f\"\\nüè∑Ô∏è Topics Identified:\"\n",
        "            for topic, count in topic_dist.head(5).items():  # Top 5 topics\n",
        "                if pd.notna(topic):\n",
        "                    topic_stats += f\"\\n   - {topic}: {count}\"\n",
        "\n",
        "        # Complete stats output with proper formatting\n",
        "        stats = f\"\"\"\n",
        "üìà COMPREHENSIVE DATABASE STATISTICS\n",
        "=====================================\n",
        "üìä Query Status:\n",
        "  - Total Queries: {total_queries}\n",
        "  - Completed: {completed}\n",
        "  - Pending: {pending}\n",
        "  - Errors: {errors}\n",
        "{time_stats}{routing_stats}{dictionary_stats}{synthesis_stats}{topic_stats}\n",
        "\n",
        "üóÇÔ∏è Database Schema: {len(df.columns)} columns\n",
        "üìã Available Columns: {', '.join(df.columns)}\n",
        "\n",
        "üìã Recent Queries (Last 5):\n",
        "\"\"\"\n",
        "\n",
        "        if not df.empty:\n",
        "            # Better column selection and handling of NaN values\n",
        "            display_columns = ['ID', 'Original_Query', 'Routing_Type', 'Synthesis_Method', 'Identified_Topic', 'Status', 'Timestamp']\n",
        "            existing_columns = [col for col in display_columns if col in df.columns]\n",
        "\n",
        "            recent = df.tail(5)[existing_columns].copy()\n",
        "\n",
        "            # Clean up NaN values for display\n",
        "            for col in recent.columns:\n",
        "                if col not in ['ID']:  # Don't modify ID column\n",
        "                    recent[col] = recent[col].fillna('Not Set')\n",
        "\n",
        "            # Truncate long text for better display\n",
        "            if 'Original_Query' in recent.columns:\n",
        "                recent['Original_Query'] = recent['Original_Query'].apply(\n",
        "                    lambda x: str(x)[:50] + '...' if len(str(x)) > 50 else str(x)\n",
        "                )\n",
        "\n",
        "            stats += recent.to_string(index=False, max_colwidth=50)\n",
        "\n",
        "        # Adds summary at the end\n",
        "        stats += f\"\"\"\n",
        "\n",
        "üìä SUMMARY:\n",
        "- Database is functioning properly with {total_queries} total queries\n",
        "- {completed} queries have been successfully processed\n",
        "- Enhanced tracking is capturing routing decisions and topic identification\n",
        "- Latest query processing time: {processing_times.iloc[-1]:.2f}s (if available)\n",
        "\"\"\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        error_details = f\"\"\"\n",
        "  ‚ùå ERROR READING DATABASE: {str(e)}\n",
        "\n",
        "  üîß TROUBLESHOOTING:\n",
        "  - Check if 'cola_database.csv' exists in the current directory\n",
        "  - Verify database schema is correct\n",
        "  - Ensure no file permissions issues\n",
        "\n",
        "  üìã Available files: {', '.join([f for f in os.listdir('.') if f.endswith('.csv')])}\n",
        "  \"\"\"\n",
        "        return error_details"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resilient LLM Interface Design\n",
        "\n",
        "**Key Design Patterns:**\n",
        "1. **Exponential Backoff Pattern**: Retry logic with delays\n",
        "2. **Graceful Degradation**: Error messages instead of crashes\n",
        "3. **Deterministic Processing**: Temperature=0 for consistency\n",
        "4. **Comprehensive Error Logging**: Tracks failures for monitoring"
      ],
      "metadata": {
        "id": "S4501ton91sM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcPsHj_Yfhvt"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt):\n",
        "  '''\n",
        "  Send prompt to Llama-3-8B model with retry logic and error handling.\n",
        "  Attempts up to 100 retries with 2-second delays, returns model response or \"Error\" on failure.\n",
        "  '''\n",
        "    max_retries = 100\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "          messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "          response = client.chat.completions.create(\n",
        "              model=\"llama-3-8b-instruct\",\n",
        "              messages=messages,\n",
        "              temperature=0\n",
        "            )\n",
        "          return response.choices[0].message.content\n",
        "        except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzsEcMKwffxy"
      },
      "outputs": [],
      "source": [
        "def get_completion_with_role(role, instruction, content):\n",
        "  '''\n",
        "  Send role-based prompt to Llama-3-8B model with system role assignment.\n",
        "  Sets AI role via system message, combines instruction and content for user message, includes retry logic on failure.\n",
        "  '''\n",
        "    max_retries = 100\n",
        "    for i in range(max_retries):\n",
        "      try:\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{instruction}\\n{content}\"}\n",
        "        ]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "          )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "      except Exception as e:  # Generic exception handling\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                logging.warning(f\"Attempt {i+1} failed: {e}\")\n",
        "            else:\n",
        "                logging.error(f'Max retries reached for prompt: {instruction}. Error: {e}')\n",
        "                return \"Error\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intent Disambiguation Engine\n",
        "\n",
        "### Ambiguity Detection Algorithm\n",
        "\n",
        "**Disambiguation Strategy Analysis:**\n",
        "1. **Semantic Ambiguity Recognition**: Identifies homonyms and polysemous words\n",
        "2. **Domain-Level Disambiguation**: Separates different subject areas\n",
        "3. **Context-Level Refinement**: Provides different perspectives\n",
        "4. **Structured Output Format**: Enforces consistent formatting"
      ],
      "metadata": {
        "id": "zDqkX2mO-dqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK0XMb9420TB"
      },
      "outputs": [],
      "source": [
        "def generate_intent_options(original_query):\n",
        "    '''\n",
        "    Generate 3 domain/topic disambiguation options for ambiguous queries.\n",
        "    Identifies ambiguous terms and provides different interpretations (e.g., Java as programming language vs. island vs. coffee).\n",
        "    '''\n",
        "    prompt = f\"\"\"Query: \"{original_query}\"\n",
        "\n",
        "        CRITICAL: I need you to identify if this query has AMBIGUOUS WORDS that could mean completely different things.\n",
        "\n",
        "        Look for words that could be:\n",
        "        - Programming language vs. place vs. other meanings (Java, Python, Ruby, etc.)\n",
        "        - Company vs. fruit vs. other (Apple, Orange, etc.)\n",
        "        - Person vs. place vs. concept (Tesla, Darwin, etc.)\n",
        "        - Multiple different meanings entirely\n",
        "\n",
        "        If you find ambiguous words, give me 3 DIFFERENT DOMAINS/SUBJECTS.\n",
        "        If no ambiguous words, give me 3 different CONTEXTS for the same topic, be explanative but brief:\n",
        "        WRONG: Iphone moldels (Travel)\n",
        "        RIGHT: Best Iphone Models to Travel\n",
        "        Exception: If the ambiguous word relates to iPhone, ALWAYS exclude food/fruit.\n",
        "\n",
        "        WRONG (all same domain):\n",
        "        - Java web development features\n",
        "        - Java mobile app development\n",
        "        - Java enterprise applications\n",
        "\n",
        "        RIGHT (different domains):\n",
        "        - Java (programming language)\n",
        "        - Java (Indonesian island)\n",
        "        - Java (coffee culture)\n",
        "\n",
        "        Format: Topic (context)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Clean and parse the response - be more aggressive about filtering\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        # More comprehensive filtering for instruction text\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"interpretations\",\n",
        "            \"different meanings\", \"ambiguous\", \"critical\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines that contain instruction-like phrases\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets more aggressively\n",
        "            cleaned_line = line\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\‚Ä¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            # Only keep lines that look like actual topic options (should contain parentheses ideally)\n",
        "            if cleaned_line and len(cleaned_line) > 3 and not any(phrase in cleaned_line.lower() for phrase in skip_phrases):\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # If we don't get good results, create manual disambiguation for common terms\n",
        "        query_lower = original_query.lower()\n",
        "\n",
        "        # Check for common ambiguous terms\n",
        "        if 'java' in query_lower:\n",
        "            return [\n",
        "                \"Java (programming language)\",\n",
        "                \"Java (Indonesian island)\",\n",
        "                \"Java (coffee culture)\"\n",
        "            ]\n",
        "        elif 'python' in query_lower:\n",
        "            return [\n",
        "                \"Python (programming language)\",\n",
        "                \"Python (snake species)\",\n",
        "                \"Python (Monty Python comedy)\"\n",
        "            ]\n",
        "        elif 'tesla' in query_lower:\n",
        "            return [\n",
        "                \"Tesla (car company)\",\n",
        "                \"Tesla (Nikola Tesla scientist)\",\n",
        "                \"Tesla (band/music)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic contextual fallback\n",
        "            return [\n",
        "                f\"Technical/professional information about {original_query}\",\n",
        "                f\"General educational information about {original_query}\",\n",
        "                f\"Practical applications of {original_query}\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating intent options: {e}\")\n",
        "        return [\n",
        "            f\"Technical information about {original_query}\",\n",
        "            f\"General information about {original_query}\",\n",
        "            f\"Practical guide for {original_query}\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipV4UcDd20TC"
      },
      "outputs": [],
      "source": [
        "def show_intent_options_clean(chatbot_history, state, options):\n",
        "    '''\n",
        "    Display intent clarification options to user in chatbot interface.\n",
        "    Formats disambiguation options with numbered choices, adds \"Other\" option, appends to chat history.\n",
        "    '''\n",
        "    options_text = \"üéØ **Please clarify your intent:**\\n\\n\"\n",
        "    options_text += \"Select the option that best matches what you're looking for:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** üîß **Other** - Specify your own topic/domain\\n\\n\"\n",
        "    options_text += \"üëá **Click the corresponding Option button below to proceed.**\"\n",
        "\n",
        "    # ADD to existing history, don't replace\n",
        "    intent_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    updated_history = chatbot_history + [intent_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTjplHpg20TC"
      },
      "outputs": [],
      "source": [
        "def generate_answer_type_options(query, selected_topic_intent):\n",
        "    '''\n",
        "    Generate 3 answer-type options based on query and selected topic to clarify desired response format.\n",
        "    Determines appropriate response styles (informative, practical tips, basic overview, etc.) with fallback logic.\n",
        "    '''\n",
        "    prompt = f\"\"\"Query: \"{query}\"\n",
        "        Selected Topic: \"{selected_topic_intent}\"\n",
        "\n",
        "        The user wants to know about this topic. What type of answer would be most helpful?\n",
        "\n",
        "        Generate 3 different ANSWER TYPE options that would be appropriate for this query:\n",
        "\n",
        "        Consider these categories:\n",
        "        - Informative (detailed explanation, facts, background information)\n",
        "        - Practical Tips (actionable advice, how-to guidance, steps to follow)\n",
        "        - Basic Overview (simple introduction, key points, beginner-friendly)\n",
        "        - Expert Analysis (in-depth professional perspective, technical details)\n",
        "        - Comparison/Evaluation (pros/cons, alternatives, recommendations)\n",
        "        - Problem-Solving (solutions, troubleshooting, addressing specific issues)\n",
        "\n",
        "        Format each option as: \"Answer Type (brief description)\"\n",
        "\n",
        "        Examples:\n",
        "        - Informative (comprehensive background and facts)\n",
        "        - Practical Tips (step-by-step actionable guidance)\n",
        "        - Basic Overview (simple introduction for beginners)\n",
        "\n",
        "        Respond with exactly 3 lines, no explanations:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = get_completion(prompt)\n",
        "\n",
        "        # Parse response similar to intent options\n",
        "        lines = [line.strip() for line in response.strip().split('\\n') if line.strip()]\n",
        "\n",
        "        options = []\n",
        "        skip_phrases = [\n",
        "            \"here are\", \"results\", \"the query\", \"could refer\", \"examples\",\n",
        "            \"format\", \"write only\", \"respond with\", \"provide\", \"options\",\n",
        "            \"different types\", \"answer type\"\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line_lower = line.lower()\n",
        "            if any(phrase in line_lower for phrase in skip_phrases):\n",
        "                continue\n",
        "\n",
        "            # Remove numbering/bullets\n",
        "            import re\n",
        "            cleaned_line = re.sub(r'^[0-9]+[\\.\\)\\-\\s]+', '', line)\n",
        "            cleaned_line = re.sub(r'^[\\-\\*\\‚Ä¢]\\s+', '', cleaned_line)\n",
        "\n",
        "            if cleaned_line and len(cleaned_line) > 3:\n",
        "                options.append(cleaned_line)\n",
        "\n",
        "        # Take first 3 options\n",
        "        if len(options) >= 3:\n",
        "            return options[:3]\n",
        "\n",
        "        # Fallback options based on query analysis\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(word in query_lower for word in ['how to', 'steps', 'guide', 'tutorial']):\n",
        "            return [\n",
        "                \"Practical Tips (step-by-step actionable guidance)\",\n",
        "                \"Informative (detailed explanation and background)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\"\n",
        "            ]\n",
        "        elif any(word in query_lower for word in ['what is', 'explain', 'about']):\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Basic Overview (simple introduction for beginners)\",\n",
        "                \"Expert Analysis (in-depth professional perspective)\"\n",
        "            ]\n",
        "        else:\n",
        "            # Generic fallback\n",
        "            return [\n",
        "                \"Informative (comprehensive background and facts)\",\n",
        "                \"Practical Tips (actionable advice and guidance)\",\n",
        "                \"Basic Overview (simple introduction and key points)\"\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer type options: {e}\")\n",
        "        return [\n",
        "            \"Informative (comprehensive background and facts)\",\n",
        "            \"Practical Tips (actionable advice and guidance)\",\n",
        "            \"Basic Overview (simple introduction and key points)\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYzW4Rcm20TD"
      },
      "outputs": [],
      "source": [
        "def show_answer_type_options_clean(chatbot_history, state, options):\n",
        "    '''\n",
        "    Display answer-type selection options to user while preserving chat history.\n",
        "    Formats response format choices with numbered options, adds \"Other\" option, appends to existing conversation.\n",
        "    '''\n",
        "    options_text = \"üìù **What type of answer do you need?**\\n\\n\"\n",
        "    options_text += \"Choose the response format that best fits your needs:\\n\\n\"\n",
        "\n",
        "    for i, option in enumerate(options, 1):\n",
        "        options_text += f\"**Option {i}:** {option}\\n\\n\"\n",
        "\n",
        "    options_text += \"**Option 4:** üîß **Other** - Specify your preferred answer format\\n\\n\"\n",
        "    options_text += \"üëá **Click the corresponding Option button below.**\"\n",
        "\n",
        "    # ADD to existing history, preserving user query and previous messages\n",
        "    answer_type_message = {\"role\": \"assistant\", \"content\": options_text}\n",
        "    updated_history = chatbot_history + [answer_type_message]\n",
        "\n",
        "    return updated_history, state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intelligent Routing System\n",
        "\n",
        "### Multi-Tier Routing Architecture\n",
        "\n",
        "**Routing Strategy Patterns:**\n",
        "1. **Hierarchical Decision Making**: Dictionary ‚Üí Partial matching ‚Üí LLM fallback\n",
        "2. **Comprehensive Tracking**: Every routing decision logged\n",
        "3. **Fallback Resilience**: Multiple layers ensure processing never fails\n",
        "4. **Performance Optimization**: Fast dictionary lookup before LLM calls"
      ],
      "metadata": {
        "id": "IRgIXiiL-laH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8glFy6XK20TE"
      },
      "outputs": [],
      "source": [
        "def intelligent_routing_agent(original_query, selected_answer_type, rewritten_query=None):\n",
        "    '''\n",
        "    Analyze query intent using LLM when dictionary-based routing fails.\n",
        "    Determines optimal synthesis method (decision_making, solution_focused, or informational) based on query analysis.\n",
        "    '''\n",
        "\n",
        "    if selected_answer_type is None:\n",
        "        print(\"WARNING: selected_answer_type is None, using LLM to analyze query intent\")\n",
        "        analysis_target = original_query\n",
        "        answer_type_description = \"Unknown - need to analyze query intent\"\n",
        "    else:\n",
        "        analysis_target = f\"Query: {original_query}\\\\nAnswer Type Requested: {selected_answer_type}\"\n",
        "        answer_type_description = selected_answer_type\n",
        "\n",
        "    routing_prompt = f\"\"\"You are an intelligent routing agent for a collaborative analysis system. Your job is to analyze user queries and determine the best synthesis approach.\n",
        "\n",
        "            AVAILABLE SYNTHESIS METHODS:\n",
        "            1. **DECISION_MAKING**: Use when the user wants evaluation, comparison, recommendations, advice, or needs to make a choice between options.\n",
        "\n",
        "            2. **SOLUTION_FOCUSED**: Use when the user wants practical tips, step-by-step solutions, implementation guidance, troubleshooting, or actionable advice.\n",
        "\n",
        "            3. **INFORMATIONAL**: Use when the user wants facts, explanations, overviews, background information, or educational content.\n",
        "\n",
        "            ANALYSIS TARGET:\n",
        "            {analysis_target}\n",
        "\n",
        "            RESPOND WITH ONLY ONE WORD: \"DECISION_MAKING\", \"SOLUTION_FOCUSED\", or \"INFORMATIONAL\"\n",
        "\n",
        "            Think about what the user really wants as an outcome from their query.\"\"\"\n",
        "\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": routing_prompt}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        routing_decision = response.choices[0].message.content.strip().upper()\n",
        "\n",
        "        valid_methods = [\"DECISION_MAKING\", \"SOLUTION_FOCUSED\", \"INFORMATIONAL\"]\n",
        "        if routing_decision in valid_methods:\n",
        "            method = routing_decision.lower()\n",
        "            print(f\"ü§ñ INTELLIGENT ROUTING: LLM determined '{method}' based on query analysis\")\n",
        "            return method\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è LLM returned invalid method '{routing_decision}', using fallback analysis\")\n",
        "            return fallback_intelligent_analysis(original_query, selected_answer_type)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in intelligent routing agent: {e}\")\n",
        "        return fallback_intelligent_analysis(original_query, selected_answer_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r3ZuLh720TE"
      },
      "outputs": [],
      "source": [
        "def fallback_intelligent_analysis(original_query, selected_answer_type):\n",
        "    '''\n",
        "    Backup routing mechanism using keyword pattern matching when LLM-based routing fails.\n",
        "    Scores query against decision-making and solution-focused keyword lists, returns highest scoring category.\n",
        "    '''\n",
        "    query_lower = original_query.lower()\n",
        "    answer_type_lower = (selected_answer_type or \"\").lower()\n",
        "    combined_text = f\"{query_lower} {answer_type_lower}\"\n",
        "\n",
        "    # Decision-making indicators\n",
        "    decision_keywords = [\n",
        "        \"should i\", \"which\", \"better\", \"best\", \"recommend\", \"choose\", \"decide\",\n",
        "        \"compare\", \"versus\", \"vs\", \"pros and cons\", \"advantages\", \"disadvantages\",\n",
        "        \"worth it\", \"advice\", \"suggest\", \"opinion\", \"prefer\", \"evaluation\"\n",
        "    ]\n",
        "\n",
        "    # Solution-focused indicators\n",
        "    solution_keywords = [\n",
        "        \"how to\", \"steps\", \"guide\", \"tutorial\", \"solve\", \"fix\", \"implement\",\n",
        "        \"create\", \"build\", \"make\", \"do\", \"achieve\", \"accomplish\", \"tips\",\n",
        "        \"practical\", \"actionable\", \"process\", \"method\", \"technique\"\n",
        "    ]\n",
        "\n",
        "    decision_score = sum(1 for keyword in decision_keywords if keyword in combined_text)\n",
        "    solution_score = sum(1 for keyword in solution_keywords if keyword in combined_text)\n",
        "\n",
        "    if decision_score > solution_score and decision_score > 0:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Decision-making intent detected (score: {decision_score})\")\n",
        "        return \"DECISION_MAKING\"\n",
        "    elif solution_score > 0:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Solution-focused intent detected (score: {solution_score})\")\n",
        "        return \"SOLUTION_FOCUSED\"\n",
        "    else:\n",
        "        print(f\"üîç FALLBACK ANALYSIS: Informational intent (default)\")\n",
        "        return \"INFORMATIONAL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urmRtiGY20TF"
      },
      "outputs": [],
      "source": [
        "def process_selected_answer_type_intelligent_dynamic(selected_answer_type, selected_topic_intent, original_query, query_id, chatbot_history, state):\n",
        "    '''\n",
        "    Process selected answer type using intelligent routing and dynamic UI management.\n",
        "    Handles CollabSearch pipeline execution, cleans chat history, manages processing state, and provides error recovery.\n",
        "    '''\n",
        "    try:\n",
        "        # Update state with processing info\n",
        "        if isinstance(state, dict):\n",
        "            state[\"processing_query_id\"] = query_id\n",
        "            state[\"selected_topic_intent\"] = selected_topic_intent\n",
        "            state[\"processing\"] = True\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "        # Process with intelligent routing\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            original_query,\n",
        "            selected_topic_intent,\n",
        "            selected_answer_type,\n",
        "            query_id,\n",
        "            state\n",
        "        )\n",
        "\n",
        "        # Remove the last two messages (custom selection + processing messages)\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else []\n",
        "\n",
        "        # Add only the final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        # Reset processing state\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield final_history, state, \"\", gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå **Error processing selected answer type:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "\n",
        "        # Clean up confirmation messages and show error\n",
        "        cleaned_history = chatbot_history[:-2] if len(chatbot_history) >= 2 else chatbot_history\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        # Reset state on error\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield error_history, state, \"\", gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjO7zpZ20TH"
      },
      "outputs": [],
      "source": [
        "def route_synthesis_by_answer_type_intelligent(selected_answer_type, original_query=None, rewritten_query=None):\n",
        "    '''\n",
        "    Wrapper function for synthesis routing with backward compatibility.\n",
        "    Calls comprehensive tracking function but returns only synthesis method to maintain existing code compatibility.\n",
        "    '''\n",
        "    # Call the new comprehensive tracking function\n",
        "    synthesis_method, routing_info = route_synthesis_with_comprehensive_tracking(\n",
        "        selected_answer_type, original_query, rewritten_query\n",
        "    )\n",
        "    return synthesis_method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh1FXVEd20TI"
      },
      "outputs": [],
      "source": [
        "def route_synthesis_with_comprehensive_tracking(selected_answer_type, original_query=None, rewritten_query=None, query_id=None):\n",
        "    '''\n",
        "    Enhanced synthesis routing with detailed decision-process tracking and fallback mechanisms.\n",
        "    Attempts dictionary matching (exact/partial) for answer types, falls back to LLM analysis, returns method and routing metadata.\n",
        "    '''\n",
        "    # Initialize tracking info\n",
        "    routing_info = {\n",
        "        'routing_type': 'UNKNOWN',\n",
        "        'routing_method': 'fallback',\n",
        "        'dictionary_match': '',\n",
        "        'match_found': False,\n",
        "        'answer_type_processed': selected_answer_type or 'None'\n",
        "    }\n",
        "\n",
        "    if not selected_answer_type or len(selected_answer_type.strip()) == 0:\n",
        "        print(\"‚ö†Ô∏è No answer type provided - routing to LLM analysis\")\n",
        "        routing_info.update({\n",
        "            'routing_type': 'LLM',\n",
        "            'routing_method': 'no_answer_type_provided',\n",
        "            'match_found': False\n",
        "        })\n",
        "        result = intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "        return result, routing_info\n",
        "\n",
        "    # Extract main answer type\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip().lower()\n",
        "    print(f\"üîç Processing answer type: '{answer_type_main}'\")\n",
        "\n",
        "    try:\n",
        "        # Comprehensive routing dictionary\n",
        "        synthesis_routing = {\n",
        "            # Decision-making patterns\n",
        "            'comparison': 'DECISION_MAKING',\n",
        "            'recommendation': 'DECISION_MAKING',\n",
        "            'advice': 'DECISION_MAKING',\n",
        "            'evaluation': 'DECISION_MAKING',\n",
        "            'assessment': 'DECISION_MAKING',\n",
        "            'pros and cons': 'DECISION_MAKING',\n",
        "            'which is better': 'DECISION_MAKING',\n",
        "            'should i': 'DECISION_MAKING',\n",
        "            'best option': 'DECISION_MAKING',\n",
        "            'choose': 'DECISION_MAKING',\n",
        "            'decision': 'DECISION_MAKING',\n",
        "            'versus': 'DECISION_MAKING',\n",
        "            'vs': 'DECISION_MAKING',\n",
        "\n",
        "            # Solution-focused patterns\n",
        "            'practical tips': 'SOLUTION_FOCUSED',\n",
        "            'problem-solving': 'SOLUTION_FOCUSED',\n",
        "            'how to': 'SOLUTION_FOCUSED',\n",
        "            'step by step': 'SOLUTION_FOCUSED',\n",
        "            'step-by-step tutorial': 'SOLUTION_FOCUSED',\n",
        "            'guide': 'SOLUTION_FOCUSED',\n",
        "            'tutorial': 'SOLUTION_FOCUSED',\n",
        "            'implementation': 'SOLUTION_FOCUSED',\n",
        "            'troubleshooting': 'SOLUTION_FOCUSED',\n",
        "            'fix': 'SOLUTION_FOCUSED',\n",
        "            'solve': 'SOLUTION_FOCUSED',\n",
        "            'create': 'SOLUTION_FOCUSED',\n",
        "            'build': 'SOLUTION_FOCUSED',\n",
        "            'actionable': 'SOLUTION_FOCUSED',\n",
        "\n",
        "            # Informational patterns\n",
        "            'expert analysis': 'INFORMATIONAL',\n",
        "            'informative': 'INFORMATIONAL',\n",
        "            'basic overview': 'INFORMATIONAL',\n",
        "            'explanation': 'INFORMATIONAL',\n",
        "            'background': 'INFORMATIONAL',\n",
        "            'what is': 'INFORMATIONAL',\n",
        "            'overview': 'INFORMATIONAL',\n",
        "            'summary': 'INFORMATIONAL',\n",
        "            'details': 'INFORMATIONAL',\n",
        "            'information': 'INFORMATIONAL',\n",
        "            'facts': 'INFORMATIONAL',\n",
        "            'learn': 'INFORMATIONAL',\n",
        "            'understand': 'INFORMATIONAL'\n",
        "        }\n",
        "\n",
        "        # Exact match check\n",
        "        if answer_type_main in synthesis_routing:\n",
        "            method = synthesis_routing[answer_type_main]\n",
        "            print(f\"‚úÖ EXACT DICTIONARY MATCH: '{answer_type_main}' ‚Üí '{method}'\")\n",
        "            routing_info.update({\n",
        "                'routing_type': 'DICTIONARY',\n",
        "                'routing_method': 'exact_match',\n",
        "                'dictionary_match': answer_type_main,\n",
        "                'match_found': True\n",
        "            })\n",
        "            return method.lower(), routing_info\n",
        "\n",
        "        # Partial match check\n",
        "        for key, method in synthesis_routing.items():\n",
        "            if key in answer_type_main or answer_type_main in key:\n",
        "                print(f\"‚úÖ PARTIAL DICTIONARY MATCH: '{answer_type_main}' ‚Üî '{key}' ‚Üí '{method}'\")\n",
        "                routing_info.update({\n",
        "                    'routing_type': 'DICTIONARY',\n",
        "                    'routing_method': 'partial_match',\n",
        "                    'dictionary_match': key,\n",
        "                    'match_found': True\n",
        "                })\n",
        "                return method.lower(), routing_info\n",
        "\n",
        "        # No dictionary match found\n",
        "        print(f\"‚ùå NO DICTIONARY MATCH for '{answer_type_main}'\")\n",
        "        print(\"ü§ñ Routing to LLM for intelligent analysis...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dictionary lookup error: {e}\")\n",
        "\n",
        "    # LLM routing fallback\n",
        "    routing_info.update({\n",
        "        'routing_type': 'LLM',\n",
        "        'routing_method': 'dictionary_fallback',\n",
        "        'match_found': False\n",
        "    })\n",
        "\n",
        "    result = intelligent_routing_agent(original_query, selected_answer_type, rewritten_query)\n",
        "    return result, routing_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REW"
      ],
      "metadata": {
        "id": "PDVifmYB-uGE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwR55KDI20TG"
      },
      "outputs": [],
      "source": [
        "def rewrite_query_with_dual_intent(original_query, selected_topic, selected_answer_type, state):\n",
        "    '''\n",
        "    Rewrite query and store in state\n",
        "    '''\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "    answer_type_description = selected_answer_type.split('(')[1].strip(')') if '(' in selected_answer_type else \"\"\n",
        "\n",
        "    instruction = f\"\"\"You have an original user query, their selected topic/domain, and their desired answer type.\n",
        "\n",
        "    Your task: Rewrite the query to be more specific and actionable based on these clarifications.\n",
        "\n",
        "    Original query: \"{original_query}\"\n",
        "    Selected topic/domain: \"{selected_topic}\"\n",
        "    Desired answer type: \"{answer_type_main}\"\n",
        "    Answer type context: \"{answer_type_description}\"\n",
        "\n",
        "    Guidelines:\n",
        "    1. Keep the core intent of the original query\n",
        "    2. Make it more specific to the selected topic, as the user has already specified to which term they refer to. DO NOT CHANGE TOPICS, selected topic: {selected_topic}.\n",
        "    3. Frame it to expect the desired answer type\n",
        "    4. Make it actionable and clear\n",
        "    5. Don't change the fundamental question, remain the query in the specified selected topic ({selected_topic})\n",
        "\n",
        "    Return only the rewritten query, nothing else.\"\"\"\n",
        "\n",
        "    working_query = get_completion(instruction)\n",
        "\n",
        "    # Clean up the response\n",
        "    working_query = working_query.strip()\n",
        "    if working_query.startswith('\"') and working_query.endswith('\"'):\n",
        "        working_query = working_query[1:-1]\n",
        "\n",
        "    # Store in state for later use\n",
        "    state[\"working_query\"] = working_query\n",
        "\n",
        "    return working_query"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Agent Analysis System\n",
        "\n",
        "### Dynamic Role Assignment and Expert Analysis\n",
        "\n",
        "The system employs three distinct expert perspectives:\n",
        "1. **Local Analysis**: Domain-specific expert with deep topical knowledge\n",
        "2. **Expert Analysis**: Subject matter expert with authoritative perspective\n",
        "3. **User Analysis**: General analyst focusing on practical user needs"
      ],
      "metadata": {
        "id": "zdP0h3cC-wVL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxuiV0hS20TG"
      },
      "outputs": [],
      "source": [
        "def get_roles(query, topic):\n",
        "  '''\n",
        "  Generate 3 expert roles dynamically based on clarified topic for multi-perspective analysis.\n",
        "  Uses LLM to create complementary specialist roles within the topic domain, includes retry logic and fallback roles.\n",
        "  '''\n",
        "    max_retries = 100\n",
        "    prompt_roles = f\"\"\"You are a Recruiting Manager. The user has clarified they want to know about: \"{topic}\"\n",
        "\n",
        "                        IMPORTANT CONSTRAINTS:\n",
        "                        - Focus ONLY on the clarified topic: \"{topic}\"\n",
        "                        - IGNORE any ambiguous terms from the original query\n",
        "                        - All 3 expert roles must be specialists within the scope of: \"{topic}\"\n",
        "                        - DO NOT go outside this specific domain\n",
        "\n",
        "                        Provide 3 different expert roles who specialize in \"{topic}\" and can analyze the question from different perspectives within this domain.\n",
        "\n",
        "                        The 3 roles should have complementary expertise areas within \"{topic}\".\n",
        "\n",
        "                        Return ONLY a Python list in this exact format:\n",
        "                        [\"{topic}\", \"expert role 1\", \"expert role 2\", \"expert role 3\"]\n",
        "\n",
        "                        No explanations, no other text, just the list.\"\"\"\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt_roles}]\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"llama-3-8b-instruct\",\n",
        "                messages=messages,\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            if response_text.startswith('[') and response_text.endswith(']'):\n",
        "                definition_list = eval(response_text)  # Convert string to list\n",
        "                if isinstance(definition_list, list) and len(definition_list) >= 4:\n",
        "                    return definition_list\n",
        "\n",
        "            raise ValueError(\"Invalid response format\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                # Fallback\n",
        "                return [\n",
        "                    \"General Knowledge\",\n",
        "                    \"Subject Matter Expert\",\n",
        "                    \"Technical Specialist\",\n",
        "                    \"End User Analyst\"\n",
        "                ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0qB5ntQ20TR"
      },
      "outputs": [],
      "source": [
        "def define_roles(definition_list, state):\n",
        "    \"\"\"\n",
        "    Define roles using state\n",
        "    \"\"\"\n",
        "    state[\"target_role_map\"] = {\n",
        "        \"Local\": definition_list[1],\n",
        "        \"Expert\": definition_list[2],\n",
        "        \"User Analysis\": definition_list[3]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu5xvck9fjdz"
      },
      "outputs": [],
      "source": [
        "def local_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    '''\n",
        "    Generate enhanced local analysis response using dynamically assigned expert role and answer type formatting.\n",
        "    Extracts role from state mapping, formats instructions with answer type requirements, calls LLM with role-based prompt.\n",
        "    '''\n",
        "    role = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "\n",
        "    # Extract the main answer type from the selection\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with deep knowledge about {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    As a {role}, provide your professional analysis addressing this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your response should reflect the expertise and viewpoint that defines your role as a {role} while following the requested answer format.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def expert_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    '''\n",
        "    Generate enhanced expert analysis response with specialized role assignment and answer type consideration.\n",
        "    Uses subject matter expert role from state, incorporates answer type instructions, provides authoritative domain analysis.\n",
        "    '''\n",
        "    role = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} specializing in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Provide your professional expert analysis of this query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should reflect the authority and comprehensive understanding that comes from being a recognized {role} in this domain.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)\n",
        "\n",
        "def user_analysis_enhanced(query, topic, answer_type, state=None):\n",
        "    '''\n",
        "    Generate enhanced user-perspective analysis with complementary expert viewpoint and answer type formatting.\n",
        "    Applies general analyst role from state mapping, formats response according to requested answer type, provides distinct analytical value.\n",
        "    '''\n",
        "    role = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    instruction = f\"\"\"You are a {role} with expertise in {topic}.\n",
        "\n",
        "    User Query: \"{query}\"\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    Analyze this query from your specialized perspective as a {role}.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    Your analysis should complement other expert perspectives while offering the distinct value that only a {role} can provide.\"\"\"\n",
        "\n",
        "    return get_completion_with_role(role, instruction, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP6qy3t320TK"
      },
      "outputs": [],
      "source": [
        "def get_answer_type_instructions(answer_type):\n",
        "    '''\n",
        "    Get specific instructions based on answer type\n",
        "    '''\n",
        "    instructions = {\n",
        "        \"Informative\": \"\"\"\n",
        "        - Provide comprehensive background information and facts\n",
        "        - Include detailed explanations and context\n",
        "        - Cover multiple aspects of the topic\n",
        "        - Use evidence and examples to support points\n",
        "        - Structure information clearly and logically\"\"\",\n",
        "\n",
        "        \"Practical Tips\": \"\"\"\n",
        "        - Focus on actionable advice and guidance\n",
        "        - Provide step-by-step instructions where applicable\n",
        "        - Include specific recommendations and best practices\n",
        "        - Emphasize what the user can actually do\n",
        "        - Make suggestions concrete and implementable\"\"\",\n",
        "\n",
        "        \"Basic Overview\": \"\"\"\n",
        "        - Keep explanations simple and accessible\n",
        "        - Focus on key points and essential information\n",
        "        - Avoid technical jargon or complex details\n",
        "        - Provide a clear, easy-to-understand introduction\n",
        "        - Structure information in a beginner-friendly way\"\"\",\n",
        "\n",
        "        \"Expert Analysis\": \"\"\"\n",
        "        - Provide in-depth professional perspective\n",
        "        - Include technical details and advanced insights\n",
        "        - Reference industry standards and best practices\n",
        "        - Demonstrate specialized knowledge and expertise\n",
        "        - Address complex aspects and nuances\"\"\",\n",
        "\n",
        "        \"Comparison\": \"\"\"\n",
        "        - Present pros and cons clearly\n",
        "        - Compare different options or approaches\n",
        "        - Provide balanced evaluation of alternatives\n",
        "        - Include recommendations based on comparison\n",
        "        - Help user understand trade-offs\"\"\",\n",
        "\n",
        "        \"Problem-Solving\": \"\"\"\n",
        "        - Focus on solutions and troubleshooting\n",
        "        - Address specific issues and challenges\n",
        "        - Provide practical problem-solving approaches\n",
        "        - Include preventive measures where applicable\n",
        "        - Emphasize resolution strategies\"\"\"\n",
        "    }\n",
        "\n",
        "    return instructions.get(answer_type, instructions[\"Informative\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUJFaLA320TL"
      },
      "outputs": [],
      "source": [
        "def stance_analysis_enhanced(query, ling_response, expert_response, user_response, topic, stance, answer_type, state=None):\n",
        "    '''\n",
        "    Enhanced stance analysis that considers answer type\n",
        "    '''\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "\n",
        "    stance_context = {\n",
        "        \"positive\": \"highly beneficial, well-founded, and strongly recommended\",\n",
        "        \"negative\": \"problematic, risky, or not advisable\"\n",
        "    }\n",
        "\n",
        "    stance_description = stance_context.get(stance, stance)\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are conducting stance detection analysis for collaborative decision-making.\n",
        "\n",
        "    Original Query: '''{query}'''\n",
        "    Topic: {topic}\n",
        "    Requested Answer Type: {answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR STANCE: You believe the approaches, recommendations, or solutions presented in response to this query are {stance_description} for the user's situation regarding {topic}.\n",
        "\n",
        "    IMPORTANT: Your argument should be formatted as {answer_type_main.upper()} since that's what the user requested.\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    TASK:\n",
        "    1. **Analyze all three expert perspectives** through your {stance} lens\n",
        "    2. **Extract supporting evidence** that supports your {stance} position\n",
        "    3. **Build your argument** using evidence while following the {answer_type_main} format\n",
        "\n",
        "    Present your {stance} argument with specific evidence from the expert analyses, formatted according to the user's requested answer type.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0rIXXuU20TL"
      },
      "outputs": [],
      "source": [
        "def final_judgement(query, favor_response, against_response, topic):\n",
        "    '''\n",
        "    Enhanced final judgement that synthesizes collaborative analysis\n",
        "    '''\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query. This means:\n",
        "\n",
        "    1. **Evaluate evidence quality**: Assess the strength and credibility of arguments from both sides\n",
        "    2. **Consider user context**: Focus on what would be most beneficial for someone asking this specific query\n",
        "    3. **Balance perspectives**: Integrate the strongest insights from both positive and negative analyses\n",
        "    4. **Provide actionable guidance**: Give the user clear, practical direction\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Deliver a comprehensive yet concise response\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Focus on practical value for the user\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Present as the authoritative answer to their query\n",
        "    - Give a brief, practical response (1-2 paragraphs).\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7alYPKN7fnYQ"
      },
      "outputs": [],
      "source": [
        "def final_judgement_enhanced(query, favor_response, against_response, topic, answer_type):\n",
        "    '''\n",
        "    Enhanced final judgement that considers answer type\n",
        "    '''\n",
        "    answer_type_main = answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are the final decision-maker in a collaborative analysis system. Your role is to synthesize multiple expert perspectives and opposing viewpoints to provide the best possible response to the user.\n",
        "\n",
        "    USER QUERY: \"{query}\"\n",
        "    TOPIC AREA: {topic}\n",
        "    REQUESTED ANSWER TYPE: {answer_type}\n",
        "\n",
        "    COLLABORATIVE ANALYSIS RESULTS:\n",
        "\n",
        "    POSITIVE PERSPECTIVE (Supporting Arguments):\n",
        "    {favor_response}\n",
        "\n",
        "    NEGATIVE PERSPECTIVE (Cautionary Arguments):\n",
        "    {against_response}\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these collaborative analyses to provide the optimal response to the user's query.\n",
        "\n",
        "    CRITICAL: Format your response as {answer_type_main.upper()} as specifically requested by the user:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    OUTPUT REQUIREMENTS:\n",
        "    - Follow the {answer_type_main} format strictly\n",
        "    - Integrate insights from the collaborative analysis\n",
        "    - Focus on practical value for the user\n",
        "    - Be definitive while acknowledging important considerations\n",
        "    - Present as the authoritative answer to their query\n",
        "\n",
        "    Your response should represent the best collective wisdom from the collaborative analysis process, delivered in exactly the format the user requested.\"\"\"\n",
        "\n",
        "    judgement = get_completion(prompt)\n",
        "    return judgement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCQQKdEm20TM"
      },
      "outputs": [],
      "source": [
        "def summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    '''\n",
        "    Synthesize multiple expert analyses into comprehensive factual response for informational queries.\n",
        "    Extracts consensus and differing viewpoints, maintains objectivity, formats according to requested answer type without forcing recommendations.\n",
        "    '''\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\", \"Expert Analyst\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\", \"Subject Matter Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\", \"General Analyst\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Summary Synthesis Agent. Your role is to extract and synthesize the most relevant and important information from multiple expert analyses.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a comprehensive, factual response that directly answers the user's query.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SYNTHESIS GUIDELINES:\n",
        "    1. **Extract key facts**: Pull the most important information from all three experts\n",
        "    2. **Identify consensus**: Where experts agree, present this as reliable information\n",
        "    3. **Note different perspectives**: Where experts offer different viewpoints, present both\n",
        "    4. **Stay factual**: Focus on information, not recommendations\n",
        "    5. **Be comprehensive**: Cover all important aspects mentioned by the experts\n",
        "    6. **Maintain objectivity**: Present information neutrally without bias\n",
        "    7. **Structure clearly**: Organize information logically for easy understanding\n",
        "\n",
        "    DO NOT:\n",
        "    - Force recommendations when the user wants information\n",
        "    - Create artificial pros/cons lists for factual queries\n",
        "    - Add opinions where experts provided facts\n",
        "    - Turn factual content into advice\n",
        "\n",
        "    Provide a well-structured synthesis that gives the user exactly the type of information they requested.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3-tuLjt20TN"
      },
      "outputs": [],
      "source": [
        "def solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state):\n",
        "    '''\n",
        "    Synthesize multiple expert analyses into practical, actionable solutions for solution-focused queries.\n",
        "    Extracts actionable advice from expert perspectives, prioritizes effective approaches, provides clear implementation guidance.\n",
        "    '''\n",
        "    role_1 = state[\"target_role_map\"].get(\"Local\")\n",
        "    role_2 = state[\"target_role_map\"].get(\"Expert\")\n",
        "    role_3 = state[\"target_role_map\"].get(\"User Analysis\")\n",
        "    answer_type_main = selected_answer_type.split('(')[0].strip()\n",
        "\n",
        "    prompt = f\"\"\"You are a Solution Synthesis Agent. Your role is to synthesize expert analyses into practical, actionable solutions.\n",
        "\n",
        "    USER QUERY: \"{working_query}\"\n",
        "    TOPIC: {topic}\n",
        "    REQUESTED ANSWER TYPE: {selected_answer_type}\n",
        "\n",
        "    EXPERT ANALYSES:\n",
        "    From {role_1}: <<<{ling_response}>>>\n",
        "    From {role_2}: [[[{expert_response}]]]\n",
        "    From {role_3}: ---{user_response}---\n",
        "\n",
        "    YOUR TASK:\n",
        "    Synthesize these expert perspectives into a practical, solution-focused response.\n",
        "\n",
        "    IMPORTANT: Format your response as {answer_type_main.upper()}:\n",
        "\n",
        "    {get_answer_type_instructions(answer_type_main)}\n",
        "\n",
        "    SOLUTION GUIDELINES:\n",
        "    1. **Identify the core need**: What is the user trying to accomplish?\n",
        "    2. **Extract actionable advice**: Pull practical steps and recommendations from experts\n",
        "    3. **Prioritize solutions**: Present the most effective approaches first\n",
        "    4. **Consider implementation**: Include practical considerations for execution\n",
        "    5. **Address potential challenges**: Note important limitations or considerations\n",
        "    6. **Provide clear guidance**: Make recommendations specific and actionable\n",
        "    7. **Focus on outcomes**: Help user understand what success looks like\n",
        "\n",
        "    Focus on giving the user a clear path forward based on the expert analyses.\"\"\"\n",
        "\n",
        "    return get_completion(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test LLM Connection"
      ],
      "metadata": {
        "id": "ZR63lZQL_FAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SIbeQC9hDI",
        "outputId": "b39b748d-ef4c-4c9f-e94e-6c772993f3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private LLM Response: \"Machine learns patterns in data, mimics human thinking\"\n"
          ]
        }
      ],
      "source": [
        "# Test your private LLM\n",
        "def test_private_llm():\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Explain how AI works in a few words\"}\n",
        "        ]\n",
        "    )\n",
        "    print(\"Private LLM Response:\", response.choices[0].message.content)\n",
        "\n",
        "test_private_llm()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG System Architecture\n",
        "\n",
        "### Enhanced RAG Processing Pipeline\n",
        "\n",
        "**RAG Architecture Insights:**\n",
        "1. **Search Integration**: Uses Google Search API for current information\n",
        "2. **Source Attribution**: Preserves and displays source URLs\n",
        "3. **Graceful Fallback**: Handles empty search results\n",
        "4. **Structured Data Processing**: Extracts titles and snippets"
      ],
      "metadata": {
        "id": "SHu2G-S__OOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHVfzy8fsM3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Initialize Google Search API wrapper for web search functionality.\n",
        "Creates search tool object for retrieving current web information and sources.\n",
        "'''\n",
        "search_tool = GoogleSearchAPIWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9p9sXiW9hDK"
      },
      "outputs": [],
      "source": [
        "def simple_rag_with_private_llm(query):\n",
        "    '''\n",
        "    Perform RAG (Retrieval-Augmented Generation) using web search to validate and enhance expert recommendations.\n",
        "    Searches Google for current information, uses LLM to analyze results against original recommendation, returns enhanced response with source URLs.\n",
        "    '''\n",
        "    print(f\"RAG processing query: {query[:100]}...\")\n",
        "\n",
        "    try:\n",
        "        # Extract simple search terms from the query\n",
        "        search_query = query #test\n",
        "        print(f\"Extracted search query: {search_query}\")\n",
        "\n",
        "        # Perform web search using structured results\n",
        "        raw_results = search_tool.results(search_query, num_results=5)\n",
        "\n",
        "        # Extract URLs from structured search results\n",
        "        source_urls = [r['link'] for r in raw_results if 'link' in r]\n",
        "        print(f\"Extracted {len(source_urls)} source URLs:\")\n",
        "        for url in source_urls:\n",
        "            print(f\"  - {url}\")\n",
        "\n",
        "        # Generate a readable text summary for the LLM\n",
        "        search_results = \"\\n\".join(\n",
        "        [f\"{r['title']}: {r['snippet']}\" for r in raw_results if 'title' in r and 'snippet' in r]\n",
        "        )\n",
        "\n",
        "        print(f\"Search results preview:\\n{search_results[:300]}...\")\n",
        "\n",
        "        # Check if search was successful\n",
        "        if not search_results.strip():\n",
        "            print(\"No substantial search results found.\")\n",
        "            return \"Based on current information, the original expert recommendation remains valid.\\n\\n---\\n\\nüìö **Note:** Web search was performed but no relevant results were found.\"\n",
        "\n",
        "        # Enhanced RAG prompt\n",
        "        rag_prompt = f\"\"\"Based on the search results, enhance and validate the expert recommendation.\n",
        "\n",
        "        ORIGINAL EXPERT RECOMMENDATION: {query}\n",
        "\n",
        "        CURRENT SEARCH RESULTS:\n",
        "        {search_results}\n",
        "\n",
        "        TASK: Use these search results to validate, update, and enhance the expert recommendation. Focus on:\n",
        "        - Current accuracy of the information\n",
        "        - Recent developments or changes\n",
        "        - Specific details that improve the recommendation\n",
        "        - Any corrections needed based on current data\n",
        "\n",
        "        Provide a clear, enhanced recommendation that incorporates the latest information.\n",
        "\n",
        "        IMPORTANT: Provide the links you used to update the information.\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3-8b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research analyst who validates expert recommendations using current search results. Do not include URLs or sources in your response and do not mention this in your response I am aware you will not include them.\"},\n",
        "                {\"role\": \"user\", \"content\": rag_prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        enhanced_response = response.choices[0].message.content\n",
        "\n",
        "        # Format response with improved source display\n",
        "        final_response_with_sources = format_response_with_sources(enhanced_response, source_urls)\n",
        "\n",
        "        return final_response_with_sources\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"RAG search error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"Unable to retrieve current information for validation.\\n\\n---\\n\\n‚ö†Ô∏è **Error Details:** {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXFQx7OY20TU"
      },
      "outputs": [],
      "source": [
        "def execute_rag_update(chatbot_history, state):\n",
        "    '''\n",
        "    Execute RAG enhancement on previous analysis with comprehensive validation and error handling.\n",
        "    Validates session state, retrieves current web information to update analysis, manages UI states, includes fallback mechanisms.\n",
        "    '''\n",
        "    try:\n",
        "        print(\"üîÑ RAG Enhancement: Starting state-based processing...\")\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 1: Check if state exists and is properly structured\n",
        "        # ========================================\n",
        "        if not state or not isinstance(state, dict):\n",
        "            print(\"‚ùå No valid state found\")\n",
        "            error_msg = \"‚ùå **No active session found.** Please start a new query to use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state or {}, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 2: Check if user has submitted a query through COLA\n",
        "        # ========================================\n",
        "        original_query = state.get(\"original_query\", \"\")\n",
        "        working_query = state.get(\"working_query\", \"\")\n",
        "        processing_query_id = state.get(\"processing_query_id\")\n",
        "\n",
        "        # Enhanced validation for pre-query clicks\n",
        "        if not original_query and not working_query and not processing_query_id:\n",
        "            print(\"‚ùå User clicked RAG before submitting any query\")\n",
        "            error_msg = \"\"\"‚ùå **No query to enhance yet!**\n",
        "\n",
        "                        Please follow these steps:\n",
        "                        1. üìù **Submit your question** in the text box above\n",
        "                        2. üéØ **Select your preferred topic focus** (Option 1, 2, or 3)\n",
        "                        3. üìã **Choose your answer type** (Option 1, 2, or 3)\n",
        "                        4. ‚è≥ **Wait for the analysis to complete**\n",
        "                        5. üîÑ **Then click this button** to enhance with current information\n",
        "\n",
        "                        *The RAG enhancement works best after you've received an initial analysis.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 3: Check if CollabSearch processing is complete\n",
        "        # ========================================\n",
        "        if original_query and not working_query:\n",
        "            print(\"‚ö†Ô∏è Query exists but COLA processing may be incomplete\")\n",
        "            error_msg = \"\"\"‚ö†Ô∏è **Analysis still in progress!**\n",
        "\n",
        "                        Your query is being processed through the COLA framework. Please:\n",
        "                        - üéØ **Complete the topic selection** if prompted\n",
        "                        - üìã **Complete the answer type selection** if prompted\n",
        "                        - ‚è≥ **Wait for the initial analysis to finish**\n",
        "\n",
        "                        *RAG enhancement will be available once the analysis is complete.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # VALIDATION 4: Check if there's content to enhance\n",
        "        # ========================================\n",
        "        if not chatbot_history or len(chatbot_history) == 0:\n",
        "            print(\"‚ùå No chat history to enhance\")\n",
        "            error_msg = \"‚ùå **No conversation history found.** Please submit a query first, then use RAG enhancement.\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # Get the most recent assistant response to enhance\n",
        "        last_assistant_response = \"\"\n",
        "        for msg in reversed(chatbot_history):\n",
        "            if msg.get(\"role\") == \"assistant\" and msg.get(\"content\"):\n",
        "                content = msg.get(\"content\", \"\")\n",
        "                # Skip RAG-related messages to get the actual analysis\n",
        "                if not content.startswith(\"üîÑ\") and not content.startswith(\"‚úÖ\") and not content.startswith(\"‚ùå\"):\n",
        "                    last_assistant_response = content\n",
        "                    break\n",
        "\n",
        "        if not last_assistant_response:\n",
        "            print(\"‚ùå No assistant response found to enhance\")\n",
        "            error_msg = \"\"\"‚ùå **No analysis found to enhance.**\n",
        "\n",
        "                        Please ensure you have:\n",
        "                        1. ‚úÖ **Submitted a complete query**\n",
        "                        2. ‚úÖ **Received an analysis response**\n",
        "                        3. ‚úÖ **Completed the COLA framework process**\n",
        "\n",
        "                        *Then try the RAG enhancement again.*\"\"\"\n",
        "\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield chatbot_history + [error_message], state, \"\"\n",
        "            return\n",
        "\n",
        "        # ========================================\n",
        "        # SUCCESSFUL VALIDATION: Proceed with RAG\n",
        "        # ========================================\n",
        "        print(f\"‚úÖ Validation passed - enhancing query: '{original_query[:50]}...'\")\n",
        "        print(f\"üìù Working query: '{working_query[:50]}...'\")\n",
        "        print(f\"üìä Last response length: {len(last_assistant_response)} characters\")\n",
        "\n",
        "        # Show processing message with helpful context\n",
        "        processing_msg = f\"\"\"üîÑ **Enhancing your analysis with current information...**\n",
        "\n",
        "                        **Your Query:** {original_query[:100]}{'...' if len(original_query) > 100 else ''}\n",
        "\n",
        "                        üîç Searching for the latest information to update and validate the analysis...\"\"\"\n",
        "\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        temp_history = chatbot_history + [processing_message]\n",
        "\n",
        "        yield temp_history, state, \"\"\n",
        "\n",
        "        # ========================================\n",
        "        # RAG ENHANCEMENT: Use state data directly\n",
        "        # ========================================\n",
        "        print(\"ü§ñ Starting RAG enhancement with state data...\")\n",
        "\n",
        "        try:\n",
        "            enhanced_response = enhanced_rag_with_session_state(\n",
        "                original_query=original_query,\n",
        "                working_query=working_query,\n",
        "                previous_analysis=last_assistant_response,\n",
        "                state=state\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ RAG enhancement completed successfully\")\n",
        "\n",
        "            # Store RAG results in state for this session\n",
        "            state[\"rag_enhanced_response\"] = enhanced_response\n",
        "            state[\"rag_timestamp\"] = time.time()\n",
        "            state[\"rag_original_query\"] = original_query\n",
        "\n",
        "        except Exception as rag_error:\n",
        "            print(f\"‚ùå RAG processing failed: {rag_error}\")\n",
        "            enhanced_response = f\"\"\"**RAG Enhancement Notice:**\n",
        "\n",
        "                            The current information lookup encountered an issue: {str(rag_error)}\n",
        "\n",
        "                            **Your original analysis remains valid and complete.** This enhancement failure doesn't affect the quality of the previous response.\n",
        "\n",
        "                            *You can try the enhancement again or continue with the existing analysis.*\"\"\"\n",
        "\n",
        "        # ========================================\n",
        "        # PRESENT ENHANCED RESULTS\n",
        "        # ========================================\n",
        "        success_msg = f\"\"\"‚úÖ **Analysis Enhanced with Current Information!**\n",
        "\n",
        "                            {enhanced_response}\n",
        "\n",
        "                            ---\n",
        "                            *üí° This response combines your original analysis with the latest available information for accuracy and relevance.*\"\"\"\n",
        "\n",
        "        final_message = {\"role\": \"assistant\", \"content\": success_msg}\n",
        "        updated_history = chatbot_history + [final_message]\n",
        "\n",
        "        # ========================================\n",
        "        # OPTIONAL: Background database logging (non-blocking)\n",
        "        # ========================================\n",
        "        try:\n",
        "            if processing_query_id:\n",
        "                # This is just for logging - doesn't affect RAG functionality\n",
        "                background_database_logging(processing_query_id, enhanced_response)\n",
        "        except Exception as db_error:\n",
        "            print(f\"‚ö†Ô∏è Database logging failed (non-critical): {db_error}\")\n",
        "            # Don't show this error to user - it's just logging\n",
        "\n",
        "        yield updated_history, state, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Critical error in execute_rag_update: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Comprehensive error message for users\n",
        "        error_msg = f\"\"\"‚ùå **Enhancement Error**\n",
        "\n",
        "                                An unexpected error occurred during RAG enhancement: `{str(e)}`\n",
        "\n",
        "                                **Your original analysis is still available** in the conversation above. You can:\n",
        "                                - üìã **Continue using the existing analysis**\n",
        "                                - üîÑ **Try enhancement again** in a few moments\n",
        "                                - üí¨ **Submit a new query** if needed\n",
        "\n",
        "                                *This error has been logged for improvement.*\"\"\"\n",
        "\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        updated_history = chatbot_history + [error_message]\n",
        "        yield updated_history, state or {}, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af_6X2C-20TT"
      },
      "outputs": [],
      "source": [
        "def get_last_query_data():\n",
        "    '''\n",
        "    Get the last query data from database\n",
        "    '''\n",
        "    try:\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        if df.empty:\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Debug: Print column names to see what we actually have\n",
        "        print(f\"DEBUG - Database columns: {list(df.columns)}\")\n",
        "\n",
        "        # Try completed queries first, fallback to any query\n",
        "        completed_queries = df[df['Status'] == 'completed']\n",
        "        if not completed_queries.empty:\n",
        "            last_query = completed_queries.iloc[-1]\n",
        "        else:\n",
        "            last_query = df.iloc[-1]\n",
        "\n",
        "        query_id = int(last_query['ID'])\n",
        "\n",
        "        if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "            # New schema\n",
        "            print(\"DEBUG - Using new schema (Original_Query, Rewritten_Query)\")\n",
        "            original_query = str(last_query['Original_Query']) if pd.notna(last_query['Original_Query']) else \"Original query not available\"\n",
        "            rewritten_query = str(last_query['Rewritten_Query']) if pd.notna(last_query['Rewritten_Query']) else original_query\n",
        "        elif 'Query' in df.columns:\n",
        "            # Old schema - fallback to 'Query' column\n",
        "            print(\"DEBUG - Using old schema (Query)\")\n",
        "            query_value = str(last_query['Query']) if pd.notna(last_query['Query']) else \"Query not available\"\n",
        "            original_query = query_value\n",
        "            rewritten_query = query_value  # Use same value for both\n",
        "        else:\n",
        "            print(\"DEBUG - No recognized query columns found\")\n",
        "            print(f\"Available columns: {list(df.columns)}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Handle different possible column names for final response\n",
        "        if 'Final Judgement' in df.columns and pd.notna(last_query['Final Judgement']):\n",
        "            final_response = str(last_query['Final Judgement'])\n",
        "        elif 'Final_Judgement' in df.columns and pd.notna(last_query['Final_Judgement']):\n",
        "            final_response = str(last_query['Final_Judgement'])\n",
        "        else:\n",
        "            final_response = rewritten_query\n",
        "\n",
        "        print(f\"DEBUG - Returning: query_id={query_id}, rewritten_query='{rewritten_query[:50]}...', original_query='{original_query[:50]}...'\")\n",
        "        return query_id, rewritten_query, final_response, original_query\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving last query data: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC8FVy--20TP"
      },
      "outputs": [],
      "source": [
        "def format_response_with_sources(enhanced_response, source_urls):\n",
        "    '''\n",
        "    Format the response with clear, visible source links\n",
        "    '''\n",
        "    if not source_urls:\n",
        "        sources_section = \"\\n\\n---\\n\\nüìö **Note:** Search was performed but specific source URLs could not be extracted. Information has been validated against current web content.\"\n",
        "    else:\n",
        "        sources_section = \"\\n\\n---\\n\\nüìö **Sources Used for This Update:**\\n\\n\"\n",
        "\n",
        "        for i, url in enumerate(source_urls, 1):\n",
        "            try:\n",
        "                from urllib.parse import urlparse\n",
        "                parsed = urlparse(url)\n",
        "                domain = parsed.netloc if parsed.netloc else parsed.path\n",
        "                if domain.startswith('www.'):\n",
        "                    domain = domain[4:]\n",
        "\n",
        "                # Format as clickable link\n",
        "                sources_section += f\"{i}. [{domain}]({url})\\n\"\n",
        "            except:\n",
        "                sources_section += f\"{i}. {url}\\n\"\n",
        "\n",
        "        sources_section += \"\\n*Click on the links above to visit the sources.*\"\n",
        "\n",
        "    return enhanced_response + sources_section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfBJHcQ020TW"
      },
      "outputs": [],
      "source": [
        "def enhanced_rag_with_session_state(original_query, working_query, previous_analysis, state):\n",
        "    '''\n",
        "    Enhance previous analysis with current web information using session state without database dependency.\n",
        "    Uses working query for better search results, validates/updates previous analysis, stores sources in state.\n",
        "    '''\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"üéØ RAG Context:\")\n",
        "    print(f\"   üìù Original: {original_query}\")\n",
        "    print(f\"   üîÑ Working: {working_query}\")\n",
        "    print(f\"   üìä Previous analysis: {len(previous_analysis)} chars\")\n",
        "\n",
        "    try:\n",
        "        # Use the working query (rewritten/contextualized) for better RAG results\n",
        "        query_for_rag = working_query if working_query else original_query\n",
        "\n",
        "        # Create RAG prompt that leverages the previous analysis\n",
        "        rag_prompt = f\"\"\"Based on this answer:\n",
        "\n",
        "                {previous_analysis[:1000]}...\n",
        "\n",
        "                Please provide updated, current information that validates, corrects, or expands upon this answer for the query: \"{query_for_rag}\"\n",
        "\n",
        "                Focus on:\n",
        "                - Latest developments or changes\n",
        "                - Current accuracy of the information\n",
        "                - Recent data or statistics\n",
        "                - Any new perspectives or considerations\"\"\"\n",
        "\n",
        "        print(\"üîç Calling RAG system...\")\n",
        "        enhanced_response = simple_rag_with_private_llm(rag_prompt)\n",
        "\n",
        "        # Extract and log source information\n",
        "        source_urls = extract_source_urls_from_response(enhanced_response)\n",
        "        print(f\"üìö Found {len(source_urls)} sources\")\n",
        "\n",
        "        # Store sources in state\n",
        "        if source_urls:\n",
        "            state[\"rag_sources\"] = source_urls\n",
        "\n",
        "        # Processing time\n",
        "        processing_time = round(time.time() - start_time, 2)\n",
        "        print(f\"‚è±Ô∏è RAG completed in {processing_time} seconds\")\n",
        "\n",
        "        return enhanced_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå RAG processing error: {e}\")\n",
        "        raise e  # Re-raise to be handled by calling function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZyrry-L20TW"
      },
      "outputs": [],
      "source": [
        "def extract_source_urls_from_response(response_text):\n",
        "    '''\n",
        "    Extract source URLs from RAG response text using regex patterns for transparency and source tracking.\n",
        "    Finds URLs in various formats (parentheses, brackets, after \"Source:\"), removes duplicates, returns clean URL list.\n",
        "    '''\n",
        "    if not response_text or not isinstance(response_text, str):\n",
        "        return []\n",
        "\n",
        "    # Common patterns for URLs in RAG responses\n",
        "    patterns = [\n",
        "        r'\\((https?://[^\\)]+)\\)',  # URLs in parentheses\n",
        "        r'Source: (https?://\\S+)',  # URLs after \"Source:\"\n",
        "        r'\\[(https?://[^\\]]+)\\]',   # URLs in brackets\n",
        "        r'https?://\\S+',            # Any standalone URLs\n",
        "    ]\n",
        "\n",
        "    urls = []\n",
        "    for pattern in patterns:\n",
        "        found_urls = re.findall(pattern, response_text)\n",
        "        urls.extend(found_urls)\n",
        "\n",
        "    # Remove duplicates and return\n",
        "    return list(set(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFkRNzLg20TX"
      },
      "outputs": [],
      "source": [
        "def background_database_logging(query_id, enhanced_response):\n",
        "    '''\n",
        "    Optional background logging to database\n",
        "    This runs independently and doesn't affect RAG functionality\n",
        "    '''\n",
        "    try:\n",
        "        import pandas as pd\n",
        "\n",
        "        if not os.path.exists('cola_database.csv'):\n",
        "            print(\"‚ö†Ô∏è Database file not found - skipping logging\")\n",
        "            return\n",
        "\n",
        "        # Simple database update for logging\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "        mask = df['ID'] == query_id\n",
        "\n",
        "        if mask.any():\n",
        "            df.loc[mask, 'After RAG Agent'] = str(enhanced_response)[:1000]  # Truncate for storage\n",
        "            df.loc[mask, 'RAG_Timestamp'] = time.time()\n",
        "            df.to_csv('cola_database.csv', index=False)\n",
        "            print(f\"üìä Logged RAG results for query {query_id}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Query {query_id} not found for logging\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Database logging failed: {e}\")\n",
        "        # Don't raise - this is non-critical logging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CollabSearch** **MAIN**"
      ],
      "metadata": {
        "id": "H1c888PV_d_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk82xbOUfxny"
      },
      "outputs": [],
      "source": [
        "def add_predictions_sequential_intelligent(original_query, selected_intent, selected_answer_type, query_id, state):\n",
        "    '''\n",
        "    Main CollabSearch framework orchestrator that processes queries through the complete multi-stage analysis pipeline.\n",
        "    Coordinates query rewriting, routing, role assignment, expert analysis, synthesis, and database logging with duplicate processing prevention.\n",
        "    '''\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\nüîí PROCESSING: Query {query_id}\")\n",
        "    print(f\"üìù Query: {original_query}\")\n",
        "    print(f\"üéØ Intent: {selected_intent}\")\n",
        "    print(f\"üìÑ Answer Type: {selected_answer_type}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # CRITICAL: Check if this query is already being processed\n",
        "    if state.get(\"currently_processing\") == query_id:\n",
        "        print(f\"‚ö†Ô∏è DUPLICATE PROCESSING PREVENTED for query {query_id}\")\n",
        "        return \"Processing already in progress for this query.\"\n",
        "\n",
        "    # Mark as currently processing\n",
        "    state[\"currently_processing\"] = query_id\n",
        "\n",
        "    try:\n",
        "        # Your existing processing logic here...\n",
        "        # (I'll include the key parts)\n",
        "\n",
        "        # STEP 1: Query rewriting\n",
        "        working_query = rewrite_query_with_dual_intent(original_query, selected_intent, selected_answer_type, state)\n",
        "        state[\"working_query\"] = working_query\n",
        "        print(f\"‚úèÔ∏è Rewritten Query: {working_query}\")\n",
        "\n",
        "        # STEP 2: Routing\n",
        "        synthesis_method, routing_info = route_synthesis_with_comprehensive_tracking(\n",
        "            selected_answer_type=selected_answer_type,\n",
        "            original_query=original_query,\n",
        "            rewritten_query=working_query,\n",
        "            query_id=query_id\n",
        "        )\n",
        "        print(f\"üéØ Synthesis Method: {synthesis_method.upper()}\")\n",
        "\n",
        "        # STEP 3: Role assignment\n",
        "        definition_list = get_roles(working_query, selected_intent)\n",
        "        topic = definition_list[0]\n",
        "        assigned_roles = definition_list[1:]\n",
        "        state[\"topic\"] = topic\n",
        "        define_roles(definition_list, state)\n",
        "\n",
        "        print(f\"üè∑Ô∏è Topic: {topic}\")\n",
        "        print(f\"üë• Roles: {assigned_roles}\")\n",
        "\n",
        "        # STEP 4: Expert analysis\n",
        "        ling_response = local_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "        expert_response = expert_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "        user_response = user_analysis_enhanced(working_query, topic, selected_answer_type, state)\n",
        "\n",
        "        # STEP 5: Synthesis\n",
        "        final_response = \"\"\n",
        "        favor_response = \"\"\n",
        "        against_response = \"\"\n",
        "\n",
        "        print(f\"‚öôÔ∏è SYNTHESIS: {synthesis_method.upper()}\")\n",
        "\n",
        "        if synthesis_method == 'decision_making':\n",
        "            favor_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"positive\", selected_answer_type, state)\n",
        "            against_response = stance_analysis_enhanced(working_query, ling_response, expert_response, user_response, topic, \"negative\", selected_answer_type, state)\n",
        "            final_response = final_judgement_enhanced(working_query, favor_response, against_response, topic, selected_answer_type)\n",
        "        elif synthesis_method == 'solution_focused':\n",
        "            final_response = solution_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "        else:  # informational\n",
        "            final_response = summary_synthesis(working_query, ling_response, expert_response, user_response, topic, selected_answer_type, state)\n",
        "\n",
        "        # STEP 6: Database update\n",
        "        end_time = time.time()\n",
        "        processing_time = round(end_time - start_time, 2)\n",
        "\n",
        "        comprehensive_results = {\n",
        "            'Original_Query': original_query,\n",
        "            'Rewritten_Query': working_query,\n",
        "            'Selected_Topic_Intent': selected_intent,\n",
        "            'Selected_Answer_Type': selected_answer_type,\n",
        "            'Routing_Type': routing_info['routing_type'],\n",
        "            'Routing_Method': routing_info['routing_method'],\n",
        "            'Dictionary_Match': routing_info.get('dictionary_match', ''),\n",
        "            'Identified_Topic': topic,\n",
        "            'Assigned_Roles': json.dumps(assigned_roles),\n",
        "            'Linguist_Analysis': ling_response,\n",
        "            'Expert_Analysis': expert_response,\n",
        "            'User_Analysis': user_response,\n",
        "            'In_Favor': favor_response,\n",
        "            'Against': against_response,\n",
        "            'Final_Judgement': final_response,\n",
        "            'Synthesis_Method': synthesis_method,\n",
        "            'Processing_Time_Seconds': processing_time,\n",
        "        }\n",
        "\n",
        "        update_comprehensive_results(query_id, comprehensive_results)\n",
        "\n",
        "        print(f\"‚úÖ PROCESSING COMPLETE: {processing_time}s\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PROCESSING ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"‚ùå **Error during analysis:** {str(e)}\"\n",
        "\n",
        "    finally:\n",
        "        # CRITICAL: Clear processing lock\n",
        "        if state.get(\"currently_processing\") == query_id:\n",
        "            state[\"currently_processing\"] = None\n",
        "            print(f\"üîì Processing lock cleared for query {query_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UI MAIN"
      ],
      "metadata": {
        "id": "Qqev9PjC_27G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIQyIX3_9hDL"
      },
      "outputs": [],
      "source": [
        "def slow_echo_with_dual_intent_disambiguation_dynamic(message, chatbot_history, state):\n",
        "    '''\n",
        "    Main chatbot entry point that handles user queries with dual-intent disambiguation while preserving chat history.\n",
        "    Resets processing state, generates unique query ID, initiates topic disambiguation, manages UI state transitions.\n",
        "    '''\n",
        "\n",
        "    # Initialize state if None\n",
        "    if not state:\n",
        "        state = {}\n",
        "\n",
        "    # Reset only processing variables, preserve chat history\n",
        "    processing_reset = {\n",
        "        \"query_id\": None,\n",
        "        \"original_query\": \"\",\n",
        "        \"intent_options\": [],\n",
        "        \"step\": \"topic_selection\",\n",
        "        \"processing_query_id\": None,\n",
        "        \"answer_type_options\": [],\n",
        "        \"selected_topic_intent\": \"\",\n",
        "        \"working_query\": \"\",\n",
        "        \"topic\": \"\",\n",
        "        \"target_role_map\": {},\n",
        "        \"selected_answer_type\": \"\",\n",
        "        \"processing\": False,\n",
        "        \"waiting_for_custom\": False,\n",
        "        \"custom_input_type\": \"\",\n",
        "        \"show_options\": False,\n",
        "        \"show_main_input\": True,\n",
        "        \"currently_processing\": None\n",
        "    }\n",
        "\n",
        "    # Update state with reset values\n",
        "    state.update(processing_reset)\n",
        "\n",
        "    # Generate unique query ID\n",
        "    import time\n",
        "    current_id = int(time.time() * 1000000)\n",
        "\n",
        "    # Set fresh state for this query\n",
        "    state[\"query_id\"] = current_id\n",
        "    state[\"original_query\"] = message\n",
        "    state[\"step\"] = \"topic_selection\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"processing\"] = False\n",
        "\n",
        "    # Preserve existing chat history properly\n",
        "    if chatbot_history is None:\n",
        "        chatbot_history = []\n",
        "\n",
        "    # Add user message to existing history (don't replace it)\n",
        "    user_message = {\"role\": \"user\", \"content\": message}\n",
        "    updated_history = chatbot_history + [user_message]\n",
        "\n",
        "    # Initial yield with clean UI state\n",
        "    yield updated_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        print(f\"üöÄ STARTING QUERY PROCESSING\")\n",
        "        print(f\"üìã Query ID: {current_id}\")\n",
        "        print(f\"üìù Query: {message}\")\n",
        "        print(f\"üìö Existing chat history: {len(chatbot_history)} messages\")\n",
        "\n",
        "        # Add query to database\n",
        "        add_new_query(current_id, message)\n",
        "\n",
        "        # Generate topic intent options\n",
        "        topic_intent_options = generate_intent_options(message)\n",
        "        print(f\"üéØ Generated options: {topic_intent_options}\")\n",
        "\n",
        "        # Store in state\n",
        "        state[\"intent_options\"] = topic_intent_options\n",
        "\n",
        "        # Show intent options WITHOUT removing user message\n",
        "        final_history, _ = show_intent_options_clean(updated_history, state, topic_intent_options)\n",
        "\n",
        "        # Show options UI\n",
        "        yield final_history, state, \"\", gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"‚ùå **Error in intent disambiguation:** {str(e)}\"\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "        error_history = updated_history + [{\"role\": \"assistant\", \"content\": error_message}]\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = False\n",
        "\n",
        "        yield error_history, state, \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUEdsfj020Tp"
      },
      "outputs": [],
      "source": [
        "def view_database_stats():\n",
        "    \"\"\"View database statistics\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('cola_database.csv')\n",
        "\n",
        "        total_queries = len(df)\n",
        "        completed = len(df[df['Status'] == 'completed'])\n",
        "        pending = len(df[df['Status'] == 'pending'])\n",
        "        errors = len(df[df['Status'] == 'error'])\n",
        "\n",
        "        # Calculate average processing time for completed queries\n",
        "        completed_df = df[df['Status'] == 'completed']\n",
        "        if not completed_df.empty and 'Processing_Time_Seconds' in completed_df.columns:\n",
        "            # Filter out NaN values before calculating mean\n",
        "            processing_times = completed_df['Processing_Time_Seconds'].dropna()\n",
        "            if not processing_times.empty:\n",
        "                avg_time = processing_times.mean()\n",
        "                avg_time_str = f\"- Average processing time: {avg_time:.2f} seconds\"\n",
        "            else:\n",
        "                avg_time_str = \"- Average processing time: N/A\"\n",
        "        else:\n",
        "            avg_time_str = \"- Average processing time: N/A\"\n",
        "\n",
        "        stats = f\"\"\"\n",
        "        Database Statistics:\n",
        "        - Total queries: {total_queries}\n",
        "        - Completed: {completed}\n",
        "        - Pending: {pending}\n",
        "        - Errors: {errors}\n",
        "        {avg_time_str}\n",
        "\n",
        "        Database Schema: {list(df.columns)}\n",
        "\n",
        "        Recent queries:\n",
        "        \"\"\"\n",
        "\n",
        "        if not df.empty:\n",
        "            # Handle both old and new schema for display\n",
        "            if 'Original_Query' in df.columns and 'Rewritten_Query' in df.columns:\n",
        "                # New schema\n",
        "                display_columns = ['ID', 'Original_Query', 'Rewritten_Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "            else:\n",
        "                # Old schema\n",
        "                display_columns = ['ID', 'Query', 'Status', 'Processing_Time_Seconds', 'Timestamp']\n",
        "\n",
        "            # Only show columns that actually exist\n",
        "            existing_columns = [col for col in display_columns if col in df.columns]\n",
        "            recent = df.tail(5)[existing_columns]\n",
        "            stats += recent.to_string(index=False)\n",
        "\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        return f\"Error reading database: {e}\\n\\nColumns found: {list(pd.read_csv('cola_database.csv').columns) if os.path.exists('cola_database.csv') else 'File not found'}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option Handlers"
      ],
      "metadata": {
        "id": "ljyX61OYADCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LeRn4EK20Tq"
      },
      "outputs": [],
      "source": [
        "def handle_option_1_click_enhanced_dynamic(chatbot_history, state):\n",
        "    '''\n",
        "    Handle Option 1 button clicks in two-step disambiguation process (topic selection then answer type selection).\n",
        "    Manages state transitions, cleans chat history, executes CollabSearch framework processing, and controls UI element visibility.\n",
        "    '''\n",
        "    if not isinstance(state, dict):\n",
        "        state = {\"step\": \"topic_selection\", \"intent_options\": [], \"answer_type_options\": [],\n",
        "                \"selected_topic_intent\": \"\", \"query_id\": None, \"original_query\": \"\",\n",
        "                \"show_options\": False, \"processing\": False, \"show_main_input\": True}\n",
        "\n",
        "    if state.get(\"step\") == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 0:\n",
        "        # STEP 1: Topic selection\n",
        "        selected_topic = state[\"intent_options\"][0]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        # Generate answer type options\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # Remove only the intent options bubble, keep user query\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            # Keep user query, remove intent options\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        # Add answer type options\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif state.get(\"step\") == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 0:\n",
        "        # STEP 2: Answer type selection\n",
        "        selected_answer_type = state[\"answer_type_options\"][0]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "\n",
        "        # Remove only the answer type options, keep user query\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            # Keep user query and any other messages, remove answer type options\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        # Show processing message\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        # Hide buttons during processing\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process query\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            # REPLACE processing message with final answer\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ1FH2xn20Tr"
      },
      "outputs": [],
      "source": [
        "def handle_option_2_click_enhanced_dynamic(chatbot_history, state):\n",
        "    '''\n",
        "    Handle Option 2 button clicks in two-step disambiguation process, mirroring Option 1 functionality for second choice.\n",
        "    Manages topic and answer type selection, cleans chat history, executes CollabSearch processing, controls UI visibility and error handling.\n",
        "    '''\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 1:\n",
        "        selected_topic = state[\"intent_options\"][1]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # Clean history and show answer type options\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 1:\n",
        "        selected_answer_type = state[\"answer_type_options\"][1]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "\n",
        "        # Clean history\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        # Show processing\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76eBptz-20Tt"
      },
      "outputs": [],
      "source": [
        "def handle_option_3_click_enhanced_dynamic(chatbot_history, state):\n",
        "    '''\n",
        "    Handle Option 3 button clicks in two-step disambiguation process, completing the trio of selection options.\n",
        "    Manages topic and answer type selection for third choice, cleans chat history, executes CollabSearch processing, controls UI visibility.\n",
        "    '''\n",
        "    if not state:\n",
        "        yield chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "        return\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\" and state.get(\"intent_options\") and len(state[\"intent_options\"]) > 2:\n",
        "        selected_topic = state[\"intent_options\"][2]\n",
        "        state[\"selected_topic_intent\"] = selected_topic\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), selected_topic)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # Clean history and show answer type options\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    elif current_step == \"answer_type_selection\" and state.get(\"answer_type_options\") and len(state[\"answer_type_options\"]) > 2:\n",
        "        selected_answer_type = state[\"answer_type_options\"][2]\n",
        "\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"processing\"] = True\n",
        "\n",
        "        # Clean history\n",
        "        if chatbot_history and len(chatbot_history) >= 2:\n",
        "            cleaned_history = chatbot_history[:-1]\n",
        "        else:\n",
        "            cleaned_history = chatbot_history\n",
        "\n",
        "        # Show processing\n",
        "        processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' using the COLA framework...\"\n",
        "        processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "        processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "        yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        # Process\n",
        "        try:\n",
        "            answer = add_predictions_sequential_intelligent(\n",
        "                state.get(\"original_query\", \"\"),\n",
        "                state.get(\"selected_topic_intent\", \"\"),\n",
        "                selected_answer_type,\n",
        "                state.get(\"query_id\"),\n",
        "                state\n",
        "            )\n",
        "\n",
        "            final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "            final_history = cleaned_history + [final_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå **Error processing query:** {str(e)}\"\n",
        "            error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            error_history = cleaned_history + [error_message]\n",
        "\n",
        "            state[\"processing\"] = False\n",
        "            state[\"show_options\"] = False\n",
        "\n",
        "            yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=state.get(\"show_options\", False)), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Q-s0Xv20Tu"
      },
      "outputs": [],
      "source": [
        "def handle_other_option_click_dynamic(chatbot_history, state):\n",
        "    '''\n",
        "    Handle \"Other\" option clicks to allow custom topic or answer type specification.\n",
        "    Prompts user for custom input based on current step (topic/answer type), manages UI transitions, updates state for custom input processing.\n",
        "    '''\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        # Topic selection phase - ask for custom topic\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"topic\"\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = False\n",
        "\n",
        "        # Clean history and show custom input request\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        custom_msg = \"\"\"üîß **Custom Topic Selection**\n",
        "\n",
        "The provided options don't match what you're looking for? No problem!\n",
        "\n",
        "Please specify your preferred topic or domain in the text box below. For example:\n",
        "- \"Python machine learning\"\n",
        "- \"Apple company stock\"\n",
        "- \"Java coffee brewing\"\n",
        "\n",
        "*Tip: Be as specific as possible to get the best results.*\"\"\"\n",
        "\n",
        "        custom_message = {\"role\": \"assistant\", \"content\": custom_msg}\n",
        "        updated_history = cleaned_history + [custom_message]\n",
        "\n",
        "        return updated_history, state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        # Answer type selection phase\n",
        "        state[\"waiting_for_custom\"] = True\n",
        "        state[\"custom_input_type\"] = \"answer_type\"\n",
        "        state[\"show_options\"] = False\n",
        "        state[\"show_main_input\"] = False\n",
        "\n",
        "        # Clean history and show custom input request\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "        custom_msg = \"\"\"üîß **Custom Answer Type**\n",
        "\n",
        "Need a different type of response? Please specify what kind of answer you're looking for:\n",
        "\n",
        "Examples:\n",
        "- \"Step-by-step tutorial\"\n",
        "- \"Pros and cons comparison\"\n",
        "- \"Historical timeline\"\n",
        "- \"Technical specifications\"\n",
        "\n",
        "*Tip: Describe the format or style of response you prefer.*\"\"\"\n",
        "\n",
        "        custom_message = {\"role\": \"assistant\", \"content\": custom_msg}\n",
        "        updated_history = cleaned_history + [custom_message]\n",
        "\n",
        "        return updated_history, state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \"\"\n",
        "\n",
        "    else:\n",
        "        return chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EELa2GWm20Tu"
      },
      "outputs": [],
      "source": [
        "def process_custom_input_with_spellcheck(user_input, input_type):\n",
        "    '''\n",
        "    Process and clean user's custom input with basic spell checking for common technical terms.\n",
        "    Applies corrections for frequent misspellings of programming languages, tech concepts, and common words using regex substitution.\n",
        "    '''\n",
        "\n",
        "    # Basic cleaning\n",
        "    cleaned_input = user_input.strip()\n",
        "\n",
        "    # Basic spell corrections\n",
        "    spell_corrections = {\n",
        "        \"phyton\": \"Python\", \"pyhton\": \"Python\", \"pythn\": \"Python\",\n",
        "        \"javas\": \"Java\", \"jave\": \"Java\",\n",
        "        \"javascript\": \"JavaScript\", \"js\": \"JavaScript\",\n",
        "        \"machien learning\": \"machine learning\",\n",
        "        \"artifical intelligence\": \"artificial intelligence\",\n",
        "        \"blockchian\": \"blockchain\",\n",
        "        \"cyrptocurrency\": \"cryptocurrency\",\n",
        "        \"tutorail\": \"tutorial\", \"tutoral\": \"tutorial\",\n",
        "        \"comparision\": \"comparison\", \"comparsion\": \"comparison\",\n",
        "        \"recomendation\": \"recommendation\", \"recomendations\": \"recommendations\"\n",
        "    }\n",
        "\n",
        "    # Apply corrections\n",
        "    for wrong, correct in spell_corrections.items():\n",
        "        if wrong.lower() in cleaned_input.lower():\n",
        "            cleaned_input = re.sub(re.escape(wrong), correct, cleaned_input, flags=re.IGNORECASE)\n",
        "\n",
        "    return cleaned_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtLMsWaN20Tv"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_submit_dynamic(custom_input, chatbot_history, state):\n",
        "    '''\n",
        "    Process custom user input during disambiguation with validation, spell checking, and state management.\n",
        "    Handles custom topic or answer type specification, validates input, updates state transitions, manages UI element visibility.\n",
        "    '''\n",
        "    if not state or not state.get(\"waiting_for_custom\"):\n",
        "        yield chatbot_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "        return\n",
        "\n",
        "    if not custom_input or not custom_input.strip():\n",
        "        error_msg = {\"role\": \"assistant\", \"content\": \"‚ùå Please enter your custom option before submitting.\"}\n",
        "        yield chatbot_history + [error_msg], state, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), custom_input\n",
        "        return\n",
        "\n",
        "    # Process the custom input\n",
        "    input_type = state.get(\"custom_input_type\", \"topic\")\n",
        "    processed_input = process_custom_input_with_spellcheck(custom_input, input_type)\n",
        "\n",
        "    # Clean state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_main_input\"] = True\n",
        "\n",
        "    current_step = state.get(\"step\", \"topic_selection\")\n",
        "\n",
        "    if current_step == \"topic_selection\":\n",
        "        # Handle custom topic selection\n",
        "        state[\"selected_topic_intent\"] = processed_input\n",
        "\n",
        "        answer_type_options = generate_answer_type_options(state.get(\"original_query\", \"\"), processed_input)\n",
        "        state[\"answer_type_options\"] = answer_type_options\n",
        "        state[\"step\"] = \"answer_type_selection\"\n",
        "        state[\"show_options\"] = True\n",
        "\n",
        "        # Clean history and show answer type options\n",
        "        cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "        final_history, _ = show_answer_type_options_clean(cleaned_history, state, answer_type_options)\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\"\n",
        "\n",
        "    elif current_step == \"answer_type_selection\":\n",
        "        # Handle custom answer type - use generator pattern\n",
        "        yield from handle_custom_answer_type_processing(processed_input, chatbot_history, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6pUSsbp20Tv"
      },
      "outputs": [],
      "source": [
        "def handle_custom_answer_type_processing(processed_input, chatbot_history, state):\n",
        "    '''\n",
        "    Generator function for processing custom answer type selection with real-time UI updates.\n",
        "    Handles processing state management, shows progress messages, executes COLA analysis, provides error handling with yield pattern.\n",
        "    '''\n",
        "    state[\"selected_answer_type\"] = processed_input\n",
        "    state[\"show_options\"] = False\n",
        "    state[\"processing\"] = True\n",
        "\n",
        "    # Clean history\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "    # Show processing message\n",
        "    processing_msg = f\"üîÑ **Processing your analysis...**\\n\\nAnalyzing '{state.get('original_query', '')}' with custom answer type: {processed_input}\"\n",
        "    processing_message = {\"role\": \"assistant\", \"content\": processing_msg}\n",
        "    processing_history = cleaned_history + [processing_message]\n",
        "\n",
        "    # Clean state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_options\"] = False\n",
        "    state[\"show_main_input\"] = True\n",
        "\n",
        "    # First yield - show processing\n",
        "    yield processing_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # Process\n",
        "    try:\n",
        "        answer = add_predictions_sequential_intelligent(\n",
        "            state.get(\"original_query\", \"\"),\n",
        "            state.get(\"selected_topic_intent\", \"\"),\n",
        "            processed_input,\n",
        "            state.get(\"query_id\"),\n",
        "            state\n",
        "        )\n",
        "\n",
        "        # Replace processing with final answer\n",
        "        final_message = {\"role\": \"assistant\", \"content\": str(answer)}\n",
        "        final_history = cleaned_history + [final_message]\n",
        "\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield final_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå **Processing Error:** {str(e)}\"\n",
        "        error_message = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        error_history = cleaned_history + [error_message]\n",
        "\n",
        "        state[\"processing\"] = False\n",
        "        state[\"show_options\"] = False\n",
        "\n",
        "        yield error_history, state, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSc6Twj320Tw"
      },
      "outputs": [],
      "source": [
        "def handle_custom_input_cancel_dynamic(chatbot_history, state):\n",
        "    '''\n",
        "    Handle cancellation of custom input during disambiguation process.\n",
        "    Clears custom input state, removes custom input request message, restores option selection UI, adds cancellation confirmation.\n",
        "    '''\n",
        "    if not state:\n",
        "        return chatbot_history, {}, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), \"\"\n",
        "\n",
        "    # Clear custom input state\n",
        "    state[\"waiting_for_custom\"] = False\n",
        "    state[\"custom_input_type\"] = \"\"\n",
        "    state[\"show_options\"] = True\n",
        "    state[\"show_main_input\"] = True\n",
        "\n",
        "    # Remove custom input request message\n",
        "    cleaned_history = chatbot_history[:-1] if chatbot_history else []\n",
        "\n",
        "    # Add cancellation message\n",
        "    cancel_msg = {\"role\": \"assistant\", \"content\": \"‚ùå **Custom input cancelled.** Please select one of the provided options above.\"}\n",
        "    updated_history = cleaned_history + [cancel_msg]\n",
        "\n",
        "    return updated_history, state, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJI9KmTS20Tw"
      },
      "outputs": [],
      "source": [
        "def safe_close():\n",
        "    '''\n",
        "    Clean shutdown function\n",
        "    '''\n",
        "    print(\"üîÑ Closing application...\")\n",
        "    # Add any cleanup here\n",
        "    return \"‚úÖ Application closed safely. You can close the browser tab now.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##User Interface Call"
      ],
      "metadata": {
        "id": "jIaPd8H-ALt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mk5Ynu220Tx"
      },
      "outputs": [],
      "source": [
        "def create_enhanced_gradio_interface():\n",
        "    '''\n",
        "    Create enhanced Gradio web interface for CollabSearch framework with dual-intent disambiguation and RAG enhancement.\n",
        "    Initializes database, sets up chatbot UI, option buttons, custom input handling, state management, and event handlers for complete user interaction.\n",
        "    '''\n",
        "    # Initialize database at startup\n",
        "    try:\n",
        "        initialize_database_with_sources()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Database initialization failed: {e}\")\n",
        "\n",
        "    with gr.Blocks(title=\"COLA Framework - Enhanced Collaborative Search\") as demo:\n",
        "\n",
        "        # Chat interface\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Enhanced Collaborative Search with Dual Intent Clarification\",\n",
        "            type=\"messages\",\n",
        "            height=500,\n",
        "            show_label=True,\n",
        "            container=True\n",
        "        )\n",
        "\n",
        "        # Main input components\n",
        "        with gr.Row(visible=True) as main_input_row:\n",
        "            with gr.Column():\n",
        "                msg = gr.Textbox(\n",
        "                    label=\"Your query\",\n",
        "                    placeholder=\"Ask me anything! (e.g., 'What is Python?', 'How to learn Java?')\",\n",
        "                    lines=2,\n",
        "                    max_lines=5\n",
        "                )\n",
        "                send_btn = gr.Button(\"üöÄ Send Query\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        # Intent selection buttons\n",
        "        with gr.Row(visible=False) as option_buttons_row:\n",
        "            option1_btn = gr.Button(\"Option 1\", size=\"lg\", variant=\"secondary\")\n",
        "            option2_btn = gr.Button(\"Option 2\", size=\"lg\", variant=\"secondary\")\n",
        "            option3_btn = gr.Button(\"Option 3\", size=\"lg\", variant=\"secondary\")\n",
        "            option4_btn = gr.Button(\"Other\", size=\"lg\", variant=\"secondary\")\n",
        "\n",
        "        # Custom input components\n",
        "        with gr.Row(visible=False) as custom_input_row:\n",
        "            with gr.Column():\n",
        "                custom_input = gr.Textbox(\n",
        "                    label=\"Your custom option\",\n",
        "                    placeholder=\"Type your preferred topic or answer type here...\",\n",
        "                    lines=2\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    submit_custom_btn = gr.Button(\"‚úÖ Submit Custom Option\", variant=\"primary\")\n",
        "                    cancel_custom_btn = gr.Button(\"‚ùå Cancel\", variant=\"secondary\")\n",
        "\n",
        "        # RAG enhancement button\n",
        "        extra_btn = gr.Button(\"üîÑ Fact Check with Google\", variant=\"secondary\")\n",
        "\n",
        "        # Management buttons\n",
        "        with gr.Row():\n",
        "            stats_btn = gr.Button(\"üìä View Database Stats\")\n",
        "            close_btn = gr.Button(\"üî¥ Close App\", variant=\"stop\")\n",
        "\n",
        "        stats_output = gr.Textbox(label=\"Database Information\", lines=10, visible=False)\n",
        "\n",
        "        # Enhanced state management\n",
        "        state = gr.State({\n",
        "            \"query_id\": None,\n",
        "            \"original_query\": \"\",\n",
        "            \"processing_query_id\": None,\n",
        "            \"intent_options\": [],\n",
        "            \"answer_type_options\": [],\n",
        "            \"selected_topic_intent\": \"\",\n",
        "            \"selected_answer_type\": \"\",\n",
        "            \"step\": \"topic_selection\",\n",
        "            \"working_query\": \"\",\n",
        "            \"topic\": \"\",\n",
        "            \"target_role_map\": {},\n",
        "            \"waiting_for_custom\": False,\n",
        "            \"custom_input_type\": \"\",\n",
        "            \"show_options\": False,\n",
        "            \"processing\": False,\n",
        "            \"show_main_input\": True,\n",
        "            \"currently_processing\": None\n",
        "        })\n",
        "\n",
        "        # Event handlers - direct function calls (no wrapper for generators)\n",
        "\n",
        "        # Main query handlers (generators)\n",
        "        send_btn.click(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        msg.submit(\n",
        "            slow_echo_with_dual_intent_disambiguation_dynamic,\n",
        "            inputs=[msg, chatbot, state],\n",
        "            outputs=[chatbot, state, msg, main_input_row, option_buttons_row, custom_input_row]\n",
        "        ).then(lambda: \"\", outputs=[msg])\n",
        "\n",
        "        # Option handlers (generators)\n",
        "        option1_btn.click(\n",
        "            handle_option_1_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        option2_btn.click(\n",
        "            handle_option_2_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        option3_btn.click(\n",
        "            handle_option_3_click_enhanced_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row]\n",
        "        )\n",
        "\n",
        "        option4_btn.click(\n",
        "            handle_other_option_click_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # Custom input handlers (generators)\n",
        "        submit_custom_btn.click(\n",
        "            handle_custom_input_submit_dynamic,\n",
        "            inputs=[custom_input, chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        cancel_custom_btn.click(\n",
        "            handle_custom_input_cancel_dynamic,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, main_input_row, option_buttons_row, custom_input_row, custom_input]\n",
        "        )\n",
        "\n",
        "        # RAG handler (generator)\n",
        "        extra_btn.click(\n",
        "            execute_rag_update,\n",
        "            inputs=[chatbot, state],\n",
        "            outputs=[chatbot, state, msg]\n",
        "        )\n",
        "\n",
        "        # Stats handler (non-generator)\n",
        "        def show_stats():\n",
        "            try:\n",
        "                stats = view_comprehensive_database_stats()\n",
        "                return gr.update(visible=True), stats\n",
        "            except Exception as e:\n",
        "                return gr.update(visible=True), f\"Error loading stats: {e}\"\n",
        "\n",
        "        stats_btn.click(show_stats, outputs=[stats_output, stats_output])\n",
        "\n",
        "        def safe_close():\n",
        "            return \"‚úÖ Application closed safely. You can close the browser tab now.\"\n",
        "\n",
        "        close_btn.click(safe_close, outputs=[stats_output])\n",
        "\n",
        "        demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKx_zsuD20Ty",
        "outputId": "3b30be0b-56fd-4806-e42d-4f096b0edd6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Database already exists\n",
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "* Running on public URL: https://e9deef606019946315.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e9deef606019946315.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ STARTING QUERY PROCESSING\n",
            "üìã Query ID: 1755694204422243\n",
            "üìù Query: Lastest Iphone models\n",
            "üìö Existing chat history: 0 messages\n",
            "‚úÖ Added query 1755694204422243 to database\n",
            "üéØ Generated options: ['Best Iphone Models for Photography', 'Latest Iphone Models for Business', 'Top Iphone Models for Gaming']\n",
            "\n",
            "üîí PROCESSING: Query 1755694204422243\n",
            "üìù Query: Lastest Iphone models\n",
            "üéØ Intent: Best Iphone Models for Photography\n",
            "üìÑ Answer Type: Comparison/Evaluation (pros and cons of latest iPhone models for photography)\n",
            "======================================================================\n",
            "‚úèÔ∏è Rewritten Query: What are the pros and cons of the latest iPhone models for photography, and which one is the best for capturing high-quality images?\n",
            "üîç Processing answer type: 'comparison/evaluation'\n",
            "‚úÖ PARTIAL DICTIONARY MATCH: 'comparison/evaluation' ‚Üî 'comparison' ‚Üí 'DECISION_MAKING'\n",
            "üéØ Synthesis Method: DECISION_MAKING\n",
            "üè∑Ô∏è Topic: Best Iphone Models for Photography\n",
            "üë• Roles: ['iPhone Photographer', 'Mobile Photography Specialist', 'Apple Device Analyst']\n",
            "‚öôÔ∏è SYNTHESIS: DECISION_MAKING\n",
            "‚úÖ Updated comprehensive results for query 1755694204422243\n",
            "‚úÖ PROCESSING COMPLETE: 79.11s\n",
            "======================================================================\n",
            "üîì Processing lock cleared for query 1755694204422243\n",
            "üîÑ RAG Enhancement: Starting state-based processing...\n",
            "‚úÖ Validation passed - enhancing query: 'Lastest Iphone models...'\n",
            "üìù Working query: 'What are the pros and cons of the latest iPhone mo...'\n",
            "üìä Last response length: 4055 characters\n",
            "ü§ñ Starting RAG enhancement with state data...\n",
            "üéØ RAG Context:\n",
            "   üìù Original: Lastest Iphone models\n",
            "   üîÑ Working: What are the pros and cons of the latest iPhone models for photography, and which one is the best for capturing high-quality images?\n",
            "   üìä Previous analysis: 4055 chars\n",
            "üîç Calling RAG system...\n",
            "RAG processing query: Based on this answer: \n",
            "\n",
            "                **COMPARISON/EVALUATION: Best iPhone Models for Photography*...\n",
            "Extracted search query: Based on this answer: \n",
            "\n",
            "                **COMPARISON/EVALUATION: Best iPhone Models for Photography**\n",
            "\n",
            "As the final decision-maker in this collaborative analysis system, I will synthesize the insights from both positive and negative perspectives to provide a comprehensive evaluation of the latest iPhone models for photography.\n",
            "\n",
            "**Background Information:**\n",
            "\n",
            "The latest iPhone models, including the iPhone 13, iPhone 14, and iPhone 15 series, have undergone significant upgrades in their camera systems. Each model boasts advanced features, such as improved sensors, new lenses, and enhanced software capabilities. However, these upgrades come with a price tag, and the user's situation may not warrant the investment.\n",
            "\n",
            "**Pros and Cons of Latest iPhone Models for Photography:**\n",
            "\n",
            "### iPhone 13 Series:\n",
            "\n",
            "Pros:\n",
            "\n",
            "1. **Improved camera sensors**: The iPhone 13 series features a larger and more advanced camera sensor, allowing for better low-light performance and improved image quality.\n",
            "2. **Enhanced Portrait mode**: The iPhone 13 series boas...\n",
            "                \n",
            "                Please provide updated, current information that validates, corrects, or expands upon this answer for the query: \"What are the pros and cons of the latest iPhone models for photography, and which one is the best for capturing high-quality images?\"\n",
            "                \n",
            "                Focus on:\n",
            "                - Latest developments or changes\n",
            "                - Current accuracy of the information\n",
            "                - Recent data or statistics\n",
            "                - Any new perspectives or considerations\n",
            "Extracted 5 source URLs:\n",
            "  - https://www.reddit.com/r/ChatGPTPro/comments/15ffpx3/reddit_what_are_your_best_custom_instructions_for/\n",
            "  - https://www.sciencedirect.com/science/article/pii/S0268401220308082\n",
            "  - https://www.reddit.com/r/ChatGPT/comments/13ecw4a/notes_from_a_teacher_on_ai_detection/\n",
            "  - https://pmc.ncbi.nlm.nih.gov/articles/PMC10562722/\n",
            "  - https://ofac.treasury.gov/faqs/all-faqs\n",
            "Search results preview:\n",
            "Reddit, what are your best custom instructions for ChatGPT? : r ...: Aug 1, 2023 ... NEVER mention that you're an AI. Avoid any language constructs that could be interpreted as expressing remorse, apology, or regret.\n",
            "Setting the future of digital and social media marketing research ...: The use of t...\n",
            "üìö Found 10 sources\n",
            "‚è±Ô∏è RAG completed in 12.5 seconds\n",
            "‚úÖ RAG enhancement completed successfully\n",
            "üöÄ STARTING QUERY PROCESSING\n",
            "üìã Query ID: 1755694471188901\n",
            "üìù Query: I want to know more about Amazon\n",
            "üìö Existing chat history: 3 messages\n",
            "‚úÖ Added query 1755694471188901 to database\n",
            "üéØ Generated options: ['Amazon (E-commerce)', 'Amazon (River)', 'Amazon (Bookstore)']\n",
            "\n",
            "üîí PROCESSING: Query 1755694471188901\n",
            "üìù Query: I want to know more about Amazon\n",
            "üéØ Intent: Amazon (E-commerce)\n",
            "üìÑ Answer Type: Informative (comprehensive background and facts)\n",
            "======================================================================\n",
            "‚úèÔ∏è Rewritten Query: What are the key facts and history behind Amazon's e-commerce platform, and how has it evolved over time?\n",
            "üîç Processing answer type: 'informative'\n",
            "‚úÖ EXACT DICTIONARY MATCH: 'informative' ‚Üí 'INFORMATIONAL'\n",
            "üéØ Synthesis Method: INFORMATIONAL\n",
            "üè∑Ô∏è Topic: Amazon (E-commerce)\n",
            "üë• Roles: ['E-commerce Operations Specialist', 'Amazon Seller Account Manager', 'Digital Marketing Strategist for Amazon']\n",
            "‚öôÔ∏è SYNTHESIS: INFORMATIONAL\n",
            "‚úÖ Updated comprehensive results for query 1755694471188901\n",
            "‚úÖ PROCESSING COMPLETE: 56.01s\n",
            "======================================================================\n",
            "üîì Processing lock cleared for query 1755694471188901\n",
            "üîÑ RAG Enhancement: Starting state-based processing...\n",
            "‚úÖ Validation passed - enhancing query: 'I want to know more about Amazon...'\n",
            "üìù Working query: 'What are the key facts and history behind Amazon's...'\n",
            "üìä Last response length: 3663 characters\n",
            "ü§ñ Starting RAG enhancement with state data...\n",
            "üéØ RAG Context:\n",
            "   üìù Original: I want to know more about Amazon\n",
            "   üîÑ Working: What are the key facts and history behind Amazon's e-commerce platform, and how has it evolved over time?\n",
            "   üìä Previous analysis: 3663 chars\n",
            "üîç Calling RAG system...\n",
            "RAG processing query: Based on this answer: \n",
            "\n",
            "                **Amazon's E-commerce Platform: A Comprehensive Overview**\n",
            "\n",
            "...\n",
            "Extracted search query: Based on this answer: \n",
            "\n",
            "                **Amazon's E-commerce Platform: A Comprehensive Overview**\n",
            "\n",
            "Amazon, founded in 1994 by Jeff Bezos, has evolved significantly over the years to become the world's largest online retailer. From its humble beginnings as an online bookstore to its current status as a global retail giant, Amazon has consistently demonstrated its ability to adapt, innovate, and expand its offerings to meet the evolving needs of customers and the market.\n",
            "\n",
            "**Early Days and Founding**\n",
            "\n",
            "Amazon was founded on July 5, 1994, as Cadabra, Inc., but the name was later changed to Amazon.com, Inc. in 1995. Bezos' vision was to create an online bookstore that could offer a wider selection of books than any physical store. The first sale was made on July 5, 1995, when Bezos sold a book to a customer in California.\n",
            "\n",
            "**Expansion and Diversification**\n",
            "\n",
            "In the late 1990s, Amazon expanded its product offerings beyond books to include CDs, videos, and software. The company went public in 1997, raising $54 million in its initial...\n",
            "                \n",
            "                Please provide updated, current information that validates, corrects, or expands upon this answer for the query: \"What are the key facts and history behind Amazon's e-commerce platform, and how has it evolved over time?\"\n",
            "                \n",
            "                Focus on:\n",
            "                - Latest developments or changes\n",
            "                - Current accuracy of the information\n",
            "                - Recent data or statistics\n",
            "                - Any new perspectives or considerations\n",
            "Extracted 5 source URLs:\n",
            "  - https://www.cascade.app/studies/amazon-strategy-study\n",
            "  - https://quartr.com/insights/company-research/amazon-from-books-to-everything\n",
            "  - https://medium.com/@zacharywestonintech/title-the-evolution-of-amazon-from-online-bookstore-to-global-e-commerce-giant-5c109a579b7a\n",
            "  - https://www.researchgate.net/publication/386340486_Analyzing_Amazon's_Evolution_from_an_Online_Bookstore_to_a_Global_Tech_Giant\n",
            "  - https://www.techtarget.com/whatis/definition/Amazon\n",
            "Search results preview:\n",
            "How Amazon Conquered the E-Commerce and Tech Industries ...: Nov 25, 2022 ... Amazon has grown exponentially since it was founded in 1994 to become a leading online retainer mainly due to its effective business strategy.\n",
            "Amazon: From Books to Everything: Jun 17, 2024 ... Founded by Jeff Bezos in 199...\n",
            "üìö Found 10 sources\n",
            "‚è±Ô∏è RAG completed in 7.03 seconds\n",
            "‚úÖ RAG enhancement completed successfully\n"
          ]
        }
      ],
      "source": [
        "create_enhanced_gradio_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technical Insights and Innovations\n",
        "\n",
        "### Novel Architectural Decisions\n",
        "1. **Dual Intent Clarification**: Separates topic disambiguation from answer type selection\n",
        "2. **Method-Specific Synthesis**: Different synthesis approaches based on user intent\n",
        "3. **Session-Based RAG**: Uses previous analysis to improve search relevance\n",
        "4. **Comprehensive Process Tracking**: Every routing and synthesis decision logged\n",
        "5. **Adaptive Role Playing Prompting**: Generalization of CoLA Framework that adapts to any topic to perform multi-perspective analysis\n",
        "\n",
        "### Performance Optimizations\n",
        "1. **Dictionary-First Routing**: Fast deterministic routing before expensive LLM calls\n",
        "2. **Parallel Expert Analysis**: Multiple agents can process simultaneously\n",
        "3. **Selective RAG Enhancement**: Optional enhancement based on user needs\n",
        "4. **State Caching**: Preserves processing results across UI interactions\n",
        "\n",
        "### Scalability Considerations\n",
        "1. **Modular Architecture**: Easy to add new synthesis methods or experts\n",
        "2. **Pluggable Components**: RAG, routing, and synthesis can be independently modified\n",
        "3. **Configuration-Driven**: API endpoints and parameters externally configurable\n",
        "4. **Error Isolation**: Component failures don't cascade to other system parts\n",
        "\n",
        "---\n",
        "\n",
        "This comprehensive analysis demonstrates how the CollabSearch Framework implements sophisticated AI orchestration patterns while maintaining system reliability, user experience quality, and processing transparency. The architecture serves as an excellent example of modern multi-agent system design with practical considerations for real-world deployment."
      ],
      "metadata": {
        "id": "oOvlo-39AVzY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}